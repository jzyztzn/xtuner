nohup: ignoring input
06/29 18:01:57 - mmengine - WARNING - Use random port: 21353
[2024-06-29 18:01:58,697] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-29 18:02:06,850] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-29 18:02:06,851] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-29 18:02:06,933] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[2024-06-29 18:02:06,951] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-29 18:02:06,953] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-29 18:02:06,958] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-29 18:02:06,965] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-29 18:02:06,973] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[2024-06-29 18:02:08,775] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-29 18:02:08,776] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-06-29 18:02:10,004] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-29 18:02:10,044] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-29 18:02:10,070] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-29 18:02:10,081] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-29 18:02:10,096] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-29 18:02:10,115] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-29 18:02:10,116] [INFO] [comm.py:637:init_distributed] cdb=None
06/29 18:02:13 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 1604851206
    GPU 0,1,2,3,4,5,6,7: NVIDIA A800 80GB PCIe
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 12.1, V12.1.105
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.3.1+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.18.1+cu121
    OpenCV: 4.10.0
    MMEngine: 0.10.4

Runtime environment:
    launcher: pytorch
    randomness: {'seed': None, 'deterministic': False}
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: None
    deterministic: False
    Distributed launcher: pytorch
    Distributed training: True
    GPU number: 8
------------------------------------------------------------

06/29 18:02:14 - mmengine - INFO - Config:
SYSTEM = ''
accumulative_counts = 1
batch_size = 16
betas = (
    0.9,
    0.999,
)
custom_hooks = [
    dict(
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/AI-ModelScope/vicuna-7b-v1___5',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.engine.hooks.DatasetInfoHook'),
    dict(
        evaluation_images='view.jpg',
        evaluation_inputs=[
            '请描述一下这张照片',
            'Please describe this picture',
        ],
        every_n_iters=500,
        image_processor=dict(
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
            trust_remote_code=True,
            type='transformers.CLIPImageProcessor.from_pretrained'),
        prompt_template='xtuner.utils.PROMPT_TEMPLATE.vicuna',
        system='',
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/AI-ModelScope/vicuna-7b-v1___5',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.engine.hooks.EvaluateChatHook'),
]
data_path = './data/llava_data/LLaVA-Instruct-150K/llava_v1_5_mix665k.json'
data_root = './data/llava_data/'
dataloader_num_workers = 4
default_hooks = dict(
    checkpoint=dict(
        by_epoch=False,
        interval=500,
        max_keep_ckpts=2,
        type='mmengine.hooks.CheckpointHook'),
    logger=dict(
        interval=10,
        log_metric_by_epoch=False,
        type='mmengine.hooks.LoggerHook'),
    param_scheduler=dict(type='mmengine.hooks.ParamSchedulerHook'),
    sampler_seed=dict(type='mmengine.hooks.DistSamplerSeedHook'),
    timer=dict(type='mmengine.hooks.IterTimerHook'))
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
evaluation_freq = 500
evaluation_images = 'view.jpg'
evaluation_inputs = [
    '请描述一下这张照片',
    'Please describe this picture',
]
image_folder = './data/llava_data/llava_images'
image_processor = dict(
    pretrained_model_name_or_path=
    '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
    trust_remote_code=True,
    type='transformers.CLIPImageProcessor.from_pretrained')
launcher = 'pytorch'
llava_dataset = dict(
    data_path='./data/llava_data/LLaVA-Instruct-150K/llava_v1_5_mix665k.json',
    dataset_map_fn='xtuner.dataset.map_fns.llava_map_fn',
    image_folder='./data/llava_data/llava_images',
    image_processor=dict(
        pretrained_model_name_or_path=
        '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
        trust_remote_code=True,
        type='transformers.CLIPImageProcessor.from_pretrained'),
    max_length=1472,
    pad_image_to_square=True,
    template_map_fn=dict(
        template='xtuner.utils.PROMPT_TEMPLATE.vicuna',
        type='xtuner.dataset.map_fns.template_map_fn_factory'),
    tokenizer=dict(
        padding_side='right',
        pretrained_model_name_or_path=
        '/root/autodl-tmp/tzn/Projects/pretrained/AI-ModelScope/vicuna-7b-v1___5',
        trust_remote_code=True,
        type='transformers.AutoTokenizer.from_pretrained'),
    type='xtuner.dataset.LLaVADataset')
llm_name_or_path = '/root/autodl-tmp/tzn/Projects/pretrained/AI-ModelScope/vicuna-7b-v1___5'
load_from = None
log_level = 'INFO'
log_processor = dict(by_epoch=False)
lr = 2e-05
max_epochs = 1
max_length = 1472
max_norm = 1
model = dict(
    freeze_llm=False,
    freeze_visual_encoder=True,
    llm=dict(
        pretrained_model_name_or_path=
        '/root/autodl-tmp/tzn/Projects/pretrained/AI-ModelScope/vicuna-7b-v1___5',
        trust_remote_code=True,
        type='transformers.AutoModelForCausalLM.from_pretrained'),
    pretrained_pth='./work_dirs/llava_v15_7b_pretrain_copy/iter_2000.pth',
    type='xtuner.model.LLaVAModel',
    visual_encoder=dict(
        pretrained_model_name_or_path=
        '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
        type='transformers.CLIPVisionModel.from_pretrained'))
optim_type = 'torch.optim.AdamW'
optim_wrapper = dict(
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        lr=2e-05,
        type='torch.optim.AdamW',
        weight_decay=0),
    type='DeepSpeedOptimWrapper')
param_scheduler = [
    dict(
        begin=0,
        by_epoch=True,
        convert_to_iter_based=True,
        end=0.03,
        start_factor=1e-05,
        type='mmengine.optim.LinearLR'),
    dict(
        begin=0.03,
        by_epoch=True,
        convert_to_iter_based=True,
        end=1,
        eta_min=0.0,
        type='mmengine.optim.CosineAnnealingLR'),
]
pretrained_pth = './work_dirs/llava_v15_7b_pretrain_copy/iter_2000.pth'
prompt_template = 'xtuner.utils.PROMPT_TEMPLATE.vicuna'
randomness = dict(deterministic=False, seed=None)
resume = False
runner_type = 'FlexibleRunner'
save_steps = 500
save_total_limit = 2
strategy = dict(
    config=dict(
        bf16=dict(enabled=True),
        fp16=dict(enabled=False, initial_scale_power=16),
        gradient_accumulation_steps='auto',
        gradient_clipping='auto',
        train_micro_batch_size_per_gpu='auto',
        zero_allow_untested_optimizer=True,
        zero_force_ds_cpu_optimizer=False,
        zero_optimization=dict(overlap_comm=True, stage=2)),
    exclude_frozen_parameters=True,
    gradient_accumulation_steps=1,
    gradient_clipping=1,
    sequence_parallel_size=1,
    train_micro_batch_size_per_gpu=16,
    type='xtuner.engine.DeepSpeedStrategy')
tokenizer = dict(
    padding_side='right',
    pretrained_model_name_or_path=
    '/root/autodl-tmp/tzn/Projects/pretrained/AI-ModelScope/vicuna-7b-v1___5',
    trust_remote_code=True,
    type='transformers.AutoTokenizer.from_pretrained')
train_cfg = dict(max_epochs=1, type='xtuner.engine.runner.TrainLoop')
train_dataloader = dict(
    batch_size=16,
    collate_fn=dict(type='xtuner.dataset.collate_fns.default_collate_fn'),
    dataset=dict(
        data_path=
        './data/llava_data/LLaVA-Instruct-150K/llava_v1_5_mix665k.json',
        dataset_map_fn='xtuner.dataset.map_fns.llava_map_fn',
        image_folder='./data/llava_data/llava_images',
        image_processor=dict(
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
            trust_remote_code=True,
            type='transformers.CLIPImageProcessor.from_pretrained'),
        max_length=1472,
        pad_image_to_square=True,
        template_map_fn=dict(
            template='xtuner.utils.PROMPT_TEMPLATE.vicuna',
            type='xtuner.dataset.map_fns.template_map_fn_factory'),
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/AI-ModelScope/vicuna-7b-v1___5',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.dataset.LLaVADataset'),
    num_workers=4,
    pin_memory=True,
    sampler=dict(
        length_property='modality_length',
        per_device_batch_size=16,
        type='xtuner.dataset.samplers.LengthGroupedSampler'))
visual_encoder_name_or_path = '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336'
visualizer = dict(
    type='mmengine.visualization.Visualizer',
    vis_backends=[
        dict(
            init_kwargs=dict(project='lava-clipL32-vicuna7b'),
            type='mmengine.visualization.WandbVisBackend'),
    ])
wandb_name = 'lava-clipL32-vicuna7b'
warmup_ratio = 0.03
weight_decay = 0
work_dir = './work_dirs/llava_v15_7b_finetune_copy'

wandb: Currently logged in as: jzyztzn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/20240629_180208/vis_data/wandb/run-20240629_180215-wfwfvnuj
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run worthy-cherry-4
wandb: ⭐️ View project at https://wandb.ai/jzyztzn/lava-clipL32-vicuna7b
wandb: 🚀 View run at https://wandb.ai/jzyztzn/lava-clipL32-vicuna7b/runs/wfwfvnuj
06/29 18:02:22 - mmengine - WARNING - Failed to search registry with scope "mmengine" in the "builder" registry tree. As a workaround, the current "builder" registry in "xtuner" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether "mmengine" is a correct scope, or whether the registry is initialized.
06/29 18:02:23 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DatasetInfoHook                    
(LOW         ) EvaluateChatHook                   
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(LOW         ) EvaluateChatHook                   
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) DatasetInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
(LOW         ) EvaluateChatHook                   
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(LOW         ) EvaluateChatHook                   
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) DatasetInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
06/29 18:02:48 - mmengine - INFO - xtuner_dataset_timeout = 1:00:00
Map (num_proc=32):   0%|          | 0/665298 [00:00<?, ? examples/s]Map (num_proc=32):   0%|          | 455/665298 [00:05<2:20:35, 78.82 examples/s]Map (num_proc=32):   0%|          | 1611/665298 [00:05<31:26, 351.85 examples/s]Map (num_proc=32):   0%|          | 2499/665298 [00:06<17:32, 629.70 examples/s]Map (num_proc=32):   0%|          | 3226/665298 [00:06<11:54, 926.87 examples/s]Map (num_proc=32):   1%|          | 4000/665298 [00:06<08:18, 1325.46 examples/s]Map (num_proc=32):   1%|          | 4991/665298 [00:06<05:30, 1998.79 examples/s]Map (num_proc=32):   1%|          | 6000/665298 [00:06<04:11, 2618.60 examples/s]Map (num_proc=32):   1%|          | 6968/665298 [00:06<03:13, 3395.94 examples/s]Map (num_proc=32):   1%|          | 8000/665298 [00:07<04:53, 2241.60 examples/s]Map (num_proc=32):   1%|▏         | 8981/665298 [00:07<03:49, 2864.59 examples/s]Map (num_proc=32):   2%|▏         | 10000/665298 [00:08<04:29, 2430.00 examples/s]Map (num_proc=32):   2%|▏         | 10497/665298 [00:08<05:01, 2173.93 examples/s]Map (num_proc=32):   3%|▎         | 21220/665298 [00:08<01:11, 9040.52 examples/s]Map (num_proc=32):   3%|▎         | 22186/665298 [00:11<04:02, 2655.93 examples/s]Map (num_proc=32):   6%|▋         | 42014/665298 [00:14<02:00, 5193.00 examples/s]Map (num_proc=32):   6%|▋         | 43055/665298 [00:14<02:06, 4935.92 examples/s]Map (num_proc=32):   7%|▋         | 43762/665298 [00:15<02:28, 4197.33 examples/s]Map (num_proc=32):  10%|▉         | 63209/665298 [00:16<01:04, 9371.69 examples/s]Map (num_proc=32):  10%|█         | 68434/665298 [00:16<00:53, 11146.91 examples/s]Map (num_proc=32):  11%|█         | 70783/665298 [00:16<00:51, 11526.89 examples/s]Map (num_proc=32):  11%|█         | 73001/665298 [00:16<00:51, 11583.23 examples/s]Map (num_proc=32):  11%|█▏        | 75287/665298 [00:16<00:48, 12235.65 examples/s]Map (num_proc=32):  12%|█▏        | 77423/665298 [00:16<00:47, 12476.07 examples/s]Map (num_proc=32):  12%|█▏        | 79724/665298 [00:17<01:25, 6855.99 examples/s] Map (num_proc=32):  12%|█▏        | 81336/665298 [00:17<01:22, 7102.08 examples/s]Map (num_proc=32):  12%|█▏        | 82517/665298 [00:18<02:21, 4111.06 examples/s]Map (num_proc=32):  13%|█▎        | 83997/665298 [00:19<02:51, 3382.26 examples/s]Map (num_proc=32):  13%|█▎        | 86059/665298 [00:19<02:11, 4390.98 examples/s]Map (num_proc=32):  13%|█▎        | 87429/665298 [00:19<01:51, 5179.39 examples/s]Map (num_proc=32):  13%|█▎        | 89083/665298 [00:19<01:30, 6397.49 examples/s]Map (num_proc=32):  14%|█▎        | 90495/665298 [00:19<01:17, 7427.98 examples/s]Map (num_proc=32):  14%|█▍        | 92122/665298 [00:20<01:06, 8617.58 examples/s]Map (num_proc=32):  14%|█▍        | 94164/665298 [00:20<00:59, 9579.18 examples/s]Map (num_proc=32):  14%|█▍        | 95793/665298 [00:21<02:03, 4601.89 examples/s]Map (num_proc=32):  15%|█▍        | 97190/665298 [00:21<01:44, 5421.03 examples/s]Map (num_proc=32):  15%|█▍        | 98776/665298 [00:21<01:29, 6343.11 examples/s]Map (num_proc=32):  15%|█▌        | 100166/665298 [00:21<01:27, 6424.91 examples/s]Map (num_proc=32):  15%|█▌        | 102164/665298 [00:22<01:58, 4771.84 examples/s]Map (num_proc=32):  15%|█▌        | 103078/665298 [00:22<02:20, 4011.05 examples/s]Map (num_proc=32):  16%|█▌        | 104795/665298 [00:23<02:36, 3577.43 examples/s]Map (num_proc=32):  16%|█▌        | 106955/665298 [00:23<01:52, 4950.91 examples/s]Map (num_proc=32):  16%|█▋        | 108406/665298 [00:23<01:32, 5999.64 examples/s]Map (num_proc=32):  17%|█▋        | 109955/665298 [00:23<01:16, 7223.76 examples/s]Map (num_proc=32):  17%|█▋        | 111583/665298 [00:23<01:03, 8680.83 examples/s]Map (num_proc=32):  17%|█▋        | 113008/665298 [00:23<01:00, 9151.40 examples/s]Map (num_proc=32):  17%|█▋        | 114356/665298 [00:23<00:56, 9741.07 examples/s]Map (num_proc=32):  18%|█▊        | 116869/665298 [00:23<00:47, 11510.58 examples/s]Map (num_proc=32):  18%|█▊        | 118970/665298 [00:24<00:43, 12660.27 examples/s]Map (num_proc=32):  18%|█▊        | 120541/665298 [00:24<01:44, 5230.29 examples/s] Map (num_proc=32):  18%|█▊        | 122858/665298 [00:25<01:20, 6703.49 examples/s]Map (num_proc=32):  19%|█▉        | 124746/665298 [00:25<01:27, 6167.16 examples/s]Map (num_proc=32):  19%|█▉        | 126154/665298 [00:26<03:06, 2892.17 examples/s]Map (num_proc=32):  19%|█▉        | 127718/665298 [00:26<02:24, 3724.08 examples/s]Map (num_proc=32):  19%|█▉        | 129035/665298 [00:26<01:58, 4529.46 examples/s]Map (num_proc=32):  20%|█▉        | 130662/665298 [00:27<01:32, 5793.23 examples/s]Map (num_proc=32):  20%|█▉        | 132018/665298 [00:27<01:27, 6102.20 examples/s]Map (num_proc=32):  20%|██        | 134877/665298 [00:27<00:58, 9099.74 examples/s]Map (num_proc=32):  21%|██        | 136660/665298 [00:27<00:50, 10459.29 examples/s]Map (num_proc=32):  21%|██        | 138746/665298 [00:28<01:40, 5249.37 examples/s] Map (num_proc=32):  21%|██        | 141051/665298 [00:28<01:15, 6926.92 examples/s]Map (num_proc=32):  21%|██▏       | 142666/665298 [00:29<02:08, 4063.08 examples/s]Map (num_proc=32):  22%|██▏       | 146375/665298 [00:29<01:34, 5484.87 examples/s]Map (num_proc=32):  22%|██▏       | 148454/665298 [00:29<01:15, 6803.70 examples/s]Map (num_proc=32):  23%|██▎       | 149880/665298 [00:29<01:07, 7628.00 examples/s]Map (num_proc=32):  23%|██▎       | 151453/665298 [00:29<00:58, 8715.26 examples/s]Map (num_proc=32):  23%|██▎       | 152875/665298 [00:30<00:53, 9610.77 examples/s]Map (num_proc=32):  23%|██▎       | 155104/665298 [00:30<00:46, 10968.67 examples/s]Map (num_proc=32):  24%|██▎       | 156537/665298 [00:30<00:50, 10015.06 examples/s]Map (num_proc=32):  24%|██▍       | 158389/665298 [00:30<00:44, 11386.41 examples/s]Map (num_proc=32):  24%|██▍       | 159989/665298 [00:31<01:45, 4787.88 examples/s] Map (num_proc=32):  24%|██▍       | 161129/665298 [00:31<01:32, 5456.05 examples/s]Map (num_proc=32):  24%|██▍       | 162368/665298 [00:31<01:22, 6064.38 examples/s]Map (num_proc=32):  25%|██▍       | 163531/665298 [00:32<02:39, 3143.55 examples/s]Map (num_proc=32):  25%|██▌       | 166751/665298 [00:33<02:32, 3260.69 examples/s]Map (num_proc=32):  25%|██▌       | 167838/665298 [00:33<02:19, 3561.69 examples/s]Map (num_proc=32):  25%|██▌       | 169029/665298 [00:33<01:59, 4138.41 examples/s]Map (num_proc=32):  26%|██▌       | 170291/665298 [00:34<02:29, 3315.88 examples/s]Map (num_proc=32):  26%|██▌       | 171001/665298 [00:34<02:41, 3051.43 examples/s]Map (num_proc=32):  26%|██▌       | 171773/665298 [00:34<02:29, 3303.71 examples/s]Map (num_proc=32):  26%|██▌       | 172397/665298 [00:35<04:18, 1910.19 examples/s]Map (num_proc=32):  28%|██▊       | 184779/665298 [00:35<00:42, 11435.25 examples/s]Map (num_proc=32):  28%|██▊       | 188633/665298 [00:36<01:04, 7417.78 examples/s] Map (num_proc=32):  29%|██▉       | 191330/665298 [00:37<01:26, 5468.82 examples/s]Map (num_proc=32):  29%|██▉       | 193265/665298 [00:37<01:17, 6105.76 examples/s]Map (num_proc=32):  29%|██▉       | 195058/665298 [00:38<01:07, 6939.69 examples/s]Map (num_proc=32):  30%|██▉       | 197020/665298 [00:38<01:41, 4613.79 examples/s]Map (num_proc=32):  31%|███       | 205563/665298 [00:39<00:44, 10444.57 examples/s]Map (num_proc=32):  31%|███▏      | 209429/665298 [00:40<01:04, 7013.41 examples/s] Map (num_proc=32):  32%|███▏      | 212111/665298 [00:41<01:26, 5222.03 examples/s]Map (num_proc=32):  32%|███▏      | 214056/665298 [00:41<01:17, 5808.03 examples/s]Map (num_proc=32):  32%|███▏      | 215910/665298 [00:42<01:46, 4234.54 examples/s]Map (num_proc=32):  34%|███▍      | 226034/665298 [00:42<00:43, 10059.53 examples/s]Map (num_proc=32):  35%|███▍      | 229572/665298 [00:43<00:58, 7406.55 examples/s] Map (num_proc=32):  35%|███▍      | 232221/665298 [00:44<01:21, 5332.15 examples/s]Map (num_proc=32):  35%|███▌      | 234174/665298 [00:45<01:50, 3906.09 examples/s]Map (num_proc=32):  38%|███▊      | 250342/665298 [00:46<00:51, 8017.51 examples/s]Map (num_proc=32):  38%|███▊      | 252492/665298 [00:46<00:49, 8356.03 examples/s]Map (num_proc=32):  38%|███▊      | 254833/665298 [00:46<00:45, 9060.16 examples/s]Map (num_proc=32):  39%|███▊      | 257041/665298 [00:47<01:03, 6468.96 examples/s]Map (num_proc=32):  39%|███▉      | 258492/665298 [00:47<00:58, 6989.56 examples/s]Map (num_proc=32):  39%|███▉      | 260121/665298 [00:47<00:53, 7600.81 examples/s]Map (num_proc=32):  39%|███▉      | 261551/665298 [00:48<01:29, 4533.98 examples/s]Map (num_proc=32):  41%|████      | 271133/665298 [00:49<00:55, 7136.58 examples/s]Map (num_proc=32):  41%|████      | 273283/665298 [00:49<00:52, 7414.56 examples/s]Map (num_proc=32):  41%|████▏     | 275646/665298 [00:49<00:45, 8648.26 examples/s]Map (num_proc=32):  42%|████▏     | 277283/665298 [00:49<00:41, 9408.98 examples/s]Map (num_proc=32):  42%|████▏     | 278940/665298 [00:50<00:57, 6673.42 examples/s]Map (num_proc=32):  42%|████▏     | 280382/665298 [00:50<01:05, 5916.24 examples/s]Map (num_proc=32):  42%|████▏     | 282190/665298 [00:50<00:54, 7043.44 examples/s]Map (num_proc=32):  43%|████▎     | 283455/665298 [00:51<01:38, 3875.06 examples/s]Map (num_proc=32):  44%|████▍     | 291924/665298 [00:52<00:58, 6337.44 examples/s]Map (num_proc=32):  44%|████▍     | 294074/665298 [00:52<00:54, 6799.64 examples/s]Map (num_proc=32):  45%|████▍     | 297161/665298 [00:53<00:43, 8389.55 examples/s]Map (num_proc=32):  45%|████▍     | 298988/665298 [00:53<01:05, 5559.13 examples/s]Map (num_proc=32):  45%|████▌     | 302072/665298 [00:54<00:49, 7298.71 examples/s]Map (num_proc=32):  46%|████▌     | 303510/665298 [00:54<01:06, 5474.91 examples/s]Map (num_proc=32):  46%|████▌     | 305074/665298 [00:55<01:09, 5200.19 examples/s]Map (num_proc=32):  47%|████▋     | 312710/665298 [00:55<00:54, 6453.48 examples/s]Map (num_proc=32):  47%|████▋     | 314029/665298 [00:56<00:52, 6707.39 examples/s]Map (num_proc=32):  47%|████▋     | 315984/665298 [00:56<00:45, 7619.83 examples/s]Map (num_proc=32):  48%|████▊     | 317320/665298 [00:57<01:17, 4511.99 examples/s]Map (num_proc=32):  48%|████▊     | 318289/665298 [00:58<01:57, 2947.81 examples/s]Map (num_proc=32):  49%|████▉     | 328657/665298 [00:58<00:36, 9141.48 examples/s]Map (num_proc=32):  50%|████▉     | 332261/665298 [00:58<00:41, 8114.56 examples/s]Map (num_proc=32):  50%|█████     | 335108/665298 [01:01<01:37, 3383.02 examples/s]Map (num_proc=32):  52%|█████▏    | 347537/665298 [01:01<00:40, 7806.53 examples/s]Map (num_proc=32):  53%|█████▎    | 352180/665298 [01:02<00:41, 7474.91 examples/s]Map (num_proc=32):  54%|█████▎    | 356017/665298 [01:02<00:39, 7792.04 examples/s]Map (num_proc=32):  54%|█████▍    | 358695/665298 [01:04<01:17, 3980.94 examples/s]Map (num_proc=32):  56%|█████▌    | 371447/665298 [01:04<00:35, 8224.75 examples/s]Map (num_proc=32):  56%|█████▋    | 374804/665298 [01:05<00:39, 7385.77 examples/s]Map (num_proc=32):  57%|█████▋    | 377541/665298 [01:08<01:14, 3852.76 examples/s]Map (num_proc=32):  58%|█████▊    | 386844/665298 [01:08<00:41, 6652.31 examples/s]Map (num_proc=32):  59%|█████▊    | 390131/665298 [01:08<00:43, 6368.49 examples/s]Map (num_proc=32):  59%|█████▉    | 392755/665298 [01:09<00:40, 6797.53 examples/s]Map (num_proc=32):  59%|█████▉    | 394749/665298 [01:09<00:39, 6927.48 examples/s]Map (num_proc=32):  60%|█████▉    | 396447/665298 [01:10<00:56, 4741.34 examples/s]Map (num_proc=32):  60%|█████▉    | 397833/665298 [01:11<01:15, 3525.92 examples/s]Map (num_proc=32):  60%|██████    | 402494/665298 [01:11<00:54, 4843.32 examples/s]Map (num_proc=32):  61%|██████▏   | 409028/665298 [01:11<00:30, 8408.79 examples/s]Map (num_proc=32):  62%|██████▏   | 411481/665298 [01:12<00:31, 8050.16 examples/s]Map (num_proc=32):  62%|██████▏   | 413551/665298 [01:12<00:32, 7827.37 examples/s]Map (num_proc=32):  62%|██████▏   | 415242/665298 [01:13<00:59, 4191.89 examples/s]Map (num_proc=32):  63%|██████▎   | 416327/665298 [01:14<01:10, 3508.24 examples/s]Map (num_proc=32):  64%|██████▎   | 423114/665298 [01:14<00:39, 6145.02 examples/s]Map (num_proc=32):  64%|██████▍   | 428818/665298 [01:14<00:25, 9334.54 examples/s]Map (num_proc=32):  65%|██████▍   | 430728/665298 [01:15<00:28, 8340.52 examples/s]Map (num_proc=32):  65%|██████▍   | 432274/665298 [01:15<00:28, 8125.41 examples/s]Map (num_proc=32):  65%|██████▌   | 433731/665298 [01:16<00:47, 4848.34 examples/s]Map (num_proc=32):  65%|██████▌   | 434741/665298 [01:16<00:46, 4964.76 examples/s]Map (num_proc=32):  65%|██████▌   | 435727/665298 [01:17<01:13, 3141.70 examples/s]Map (num_proc=32):  67%|██████▋   | 444863/665298 [01:17<00:29, 7531.74 examples/s]Map (num_proc=32):  68%|██████▊   | 449615/665298 [01:17<00:21, 10201.80 examples/s]Map (num_proc=32):  68%|██████▊   | 451437/665298 [01:18<00:25, 8413.11 examples/s] Map (num_proc=32):  68%|██████▊   | 452881/665298 [01:19<00:43, 4885.59 examples/s]Map (num_proc=32):  68%|██████▊   | 454030/665298 [01:20<01:02, 3357.16 examples/s]Map (num_proc=32):  68%|██████▊   | 454822/665298 [01:20<00:58, 3603.45 examples/s]Map (num_proc=32):  70%|██████▉   | 463120/665298 [01:20<00:21, 9526.15 examples/s]Map (num_proc=32):  70%|███████   | 465980/665298 [01:23<01:08, 2923.91 examples/s]Map (num_proc=32):  72%|███████▏  | 480628/665298 [01:23<00:23, 7796.04 examples/s]Map (num_proc=32):  73%|███████▎  | 485858/665298 [01:24<00:21, 8178.86 examples/s]Map (num_proc=32):  74%|███████▎  | 489708/665298 [01:24<00:20, 8737.86 examples/s]Map (num_proc=32):  74%|███████▍  | 492792/665298 [01:25<00:29, 5942.24 examples/s]Map (num_proc=32):  74%|███████▍  | 495232/665298 [01:26<00:34, 4993.85 examples/s]Map (num_proc=32):  76%|███████▋  | 507702/665298 [01:27<00:17, 8957.65 examples/s]Map (num_proc=32):  78%|███████▊  | 516076/665298 [01:27<00:13, 10936.37 examples/s]Map (num_proc=32):  78%|███████▊  | 520381/665298 [01:29<00:26, 5530.27 examples/s] Map (num_proc=32):  80%|████████  | 535257/665298 [01:30<00:12, 10640.26 examples/s]Map (num_proc=32):  81%|████████  | 540015/665298 [01:30<00:12, 9942.05 examples/s] Map (num_proc=32):  82%|████████▏ | 543585/665298 [01:33<00:24, 5018.80 examples/s]Map (num_proc=32):  84%|████████▍ | 560558/665298 [01:33<00:10, 10218.12 examples/s]Map (num_proc=32):  85%|████████▌ | 566320/665298 [01:35<00:13, 7107.46 examples/s] Map (num_proc=32):  86%|████████▌ | 570320/665298 [01:36<00:16, 5801.20 examples/s]Map (num_proc=32):  87%|████████▋ | 581348/665298 [01:36<00:09, 9213.61 examples/s]Map (num_proc=32):  88%|████████▊ | 585685/665298 [01:37<00:09, 8707.00 examples/s]Map (num_proc=32):  89%|████████▊ | 589114/665298 [01:37<00:08, 8746.46 examples/s]Map (num_proc=32):  89%|████████▉ | 591839/665298 [01:38<00:11, 6578.80 examples/s]Map (num_proc=32):  89%|████████▉ | 593783/665298 [01:38<00:10, 7030.99 examples/s]Map (num_proc=32):  90%|████████▉ | 595854/665298 [01:39<00:13, 5180.90 examples/s]Map (num_proc=32):  91%|█████████ | 603771/665298 [01:40<00:09, 6273.97 examples/s]Map (num_proc=32):  91%|█████████ | 605083/665298 [01:40<00:09, 6516.71 examples/s]Map (num_proc=32):  91%|█████████▏| 607546/665298 [01:40<00:07, 7522.68 examples/s]Map (num_proc=32):  92%|█████████▏| 608986/665298 [01:41<00:11, 5021.65 examples/s]Map (num_proc=32):  92%|█████████▏| 611312/665298 [01:41<00:08, 6295.31 examples/s]Map (num_proc=32):  92%|█████████▏| 612928/665298 [01:41<00:07, 7155.93 examples/s]Map (num_proc=32):  92%|█████████▏| 614553/665298 [01:42<00:11, 4317.68 examples/s]Map (num_proc=32):  94%|█████████▍| 624560/665298 [01:43<00:05, 7307.55 examples/s]Map (num_proc=32):  94%|█████████▍| 626036/665298 [01:43<00:05, 7245.33 examples/s]Map (num_proc=32):  94%|█████████▍| 627722/665298 [01:43<00:04, 7965.84 examples/s]Map (num_proc=32):  95%|█████████▍| 629581/665298 [01:44<00:04, 8607.50 examples/s]Map (num_proc=32):  95%|█████████▍| 631415/665298 [01:44<00:03, 8856.80 examples/s]Map (num_proc=32):  95%|█████████▌| 632549/665298 [01:44<00:03, 9193.19 examples/s]Map (num_proc=32):  95%|█████████▌| 633691/665298 [01:44<00:03, 9553.28 examples/s]Map (num_proc=32):  96%|█████████▌| 635422/665298 [01:44<00:03, 9873.10 examples/s]Map (num_proc=32):  96%|█████████▌| 636878/665298 [01:44<00:02, 9827.88 examples/s]Map (num_proc=32):  96%|█████████▌| 638011/665298 [01:44<00:02, 10128.79 examples/s]Map (num_proc=32):  96%|█████████▌| 639433/665298 [01:45<00:02, 10596.81 examples/s]Map (num_proc=32):  96%|█████████▋| 640576/665298 [01:45<00:02, 10790.97 examples/s]Map (num_proc=32):  96%|█████████▋| 641716/665298 [01:45<00:02, 10941.37 examples/s]Map (num_proc=32):  97%|█████████▋| 643308/665298 [01:45<00:02, 10614.63 examples/s]Map (num_proc=32):  97%|█████████▋| 644441/665298 [01:45<00:01, 10783.61 examples/s]Map (num_proc=32):  97%|█████████▋| 646050/665298 [01:46<00:07, 2680.77 examples/s] Map (num_proc=32):  98%|█████████▊| 652442/665298 [01:47<00:01, 7132.45 examples/s]Map (num_proc=32):  98%|█████████▊| 654622/665298 [01:47<00:01, 7703.56 examples/s]Map (num_proc=32):  99%|█████████▊| 656333/665298 [01:47<00:01, 8279.47 examples/s]Map (num_proc=32):  99%|█████████▉| 657874/665298 [01:47<00:00, 8641.79 examples/s]Map (num_proc=32):  99%|█████████▉| 659588/665298 [01:47<00:00, 9010.96 examples/s]Map (num_proc=32):  99%|█████████▉| 661322/665298 [01:47<00:00, 9579.79 examples/s]Map (num_proc=32): 100%|█████████▉| 662914/665298 [01:48<00:00, 9828.22 examples/s]Map (num_proc=32): 100%|█████████▉| 664211/665298 [01:48<00:00, 10180.39 examples/s]Map (num_proc=32): 100%|██████████| 665298/665298 [01:49<00:00, 6048.23 examples/s] 
Map (num_proc=32):   0%|          | 0/665298 [00:00<?, ? examples/s]Map (num_proc=32):   0%|          | 537/665298 [00:01<20:40, 536.07 examples/s]Map (num_proc=32):   0%|          | 1589/665298 [00:01<06:20, 1744.90 examples/s]Map (num_proc=32):   0%|          | 2594/665298 [00:01<03:48, 2904.25 examples/s]Map (num_proc=32):   1%|          | 3586/665298 [00:01<02:42, 4063.78 examples/s]Map (num_proc=32):   1%|          | 5586/665298 [00:01<01:36, 6816.28 examples/s]Map (num_proc=32):   1%|          | 7579/665298 [00:01<01:13, 8990.01 examples/s]Map (num_proc=32):   2%|▏         | 10133/665298 [00:01<00:52, 12533.32 examples/s]Map (num_proc=32):   2%|▏         | 12584/665298 [00:01<00:43, 14996.66 examples/s]Map (num_proc=32):   2%|▏         | 16061/665298 [00:01<00:33, 19641.96 examples/s]Map (num_proc=32):   3%|▎         | 21583/665298 [00:02<00:22, 28519.90 examples/s]Map (num_proc=32):   4%|▎         | 24849/665298 [00:02<00:22, 28840.65 examples/s]Map (num_proc=32):   5%|▍         | 30406/665298 [00:02<00:17, 35541.17 examples/s]Map (num_proc=32):   5%|▌         | 35739/665298 [00:02<00:15, 40046.55 examples/s]Map (num_proc=32):   6%|▌         | 41196/665298 [00:02<00:14, 43697.64 examples/s]Map (num_proc=32):   7%|▋         | 48976/665298 [00:02<00:11, 51884.69 examples/s]Map (num_proc=32):   8%|▊         | 54708/665298 [00:02<00:12, 50133.89 examples/s]Map (num_proc=32):   9%|▉         | 60199/665298 [00:02<00:12, 50151.85 examples/s]Map (num_proc=32):  11%|█▏        | 75661/665298 [00:02<00:07, 76957.07 examples/s]Map (num_proc=32):  13%|█▎        | 83708/665298 [00:03<00:08, 66376.65 examples/s]Map (num_proc=32):  15%|█▌        | 99971/665298 [00:03<00:06, 89284.60 examples/s]Map (num_proc=32):  17%|█▋        | 110168/665298 [00:03<00:06, 91684.14 examples/s]Map (num_proc=32):  18%|█▊        | 120981/665298 [00:03<00:05, 95227.86 examples/s]Map (num_proc=32):  20%|█▉        | 130923/665298 [00:03<00:05, 91971.70 examples/s]Map (num_proc=32):  21%|██        | 140927/665298 [00:03<00:05, 90291.43 examples/s]Map (num_proc=32):  23%|██▎       | 155101/665298 [00:03<00:05, 100615.19 examples/s]Map (num_proc=32):  25%|██▍       | 165633/665298 [00:03<00:05, 91566.42 examples/s] Map (num_proc=32):  26%|██▋       | 175618/665298 [00:04<00:07, 62304.74 examples/s]Map (num_proc=32):  28%|██▊       | 183733/665298 [00:05<00:31, 15384.72 examples/s]Map (num_proc=32):  30%|██▉       | 197114/665298 [00:06<00:29, 15967.39 examples/s]Map (num_proc=32):  30%|███       | 201516/665298 [00:07<00:41, 11062.57 examples/s]Map (num_proc=32):  31%|███       | 204934/665298 [00:09<01:03, 7268.29 examples/s] Map (num_proc=32):  32%|███▏      | 214301/665298 [00:09<00:48, 9251.32 examples/s]Map (num_proc=32):  33%|███▎      | 216508/665298 [00:10<00:53, 8398.75 examples/s]Map (num_proc=32):  49%|████▉     | 327374/665298 [00:10<00:06, 55213.35 examples/s]Map (num_proc=32):  55%|█████▌    | 367298/665298 [00:10<00:04, 71883.64 examples/s]Map (num_proc=32):  57%|█████▋    | 379170/665298 [00:12<00:07, 39225.43 examples/s]Map (num_proc=32):  61%|██████    | 407009/665298 [00:12<00:06, 41766.95 examples/s]Map (num_proc=32):  66%|██████▋   | 440877/665298 [00:13<00:04, 45405.23 examples/s]Map (num_proc=32):  67%|██████▋   | 447816/665298 [00:13<00:05, 40759.96 examples/s]Map (num_proc=32):  70%|███████   | 465765/665298 [00:14<00:05, 35726.83 examples/s]Map (num_proc=32):  71%|███████   | 470828/665298 [00:14<00:05, 34900.18 examples/s]Map (num_proc=32):  71%|███████▏  | 475343/665298 [00:14<00:05, 35711.86 examples/s]Map (num_proc=32):  80%|███████▉  | 529570/665298 [00:15<00:02, 48662.03 examples/s]Map (num_proc=32):  90%|████████▉ | 597044/665298 [00:16<00:00, 73738.23 examples/s]Map (num_proc=32):  91%|█████████ | 604461/665298 [00:16<00:01, 57410.32 examples/s]Map (num_proc=32):  93%|█████████▎| 621826/665298 [00:17<00:01, 41742.01 examples/s]Map (num_proc=32):  95%|█████████▌| 634501/665298 [00:17<00:00, 44035.22 examples/s]Map (num_proc=32):  96%|█████████▌| 639897/665298 [00:17<00:00, 40457.38 examples/s]Map (num_proc=32):  97%|█████████▋| 644409/665298 [00:18<00:00, 34336.24 examples/s]Map (num_proc=32):  98%|█████████▊| 654974/665298 [00:18<00:00, 29715.25 examples/s]Map (num_proc=32):  99%|█████████▉| 658625/665298 [00:19<00:00, 22244.55 examples/s]Map (num_proc=32): 100%|██████████| 665298/665298 [00:21<00:00, 30386.09 examples/s]
Filter (num_proc=32):   0%|          | 0/665298 [00:00<?, ? examples/s]Filter (num_proc=32):   0%|          | 1000/665298 [00:01<12:30, 884.98 examples/s]Filter (num_proc=32):   0%|          | 2000/665298 [00:01<05:50, 1894.78 examples/s]Filter (num_proc=32):   0%|          | 3000/665298 [00:01<03:53, 2831.58 examples/s]Filter (num_proc=32):   1%|          | 5000/665298 [00:01<02:14, 4895.45 examples/s]Filter (num_proc=32):   1%|▏         | 9000/665298 [00:01<01:06, 9803.15 examples/s]Filter (num_proc=32):   2%|▏         | 12000/665298 [00:01<00:48, 13343.99 examples/s]Filter (num_proc=32):   2%|▏         | 14000/665298 [00:01<00:45, 14293.32 examples/s]Filter (num_proc=32):   3%|▎         | 17000/665298 [00:02<00:41, 15806.60 examples/s]Filter (num_proc=32):   3%|▎         | 20000/665298 [00:02<00:34, 18759.27 examples/s]Filter (num_proc=32):   4%|▎         | 24000/665298 [00:02<00:28, 22800.69 examples/s]Filter (num_proc=32):   4%|▍         | 28000/665298 [00:02<00:26, 24304.98 examples/s]Filter (num_proc=32):   5%|▌         | 35000/665298 [00:02<00:17, 35118.32 examples/s]Filter (num_proc=32):   7%|▋         | 44000/665298 [00:02<00:12, 47928.74 examples/s]Filter (num_proc=32):   8%|▊         | 50000/665298 [00:02<00:12, 48913.28 examples/s]Filter (num_proc=32):   9%|▉         | 62000/665298 [00:02<00:10, 59316.52 examples/s]Filter (num_proc=32):  11%|█▏        | 75582/665298 [00:03<00:07, 77088.71 examples/s]Filter (num_proc=32):  13%|█▎        | 84582/665298 [00:03<00:08, 66543.29 examples/s]Filter (num_proc=32):  15%|█▍        | 98582/665298 [00:03<00:06, 81919.85 examples/s]Filter (num_proc=32):  16%|█▋        | 109164/665298 [00:03<00:08, 65183.78 examples/s]Filter (num_proc=32):  19%|█▉        | 127164/665298 [00:03<00:06, 88636.07 examples/s]Filter (num_proc=32):  21%|██        | 138746/665298 [00:03<00:06, 84206.66 examples/s]Filter (num_proc=32):  23%|██▎       | 150746/665298 [00:03<00:06, 85487.99 examples/s]Filter (num_proc=32):  24%|██▍       | 160537/665298 [00:04<00:07, 68875.11 examples/s]Filter (num_proc=32):  25%|██▌       | 168537/665298 [00:04<00:09, 51204.39 examples/s]Filter (num_proc=32):  26%|██▋       | 175537/665298 [00:04<00:10, 46982.60 examples/s]Filter (num_proc=32):  27%|██▋       | 181537/665298 [00:05<00:14, 34343.56 examples/s]Filter (num_proc=32):  31%|███       | 207328/665298 [00:05<00:06, 67618.72 examples/s]Filter (num_proc=32):  33%|███▎      | 218328/665298 [00:05<00:06, 70628.74 examples/s]Filter (num_proc=32):  34%|███▍      | 229328/665298 [00:05<00:05, 77665.60 examples/s]Filter (num_proc=32):  36%|███▌      | 240119/665298 [00:05<00:05, 78000.30 examples/s]Filter (num_proc=32):  38%|███▊      | 250119/665298 [00:05<00:05, 76240.42 examples/s]Filter (num_proc=32):  40%|████      | 267119/665298 [00:05<00:04, 93962.89 examples/s]Filter (num_proc=32):  42%|████▏     | 277910/665298 [00:05<00:04, 90908.42 examples/s]Filter (num_proc=32):  45%|████▍     | 298910/665298 [00:05<00:03, 115065.62 examples/s]Filter (num_proc=32):  47%|████▋     | 311283/665298 [00:06<00:04, 76427.36 examples/s] Filter (num_proc=32):  48%|████▊     | 322074/665298 [00:06<00:05, 65585.94 examples/s]Filter (num_proc=32):  50%|████▉     | 330865/665298 [00:06<00:06, 48014.33 examples/s]Filter (num_proc=32):  51%|█████     | 337865/665298 [00:07<00:07, 42483.86 examples/s]Filter (num_proc=32):  52%|█████▏    | 343865/665298 [00:07<00:07, 41502.23 examples/s]Filter (num_proc=32):  52%|█████▏    | 348865/665298 [00:07<00:07, 41363.54 examples/s]Filter (num_proc=32):  53%|█████▎    | 353865/665298 [00:07<00:08, 36572.56 examples/s]Filter (num_proc=32):  54%|█████▍    | 358865/665298 [00:07<00:08, 37744.68 examples/s]Filter (num_proc=32):  55%|█████▍    | 363865/665298 [00:07<00:08, 34980.83 examples/s]Filter (num_proc=32):  55%|█████▌    | 368865/665298 [00:08<00:08, 35999.49 examples/s]Filter (num_proc=32):  56%|█████▌    | 373656/665298 [00:08<00:07, 36895.84 examples/s]Filter (num_proc=32):  57%|█████▋    | 378656/665298 [00:08<00:07, 37136.25 examples/s]Filter (num_proc=32):  58%|█████▊    | 383656/665298 [00:08<00:07, 36314.26 examples/s]Filter (num_proc=32):  58%|█████▊    | 388656/665298 [00:08<00:07, 37215.73 examples/s]Filter (num_proc=32):  59%|█████▉    | 392656/665298 [00:08<00:07, 36843.83 examples/s]Filter (num_proc=32):  60%|█████▉    | 396656/665298 [00:08<00:07, 35582.31 examples/s]Filter (num_proc=32):  60%|██████    | 402447/665298 [00:08<00:07, 36750.04 examples/s]Filter (num_proc=32):  61%|██████    | 406447/665298 [00:09<00:06, 37506.98 examples/s]Filter (num_proc=32):  62%|██████▏   | 410447/665298 [00:09<00:06, 36655.67 examples/s]Filter (num_proc=32):  63%|██████▎   | 417238/665298 [00:09<00:06, 41034.43 examples/s]Filter (num_proc=32):  63%|██████▎   | 422238/665298 [00:09<00:05, 42758.39 examples/s]Filter (num_proc=32):  64%|██████▍   | 427238/665298 [00:09<00:05, 44557.36 examples/s]Filter (num_proc=32):  65%|██████▍   | 432238/665298 [00:09<00:05, 44226.08 examples/s]Filter (num_proc=32):  66%|██████▋   | 442238/665298 [00:09<00:04, 52885.67 examples/s]Filter (num_proc=32):  67%|██████▋   | 448238/665298 [00:10<00:05, 39964.53 examples/s]Filter (num_proc=32):  70%|███████   | 467238/665298 [00:10<00:02, 71600.36 examples/s]Filter (num_proc=32):  72%|███████▏  | 476238/665298 [00:10<00:02, 73418.88 examples/s]Filter (num_proc=32):  73%|███████▎  | 486238/665298 [00:10<00:02, 78250.56 examples/s]Filter (num_proc=32):  74%|███████▍  | 495238/665298 [00:10<00:02, 76225.38 examples/s]Filter (num_proc=32):  78%|███████▊  | 519238/665298 [00:10<00:01, 111886.28 examples/s]Filter (num_proc=32):  80%|███████▉  | 531028/665298 [00:10<00:01, 107573.22 examples/s]Filter (num_proc=32):  82%|████████▏ | 543028/665298 [00:10<00:01, 76766.39 examples/s] Filter (num_proc=32):  84%|████████▍ | 559608/665298 [00:11<00:01, 93355.80 examples/s]Filter (num_proc=32):  86%|████████▌ | 570608/665298 [00:11<00:01, 93444.83 examples/s]Filter (num_proc=32):  87%|████████▋ | 581398/665298 [00:11<00:00, 88547.28 examples/s]Filter (num_proc=32):  89%|████████▉ | 591978/665298 [00:11<00:00, 80468.34 examples/s]Filter (num_proc=32):  90%|█████████ | 600978/665298 [00:11<00:00, 79349.44 examples/s]Filter (num_proc=32):  92%|█████████▏| 609768/665298 [00:11<00:00, 80088.31 examples/s]Filter (num_proc=32):  93%|█████████▎| 618558/665298 [00:11<00:00, 74732.46 examples/s]Filter (num_proc=32):  94%|█████████▍| 626348/665298 [00:11<00:00, 75054.02 examples/s]Filter (num_proc=32):  95%|█████████▌| 634138/665298 [00:12<00:00, 59707.03 examples/s]Filter (num_proc=32):  96%|█████████▋| 641138/665298 [00:12<00:00, 48434.95 examples/s]Filter (num_proc=32):  97%|█████████▋| 646928/665298 [00:12<00:00, 39994.42 examples/s]Filter (num_proc=32):  98%|█████████▊| 651928/665298 [00:13<00:00, 24610.07 examples/s]Filter (num_proc=32):  99%|█████████▊| 655718/665298 [00:13<00:00, 20174.01 examples/s]Filter (num_proc=32):  99%|█████████▉| 658718/665298 [00:13<00:00, 16840.54 examples/s]Filter (num_proc=32):  99%|█████████▉| 661508/665298 [00:14<00:00, 11059.55 examples/s]Filter (num_proc=32): 100%|█████████▉| 663508/665298 [00:14<00:00, 7861.46 examples/s] Filter (num_proc=32): 100%|██████████| 665298/665298 [00:15<00:00, 6432.41 examples/s]Filter (num_proc=32): 100%|██████████| 665298/665298 [00:15<00:00, 42347.75 examples/s]
Map (num_proc=32):   0%|          | 0/665298 [00:00<?, ? examples/s]Map (num_proc=32):   0%|          | 34/665298 [00:13<75:06:30,  2.46 examples/s]Map (num_proc=32):   0%|          | 100/665298 [00:13<20:06:51,  9.19 examples/s]Map (num_proc=32):   0%|          | 165/665298 [00:14<10:01:58, 18.42 examples/s]Map (num_proc=32):   0%|          | 257/665298 [00:14<5:06:08, 36.21 examples/s] Map (num_proc=32):   0%|          | 355/665298 [00:14<3:00:17, 61.47 examples/s]Map (num_proc=32):   0%|          | 420/665298 [00:14<2:12:41, 83.51 examples/s]Map (num_proc=32):   0%|          | 487/665298 [00:14<1:37:52, 113.22 examples/s]Map (num_proc=32):   0%|          | 587/665298 [00:14<1:06:17, 167.14 examples/s]Map (num_proc=32):   0%|          | 687/665298 [00:14<48:35, 227.95 examples/s]  Map (num_proc=32):   0%|          | 784/665298 [00:14<38:11, 289.98 examples/s]Map (num_proc=32):   0%|          | 850/665298 [00:15<33:03, 335.06 examples/s]Map (num_proc=32):   0%|          | 916/665298 [00:15<28:57, 382.36 examples/s]Map (num_proc=32):   0%|          | 1000/665298 [00:15<29:23, 376.80 examples/s]Map (num_proc=32):   0%|          | 1095/665298 [00:15<25:16, 437.93 examples/s]Map (num_proc=32):   0%|          | 1196/665298 [00:15<22:18, 496.33 examples/s]Map (num_proc=32):   0%|          | 1296/665298 [00:15<20:19, 544.57 examples/s]Map (num_proc=32):   0%|          | 1364/665298 [00:15<19:22, 571.32 examples/s]Map (num_proc=32):   0%|          | 1430/665298 [00:16<18:46, 589.32 examples/s]Map (num_proc=32):   0%|          | 1499/665298 [00:16<18:03, 612.63 examples/s]Map (num_proc=32):   0%|          | 1568/665298 [00:16<17:38, 627.12 examples/s]Map (num_proc=32):   0%|          | 1666/665298 [00:16<17:33, 629.91 examples/s]Map (num_proc=32):   0%|          | 1764/665298 [00:16<17:25, 634.74 examples/s]Map (num_proc=32):   0%|          | 1834/665298 [00:16<17:05, 647.13 examples/s]Map (num_proc=32):   0%|          | 1901/665298 [00:16<26:01, 424.86 examples/s]Map (num_proc=32):   0%|          | 2031/665298 [00:17<18:44, 589.93 examples/s]Map (num_proc=32):   0%|          | 2125/665298 [00:17<18:27, 598.88 examples/s]Map (num_proc=32):   0%|          | 2224/665298 [00:17<17:56, 615.74 examples/s]Map (num_proc=32):   0%|          | 2328/665298 [00:17<17:00, 649.69 examples/s]Map (num_proc=32):   0%|          | 2428/665298 [00:17<16:58, 651.07 examples/s]Map (num_proc=32):   0%|          | 2535/665298 [00:17<16:35, 665.80 examples/s]Map (num_proc=32):   0%|          | 2633/665298 [00:17<16:43, 660.60 examples/s]Map (num_proc=32):   0%|          | 2733/665298 [00:18<16:46, 658.42 examples/s]Map (num_proc=32):   0%|          | 2801/665298 [00:18<16:41, 661.31 examples/s]Map (num_proc=32):   0%|          | 2901/665298 [00:18<16:45, 658.80 examples/s]Map (num_proc=32):   0%|          | 2972/665298 [00:18<16:32, 667.66 examples/s]Map (num_proc=32):   0%|          | 3066/665298 [00:18<20:10, 546.95 examples/s]Map (num_proc=32):   0%|          | 3128/665298 [00:18<19:39, 561.18 examples/s]Map (num_proc=32):   0%|          | 3193/665298 [00:18<19:00, 580.38 examples/s]Map (num_proc=32):   0%|          | 3261/665298 [00:19<18:20, 601.70 examples/s]Map (num_proc=32):   1%|          | 3328/665298 [00:19<17:51, 618.04 examples/s]Map (num_proc=32):   1%|          | 3394/665298 [00:19<17:39, 624.54 examples/s]Map (num_proc=32):   1%|          | 3461/665298 [00:19<17:22, 634.98 examples/s]Map (num_proc=32):   1%|          | 3554/665298 [00:19<17:40, 624.25 examples/s]Map (num_proc=32):   1%|          | 3623/665298 [00:19<17:18, 637.34 examples/s]Map (num_proc=32):   1%|          | 3719/665298 [00:19<17:21, 635.21 examples/s]Map (num_proc=32):   1%|          | 3785/665298 [00:19<17:15, 639.08 examples/s]Map (num_proc=32):   1%|          | 3855/665298 [00:19<16:56, 650.91 examples/s]Map (num_proc=32):   1%|          | 3927/665298 [00:20<16:33, 665.76 examples/s]Map (num_proc=32):   1%|          | 4000/665298 [00:20<27:41, 397.94 examples/s]Map (num_proc=32):   1%|          | 4159/665298 [00:20<18:59, 580.11 examples/s]Map (num_proc=32):   1%|          | 4261/665298 [00:20<18:13, 604.36 examples/s]Map (num_proc=32):   1%|          | 4360/665298 [00:20<17:48, 618.79 examples/s]Map (num_proc=32):   1%|          | 4457/665298 [00:21<17:38, 624.36 examples/s]Map (num_proc=32):   1%|          | 4527/665298 [00:21<17:17, 636.62 examples/s]Map (num_proc=32):   1%|          | 4624/665298 [00:21<17:17, 636.76 examples/s]Map (num_proc=32):   1%|          | 4722/665298 [00:21<16:58, 648.81 examples/s]Map (num_proc=32):   1%|          | 4792/665298 [00:21<16:57, 648.91 examples/s]Map (num_proc=32):   1%|          | 4865/665298 [00:21<16:33, 664.67 examples/s]Map (num_proc=32):   1%|          | 4961/665298 [00:21<17:48, 618.18 examples/s]Map (num_proc=32):   1%|          | 5031/665298 [00:22<21:12, 519.06 examples/s]Map (num_proc=32):   1%|          | 5099/665298 [00:22<19:55, 552.19 examples/s]Map (num_proc=32):   1%|          | 5167/665298 [00:22<19:02, 577.84 examples/s]Map (num_proc=32):   1%|          | 5238/665298 [00:22<18:03, 609.26 examples/s]Map (num_proc=32):   1%|          | 5303/665298 [00:22<18:17, 601.60 examples/s]Map (num_proc=32):   1%|          | 5405/665298 [00:22<16:54, 650.39 examples/s]Map (num_proc=32):   1%|          | 5475/665298 [00:22<16:53, 651.17 examples/s]Map (num_proc=32):   1%|          | 5541/665298 [00:22<17:47, 618.33 examples/s]Map (num_proc=32):   1%|          | 5645/665298 [00:22<16:50, 652.52 examples/s]Map (num_proc=32):   1%|          | 5713/665298 [00:23<16:44, 656.60 examples/s]Map (num_proc=32):   1%|          | 5781/665298 [00:23<16:37, 660.98 examples/s]Map (num_proc=32):   1%|          | 5849/665298 [00:23<16:33, 663.49 examples/s]Map (num_proc=32):   1%|          | 5916/665298 [00:23<16:36, 661.38 examples/s]Map (num_proc=32):   1%|          | 6000/665298 [00:23<20:32, 534.91 examples/s]Map (num_proc=32):   1%|          | 6058/665298 [00:23<20:14, 542.70 examples/s]Map (num_proc=32):   1%|          | 6127/665298 [00:23<19:04, 576.07 examples/s]Map (num_proc=32):   1%|          | 6224/665298 [00:23<16:16, 675.25 examples/s]Map (num_proc=32):   1%|          | 6361/665298 [00:23<12:45, 861.26 examples/s]Map (num_proc=32):   1%|          | 6497/665298 [00:24<11:00, 997.23 examples/s]Map (num_proc=32):   1%|          | 6634/665298 [00:24<09:58, 1100.61 examples/s]Map (num_proc=32):   1%|          | 6763/665298 [00:24<15:03, 728.88 examples/s] Map (num_proc=32):   1%|          | 7049/665298 [00:24<09:33, 1148.77 examples/s]Map (num_proc=32):   1%|          | 7319/665298 [00:24<07:30, 1460.46 examples/s]Map (num_proc=32):   1%|          | 7521/665298 [00:24<07:53, 1387.76 examples/s]Map (num_proc=32):   1%|          | 7690/665298 [00:24<07:57, 1376.42 examples/s]Map (num_proc=32):   1%|          | 7846/665298 [00:25<08:12, 1334.69 examples/s]Map (num_proc=32):   1%|          | 8011/665298 [00:25<09:10, 1193.64 examples/s]Map (num_proc=32):   1%|          | 8143/665298 [00:25<10:01, 1093.20 examples/s]Map (num_proc=32):   1%|          | 8259/665298 [00:25<09:54, 1104.34 examples/s]Map (num_proc=32):   1%|▏         | 8399/665298 [00:25<09:20, 1172.49 examples/s]Map (num_proc=32):   1%|▏         | 8537/665298 [00:25<08:58, 1220.02 examples/s]Map (num_proc=32):   1%|▏         | 8671/665298 [00:25<08:47, 1245.74 examples/s]Map (num_proc=32):   1%|▏         | 8811/665298 [00:25<08:31, 1282.56 examples/s]Map (num_proc=32):   1%|▏         | 8948/665298 [00:26<08:24, 1302.03 examples/s]Map (num_proc=32):   1%|▏         | 9111/665298 [00:26<08:22, 1307.08 examples/s]Map (num_proc=32):   1%|▏         | 9252/665298 [00:26<08:13, 1328.10 examples/s]Map (num_proc=32):   1%|▏         | 9422/665298 [00:26<08:17, 1317.80 examples/s]Map (num_proc=32):   1%|▏         | 9589/665298 [00:26<08:15, 1323.36 examples/s]Map (num_proc=32):   1%|▏         | 9752/665298 [00:26<08:24, 1300.58 examples/s]Map (num_proc=32):   1%|▏         | 9890/665298 [00:26<10:09, 1075.82 examples/s]Map (num_proc=32):   2%|▏         | 10089/665298 [00:26<08:38, 1263.83 examples/s]Map (num_proc=32):   2%|▏         | 10231/665298 [00:27<09:27, 1154.53 examples/s]Map (num_proc=32):   2%|▏         | 10358/665298 [00:27<09:16, 1176.20 examples/s]Map (num_proc=32):   2%|▏         | 10486/665298 [00:27<09:07, 1196.22 examples/s]Map (num_proc=32):   2%|▏         | 10619/665298 [00:27<08:53, 1226.16 examples/s]Map (num_proc=32):   2%|▏         | 10758/665298 [00:27<08:36, 1266.91 examples/s]Map (num_proc=32):   2%|▏         | 10911/665298 [00:27<09:07, 1194.27 examples/s]Map (num_proc=32):   2%|▏         | 11044/665298 [00:27<08:53, 1226.56 examples/s]Map (num_proc=32):   2%|▏         | 11177/665298 [00:27<08:42, 1251.67 examples/s]Map (num_proc=32):   2%|▏         | 11322/665298 [00:27<08:22, 1300.16 examples/s]Map (num_proc=32):   2%|▏         | 11484/665298 [00:28<08:06, 1344.61 examples/s]Map (num_proc=32):   2%|▏         | 11622/665298 [00:28<08:03, 1352.97 examples/s]Map (num_proc=32):   2%|▏         | 11760/665298 [00:28<09:01, 1206.90 examples/s]Map (num_proc=32):   2%|▏         | 11913/665298 [00:28<09:07, 1193.43 examples/s]Map (num_proc=32):   2%|▏         | 12046/665298 [00:28<08:54, 1221.41 examples/s]Map (num_proc=32):   2%|▏         | 12200/665298 [00:28<09:28, 1148.32 examples/s]Map (num_proc=32):   2%|▏         | 12324/665298 [00:28<09:33, 1139.13 examples/s]Map (num_proc=32):   2%|▏         | 12459/665298 [00:28<09:08, 1190.16 examples/s]Map (num_proc=32):   2%|▏         | 12611/665298 [00:29<09:30, 1145.04 examples/s]Map (num_proc=32):   2%|▏         | 12746/665298 [00:29<09:07, 1192.21 examples/s]Map (num_proc=32):   2%|▏         | 12885/665298 [00:29<08:47, 1236.80 examples/s]Map (num_proc=32):   2%|▏         | 13027/665298 [00:29<08:28, 1282.61 examples/s]Map (num_proc=32):   2%|▏         | 13158/665298 [00:29<08:28, 1281.56 examples/s]Map (num_proc=32):   2%|▏         | 13297/665298 [00:29<08:19, 1304.49 examples/s]Map (num_proc=32):   2%|▏         | 13462/665298 [00:29<09:52, 1100.95 examples/s]Map (num_proc=32):   2%|▏         | 13660/665298 [00:29<08:27, 1284.58 examples/s]Map (num_proc=32):   2%|▏         | 13827/665298 [00:30<08:51, 1224.71 examples/s]Map (num_proc=32):   2%|▏         | 13982/665298 [00:30<08:42, 1247.67 examples/s]Map (num_proc=32):   2%|▏         | 14120/665298 [00:30<08:41, 1248.20 examples/s]Map (num_proc=32):   2%|▏         | 14257/665298 [00:30<09:19, 1163.33 examples/s]Map (num_proc=32):   2%|▏         | 14402/665298 [00:30<09:49, 1104.33 examples/s]Map (num_proc=32):   2%|▏         | 14575/665298 [00:30<08:47, 1233.41 examples/s]Map (num_proc=32):   2%|▏         | 14728/665298 [00:30<09:02, 1198.36 examples/s]Map (num_proc=32):   2%|▏         | 14898/665298 [00:30<08:31, 1270.91 examples/s]Map (num_proc=32):   2%|▏         | 15068/665298 [00:31<08:36, 1258.19 examples/s]Map (num_proc=32):   2%|▏         | 15229/665298 [00:31<08:21, 1295.30 examples/s]Map (num_proc=32):   2%|▏         | 15364/665298 [00:31<12:33, 862.81 examples/s] Map (num_proc=32):   2%|▏         | 15703/665298 [00:31<08:08, 1330.83 examples/s]Map (num_proc=32):   2%|▏         | 15886/665298 [00:31<08:19, 1301.35 examples/s]Map (num_proc=32):   2%|▏         | 16052/665298 [00:31<08:18, 1301.48 examples/s]Map (num_proc=32):   2%|▏         | 16217/665298 [00:31<08:18, 1301.58 examples/s]Map (num_proc=32):   2%|▏         | 16392/665298 [00:32<08:58, 1205.32 examples/s]Map (num_proc=32):   2%|▏         | 16522/665298 [00:32<08:56, 1209.07 examples/s]Map (num_proc=32):   3%|▎         | 16681/665298 [00:32<08:33, 1263.44 examples/s]Map (num_proc=32):   3%|▎         | 16878/665298 [00:32<07:42, 1401.58 examples/s]Map (num_proc=32):   3%|▎         | 17076/665298 [00:32<06:59, 1545.01 examples/s]Map (num_proc=32):   3%|▎         | 17274/665298 [00:32<06:30, 1658.73 examples/s]Map (num_proc=32):   3%|▎         | 17473/665298 [00:32<06:12, 1739.49 examples/s]Map (num_proc=32):   3%|▎         | 17667/665298 [00:32<06:02, 1788.66 examples/s]Map (num_proc=32):   3%|▎         | 17864/665298 [00:32<05:54, 1828.75 examples/s]Map (num_proc=32):   3%|▎         | 18069/665298 [00:33<05:43, 1885.42 examples/s]Map (num_proc=32):   3%|▎         | 18265/665298 [00:33<06:29, 1660.85 examples/s]Map (num_proc=32):   3%|▎         | 18457/665298 [00:33<07:06, 1517.73 examples/s]Map (num_proc=32):   3%|▎         | 18647/665298 [00:33<06:48, 1582.50 examples/s]Map (num_proc=32):   3%|▎         | 18842/665298 [00:33<06:26, 1671.55 examples/s]Map (num_proc=32):   3%|▎         | 19043/665298 [00:33<06:47, 1585.82 examples/s]Map (num_proc=32):   3%|▎         | 19239/665298 [00:33<06:25, 1674.46 examples/s]Map (num_proc=32):   3%|▎         | 19435/665298 [00:33<06:37, 1623.16 examples/s]Map (num_proc=32):   3%|▎         | 19626/665298 [00:34<06:25, 1674.10 examples/s]Map (num_proc=32):   3%|▎         | 19866/665298 [00:34<06:05, 1766.70 examples/s]Map (num_proc=32):   3%|▎         | 20066/665298 [00:34<05:54, 1819.04 examples/s]Map (num_proc=32):   3%|▎         | 20268/665298 [00:34<05:45, 1864.95 examples/s]Map (num_proc=32):   3%|▎         | 20473/665298 [00:34<05:41, 1885.87 examples/s]Map (num_proc=32):   3%|▎         | 20670/665298 [00:34<05:40, 1893.33 examples/s]Map (num_proc=32):   3%|▎         | 20889/665298 [00:35<10:33, 1017.94 examples/s]Map (num_proc=32):   3%|▎         | 21187/665298 [00:35<08:00, 1340.70 examples/s]Map (num_proc=32):   3%|▎         | 21513/665298 [00:35<06:16, 1708.64 examples/s]Map (num_proc=32):   3%|▎         | 21807/665298 [00:35<05:34, 1921.85 examples/s]Map (num_proc=32):   3%|▎         | 22064/665298 [00:35<05:21, 1997.84 examples/s]Map (num_proc=32):   3%|▎         | 22311/665298 [00:35<05:11, 2064.48 examples/s]Map (num_proc=32):   3%|▎         | 22565/665298 [00:35<05:00, 2142.03 examples/s]Map (num_proc=32):   3%|▎         | 22828/665298 [00:35<04:50, 2211.56 examples/s]Map (num_proc=32):   3%|▎         | 23093/665298 [00:35<05:00, 2139.42 examples/s]Map (num_proc=32):   4%|▎         | 23346/665298 [00:36<05:20, 2001.06 examples/s]Map (num_proc=32):   4%|▎         | 23584/665298 [00:36<05:11, 2060.40 examples/s]Map (num_proc=32):   4%|▎         | 23820/665298 [00:36<05:57, 1791.94 examples/s]Map (num_proc=32):   4%|▎         | 24151/665298 [00:36<05:08, 2081.27 examples/s]Map (num_proc=32):   4%|▎         | 24404/665298 [00:36<05:31, 1935.96 examples/s]Map (num_proc=32):   4%|▎         | 24636/665298 [00:36<05:35, 1907.41 examples/s]Map (num_proc=32):   4%|▎         | 24841/665298 [00:36<05:32, 1928.01 examples/s]Map (num_proc=32):   4%|▍         | 25061/665298 [00:36<05:22, 1985.23 examples/s]Map (num_proc=32):   4%|▍         | 25290/665298 [00:37<05:53, 1810.81 examples/s]Map (num_proc=32):   4%|▍         | 25483/665298 [00:37<05:51, 1821.80 examples/s]Map (num_proc=32):   4%|▍         | 25685/665298 [00:37<06:22, 1670.85 examples/s]Map (num_proc=32):   4%|▍         | 25885/665298 [00:37<06:06, 1744.43 examples/s]Map (num_proc=32):   4%|▍         | 26081/665298 [00:37<05:55, 1797.40 examples/s]Map (num_proc=32):   4%|▍         | 26284/665298 [00:37<05:45, 1847.13 examples/s]Map (num_proc=32):   4%|▍         | 26482/665298 [00:37<05:40, 1877.65 examples/s]Map (num_proc=32):   4%|▍         | 26688/665298 [00:37<05:32, 1920.98 examples/s]Map (num_proc=32):   4%|▍         | 26891/665298 [00:37<05:28, 1943.02 examples/s]Map (num_proc=32):   4%|▍         | 27095/665298 [00:38<05:25, 1958.98 examples/s]Map (num_proc=32):   4%|▍         | 27317/665298 [00:38<05:37, 1893.00 examples/s]Map (num_proc=32):   4%|▍         | 27508/665298 [00:38<05:37, 1887.91 examples/s]Map (num_proc=32):   4%|▍         | 27707/665298 [00:38<05:35, 1901.45 examples/s]Map (num_proc=32):   4%|▍         | 27900/665298 [00:38<05:34, 1906.44 examples/s]Map (num_proc=32):   4%|▍         | 28100/665298 [00:38<05:30, 1929.50 examples/s]Map (num_proc=32):   4%|▍         | 28316/665298 [00:38<06:07, 1733.59 examples/s]Map (num_proc=32):   4%|▍         | 28517/665298 [00:38<05:54, 1796.95 examples/s]Map (num_proc=32):   4%|▍         | 28716/665298 [00:39<06:34, 1614.78 examples/s]Map (num_proc=32):   4%|▍         | 28913/665298 [00:39<06:14, 1698.17 examples/s]Map (num_proc=32):   4%|▍         | 29112/665298 [00:39<07:43, 1373.12 examples/s]Map (num_proc=32):   4%|▍         | 29308/665298 [00:39<07:38, 1385.93 examples/s]Map (num_proc=32):   4%|▍         | 29547/665298 [00:39<06:34, 1613.05 examples/s]Map (num_proc=32):   4%|▍         | 29813/665298 [00:39<05:41, 1859.54 examples/s]Map (num_proc=32):   5%|▍         | 30078/665298 [00:39<05:13, 2029.45 examples/s]Map (num_proc=32):   5%|▍         | 30298/665298 [00:39<05:08, 2060.46 examples/s]Map (num_proc=32):   5%|▍         | 30537/665298 [00:39<05:16, 2008.67 examples/s]Map (num_proc=32):   5%|▍         | 30770/665298 [00:40<05:12, 2033.36 examples/s]Map (num_proc=32):   5%|▍         | 30999/665298 [00:40<05:31, 1913.13 examples/s]Map (num_proc=32):   5%|▍         | 31210/665298 [00:40<05:42, 1852.77 examples/s]Map (num_proc=32):   5%|▍         | 31401/665298 [00:40<05:44, 1838.82 examples/s]Map (num_proc=32):   5%|▍         | 31613/665298 [00:40<06:11, 1704.78 examples/s]Map (num_proc=32):   5%|▍         | 31805/665298 [00:40<06:03, 1742.72 examples/s]Map (num_proc=32):   5%|▍         | 32004/665298 [00:40<05:52, 1796.61 examples/s]Map (num_proc=32):   5%|▍         | 32194/665298 [00:40<05:51, 1802.77 examples/s]Map (num_proc=32):   5%|▍         | 32401/665298 [00:41<05:38, 1868.83 examples/s]Map (num_proc=32):   5%|▍         | 32602/665298 [00:41<05:31, 1908.38 examples/s]Map (num_proc=32):   5%|▍         | 32802/665298 [00:41<05:27, 1932.25 examples/s]Map (num_proc=32):   5%|▍         | 33037/665298 [00:41<05:14, 2012.51 examples/s]Map (num_proc=32):   5%|▍         | 33264/665298 [00:41<05:38, 1867.25 examples/s]Map (num_proc=32):   5%|▌         | 33456/665298 [00:41<05:49, 1806.92 examples/s]Map (num_proc=32):   5%|▌         | 33659/665298 [00:41<05:45, 1826.20 examples/s]Map (num_proc=32):   5%|▌         | 33899/665298 [00:41<05:26, 1936.19 examples/s]Map (num_proc=32):   5%|▌         | 34107/665298 [00:41<05:20, 1972.29 examples/s]Map (num_proc=32):   5%|▌         | 34321/665298 [00:42<05:49, 1806.52 examples/s]Map (num_proc=32):   5%|▌         | 34537/665298 [00:42<06:22, 1648.70 examples/s]Map (num_proc=32):   5%|▌         | 34733/665298 [00:42<09:32, 1101.25 examples/s]Map (num_proc=32):   5%|▌         | 35028/665298 [00:42<07:22, 1423.91 examples/s]Map (num_proc=32):   5%|▌         | 35398/665298 [00:42<05:34, 1882.06 examples/s]Map (num_proc=32):   5%|▌         | 35798/665298 [00:42<04:29, 2335.31 examples/s]Map (num_proc=32):   5%|▌         | 36226/665298 [00:42<03:51, 2713.42 examples/s]Map (num_proc=32):   5%|▌         | 36566/665298 [00:43<03:38, 2873.63 examples/s]Map (num_proc=32):   6%|▌         | 36960/665298 [00:43<03:23, 3087.32 examples/s]Map (num_proc=32):   6%|▌         | 37318/665298 [00:43<04:07, 2538.77 examples/s]Map (num_proc=32):   6%|▌         | 37612/665298 [00:43<04:00, 2606.85 examples/s]Map (num_proc=32):   6%|▌         | 37914/665298 [00:43<03:52, 2696.29 examples/s]Map (num_proc=32):   6%|▌         | 38279/665298 [00:43<03:49, 2727.91 examples/s]Map (num_proc=32):   6%|▌         | 38641/665298 [00:43<03:36, 2897.88 examples/s]Map (num_proc=32):   6%|▌         | 39041/665298 [00:43<03:17, 3164.60 examples/s]Map (num_proc=32):   6%|▌         | 39415/665298 [00:44<03:15, 3207.01 examples/s]Map (num_proc=32):   6%|▌         | 39831/665298 [00:44<03:01, 3441.86 examples/s]Map (num_proc=32):   6%|▌         | 40223/665298 [00:44<02:59, 3491.22 examples/s]Map (num_proc=32):   6%|▌         | 40588/665298 [00:44<03:01, 3450.93 examples/s]Map (num_proc=32):   6%|▌         | 40966/665298 [00:44<02:59, 3469.19 examples/s]Map (num_proc=32):   6%|▌         | 41337/665298 [00:44<02:58, 3491.83 examples/s]Map (num_proc=32):   6%|▋         | 41782/665298 [00:44<02:49, 3671.36 examples/s]Map (num_proc=32):   6%|▋         | 42166/665298 [00:44<02:48, 3690.59 examples/s]Map (num_proc=32):   6%|▋         | 42564/665298 [00:44<02:48, 3687.19 examples/s]Map (num_proc=32):   6%|▋         | 43002/665298 [00:45<02:49, 3676.84 examples/s]Map (num_proc=32):   7%|▋         | 43373/665298 [00:45<03:01, 3421.47 examples/s]Map (num_proc=32):   7%|▋         | 43734/665298 [00:45<03:11, 3240.01 examples/s]Map (num_proc=32):   7%|▋         | 44090/665298 [00:45<03:19, 3117.78 examples/s]Map (num_proc=32):   7%|▋         | 44422/665298 [00:45<03:19, 3110.17 examples/s]Map (num_proc=32):   7%|▋         | 44797/665298 [00:45<03:11, 3237.49 examples/s]Map (num_proc=32):   7%|▋         | 45196/665298 [00:45<03:03, 3371.70 examples/s]Map (num_proc=32):   7%|▋         | 45566/665298 [00:45<03:00, 3425.34 examples/s]Map (num_proc=32):   7%|▋         | 45932/665298 [00:45<03:23, 3044.61 examples/s]Map (num_proc=32):   7%|▋         | 46411/665298 [00:46<02:58, 3469.36 examples/s]Map (num_proc=32):   7%|▋         | 46775/665298 [00:46<03:00, 3432.80 examples/s]Map (num_proc=32):   7%|▋         | 47138/665298 [00:46<03:01, 3410.56 examples/s]Map (num_proc=32):   7%|▋         | 47502/665298 [00:46<03:02, 3381.35 examples/s]Map (num_proc=32):   7%|▋         | 48820/665298 [00:46<01:42, 6012.64 examples/s]Map (num_proc=32):   7%|▋         | 49470/665298 [00:46<02:07, 4815.97 examples/s]Map (num_proc=32):   8%|▊         | 50014/665298 [00:46<02:19, 4406.91 examples/s]Map (num_proc=32):   8%|▊         | 50553/665298 [00:47<02:32, 4022.14 examples/s]Map (num_proc=32):   8%|▊         | 51011/665298 [00:47<02:44, 3723.74 examples/s]Map (num_proc=32):   8%|▊         | 51433/665298 [00:47<02:52, 3555.75 examples/s]Map (num_proc=32):   8%|▊         | 51803/665298 [00:47<02:51, 3568.14 examples/s]Map (num_proc=32):   8%|▊         | 52200/665298 [00:47<02:47, 3652.74 examples/s]Map (num_proc=32):   8%|▊         | 52593/665298 [00:47<03:09, 3225.02 examples/s]Map (num_proc=32):   8%|▊         | 52969/665298 [00:47<03:10, 3212.52 examples/s]Map (num_proc=32):   8%|▊         | 53330/665298 [00:47<03:21, 3035.50 examples/s]Map (num_proc=32):   8%|▊         | 53708/665298 [00:48<03:16, 3105.13 examples/s]Map (num_proc=32):   8%|▊         | 54032/665298 [00:48<03:24, 2992.85 examples/s]Map (num_proc=32):   8%|▊         | 54337/665298 [00:48<06:34, 1548.54 examples/s]Map (num_proc=32):   8%|▊         | 54659/665298 [00:48<06:04, 1674.18 examples/s]Map (num_proc=32):   8%|▊         | 54891/665298 [00:50<23:44, 428.62 examples/s] Map (num_proc=32):   8%|▊         | 55064/665298 [00:50<22:23, 454.05 examples/s]Map (num_proc=32):   8%|▊         | 55289/665298 [00:51<17:42, 573.97 examples/s]Map (num_proc=32):   8%|▊         | 55641/665298 [00:51<12:13, 831.56 examples/s]Map (num_proc=32):   8%|▊         | 56018/665298 [00:51<08:51, 1146.38 examples/s]Map (num_proc=32):   8%|▊         | 56401/665298 [00:51<06:45, 1502.81 examples/s]Map (num_proc=32):   9%|▊         | 56843/665298 [00:51<05:09, 1963.45 examples/s]Map (num_proc=32):   9%|▊         | 57183/665298 [00:51<04:52, 2082.27 examples/s]Map (num_proc=32):   9%|▊         | 57575/665298 [00:51<04:14, 2385.39 examples/s]Map (num_proc=32):   9%|▊         | 57933/665298 [00:51<03:53, 2600.79 examples/s]Map (num_proc=32):   9%|▉         | 58369/665298 [00:51<03:23, 2976.71 examples/s]Map (num_proc=32):   9%|▉         | 58823/665298 [00:52<03:05, 3261.37 examples/s]Map (num_proc=32):   9%|▉         | 59265/665298 [00:52<03:00, 3353.82 examples/s]Map (num_proc=32):   9%|▉         | 59727/665298 [00:52<02:46, 3628.59 examples/s]Map (num_proc=32):   9%|▉         | 60135/665298 [00:52<03:05, 3267.77 examples/s]Map (num_proc=32):   9%|▉         | 60522/665298 [00:52<03:11, 3159.64 examples/s]Map (num_proc=32):   9%|▉         | 60938/665298 [00:52<02:59, 3375.80 examples/s]Map (num_proc=32):   9%|▉         | 61419/665298 [00:52<02:43, 3704.07 examples/s]Map (num_proc=32):   9%|▉         | 61823/665298 [00:52<02:41, 3726.93 examples/s]Map (num_proc=32):   9%|▉         | 62251/665298 [00:53<02:37, 3829.15 examples/s]Map (num_proc=32):   9%|▉         | 62729/665298 [00:53<02:31, 3976.15 examples/s]Map (num_proc=32):   9%|▉         | 63200/665298 [00:53<02:33, 3915.08 examples/s]Map (num_proc=32):  10%|▉         | 63667/665298 [00:53<02:33, 3920.51 examples/s]Map (num_proc=32):  10%|▉         | 64259/665298 [00:53<02:17, 4358.43 examples/s]Map (num_proc=32):  10%|▉         | 64747/665298 [00:53<02:18, 4335.66 examples/s]Map (num_proc=32):  10%|▉         | 65274/665298 [00:53<02:13, 4484.64 examples/s]Map (num_proc=32):  10%|▉         | 65790/665298 [00:53<02:13, 4500.46 examples/s]Map (num_proc=32):  10%|▉         | 66264/665298 [00:53<02:12, 4512.35 examples/s]Map (num_proc=32):  10%|█         | 66784/665298 [00:54<03:40, 2710.42 examples/s]Map (num_proc=32):  10%|█         | 67239/665298 [00:54<03:20, 2980.72 examples/s]Map (num_proc=32):  10%|█         | 67670/665298 [00:54<03:07, 3189.32 examples/s]Map (num_proc=32):  10%|█         | 68148/665298 [00:54<02:51, 3481.35 examples/s]Map (num_proc=32):  10%|█         | 68610/665298 [00:54<02:42, 3662.82 examples/s]Map (num_proc=32):  10%|█         | 69049/665298 [00:54<02:36, 3805.67 examples/s]Map (num_proc=32):  10%|█         | 69489/665298 [00:54<02:42, 3668.61 examples/s]Map (num_proc=32):  11%|█         | 69929/665298 [00:55<02:37, 3780.28 examples/s]Map (num_proc=32):  11%|█         | 70411/665298 [00:55<02:27, 4024.57 examples/s]Map (num_proc=32):  11%|█         | 70858/665298 [00:55<02:24, 4114.93 examples/s]Map (num_proc=32):  11%|█         | 71343/665298 [00:55<02:24, 4112.66 examples/s]Map (num_proc=32):  11%|█         | 71764/665298 [00:55<02:26, 4051.32 examples/s]Map (num_proc=32):  11%|█         | 72198/665298 [00:55<02:29, 3972.81 examples/s]Map (num_proc=32):  11%|█         | 72633/665298 [00:55<02:32, 3890.38 examples/s]Map (num_proc=32):  11%|█         | 73066/665298 [00:55<02:35, 3796.80 examples/s]Map (num_proc=32):  11%|█         | 73468/665298 [00:55<02:45, 3586.37 examples/s]Map (num_proc=32):  11%|█         | 74043/665298 [00:56<02:25, 4065.52 examples/s]Map (num_proc=32):  11%|█         | 74518/665298 [00:56<02:21, 4185.57 examples/s]Map (num_proc=32):  11%|█▏        | 74958/665298 [00:56<02:23, 4125.00 examples/s]Map (num_proc=32):  11%|█▏        | 75418/665298 [00:56<02:20, 4197.16 examples/s]Map (num_proc=32):  11%|█▏        | 75847/665298 [00:56<02:23, 4115.25 examples/s]Map (num_proc=32):  11%|█▏        | 76358/665298 [00:56<02:19, 4229.22 examples/s]Map (num_proc=32):  12%|█▏        | 76788/665298 [00:56<02:21, 4156.90 examples/s]Map (num_proc=32):  12%|█▏        | 77268/665298 [00:56<02:35, 3790.15 examples/s]Map (num_proc=32):  12%|█▏        | 77717/665298 [00:57<02:43, 3597.75 examples/s]Map (num_proc=32):  12%|█▏        | 78208/665298 [00:57<02:34, 3792.97 examples/s]Map (num_proc=32):  12%|█▏        | 78623/665298 [00:57<03:17, 2971.53 examples/s]Map (num_proc=32):  12%|█▏        | 79016/665298 [00:57<03:04, 3173.95 examples/s]Map (num_proc=32):  12%|█▏        | 82416/665298 [00:58<02:03, 4711.73 examples/s]Map (num_proc=32):  13%|█▎        | 84021/665298 [00:58<01:34, 6152.78 examples/s]Map (num_proc=32):  13%|█▎        | 84769/665298 [00:58<01:58, 4880.68 examples/s]Map (num_proc=32):  13%|█▎        | 85372/665298 [00:58<02:15, 4274.59 examples/s]Map (num_proc=32):  13%|█▎        | 85889/665298 [00:58<02:31, 3829.25 examples/s]Map (num_proc=32):  13%|█▎        | 86336/665298 [00:59<02:51, 3381.60 examples/s]Map (num_proc=32):  13%|█▎        | 86731/665298 [00:59<03:12, 3010.99 examples/s]Map (num_proc=32):  13%|█▎        | 87067/665298 [00:59<03:40, 2622.30 examples/s]Map (num_proc=32):  13%|█▎        | 87380/665298 [00:59<03:51, 2493.16 examples/s]Map (num_proc=32):  13%|█▎        | 87702/665298 [00:59<03:57, 2435.54 examples/s]Map (num_proc=32):  13%|█▎        | 87994/665298 [01:00<06:15, 1538.68 examples/s]Map (num_proc=32):  13%|█▎        | 88313/665298 [01:00<05:29, 1750.49 examples/s]Map (num_proc=32):  13%|█▎        | 88606/665298 [01:01<16:05, 596.99 examples/s] Map (num_proc=32):  14%|█▍        | 92071/665298 [01:01<03:26, 2780.43 examples/s]Map (num_proc=32):  14%|█▍        | 92752/665298 [01:02<03:11, 2997.12 examples/s]Map (num_proc=32):  14%|█▍        | 93373/665298 [01:02<03:08, 3035.10 examples/s]Map (num_proc=32):  14%|█▍        | 93910/665298 [01:02<03:12, 2965.82 examples/s]Map (num_proc=32):  14%|█▍        | 94365/665298 [01:02<03:11, 2983.95 examples/s]Map (num_proc=32):  14%|█▍        | 94803/665298 [01:02<03:11, 2971.66 examples/s]Map (num_proc=32):  14%|█▍        | 95178/665298 [01:02<03:17, 2882.84 examples/s]Map (num_proc=32):  14%|█▍        | 95553/665298 [01:03<03:11, 2981.42 examples/s]Map (num_proc=32):  14%|█▍        | 95938/665298 [01:03<04:40, 2026.82 examples/s]Map (num_proc=32):  15%|█▍        | 96491/665298 [01:03<03:43, 2547.11 examples/s]Map (num_proc=32):  15%|█▍        | 96848/665298 [01:03<04:11, 2256.86 examples/s]Map (num_proc=32):  15%|█▍        | 97152/665298 [01:05<16:07, 587.23 examples/s] Map (num_proc=32):  15%|█▍        | 97599/665298 [01:05<11:40, 810.52 examples/s]Map (num_proc=32):  15%|█▍        | 98293/665298 [01:05<07:24, 1275.69 examples/s]Map (num_proc=32):  15%|█▍        | 99049/665298 [01:05<05:01, 1876.51 examples/s]Map (num_proc=32):  15%|█▍        | 99637/665298 [01:06<04:00, 2355.91 examples/s]Map (num_proc=32):  15%|█▌        | 100263/665298 [01:06<03:13, 2919.20 examples/s]Map (num_proc=32):  15%|█▌        | 100894/665298 [01:06<02:43, 3458.53 examples/s]Map (num_proc=32):  15%|█▌        | 101510/665298 [01:06<02:38, 3554.33 examples/s]Map (num_proc=32):  15%|█▌        | 102058/665298 [01:06<02:53, 3244.59 examples/s]Map (num_proc=32):  15%|█▌        | 102541/665298 [01:06<02:56, 3192.20 examples/s]Map (num_proc=32):  15%|█▌        | 102992/665298 [01:06<03:13, 2907.73 examples/s]Map (num_proc=32):  16%|█▌        | 103398/665298 [01:07<03:20, 2797.74 examples/s]Map (num_proc=32):  16%|█▌        | 103775/665298 [01:07<03:26, 2717.33 examples/s]Map (num_proc=32):  16%|█▌        | 104135/665298 [01:07<05:08, 1820.34 examples/s]Map (num_proc=32):  16%|█▌        | 104834/665298 [01:07<03:36, 2582.97 examples/s]Map (num_proc=32):  16%|█▌        | 105404/665298 [01:07<03:05, 3021.85 examples/s]Map (num_proc=32):  16%|█▌        | 105854/665298 [01:08<03:13, 2889.15 examples/s]Map (num_proc=32):  16%|█▌        | 106208/665298 [01:08<03:13, 2890.65 examples/s]Map (num_proc=32):  16%|█▌        | 106557/665298 [01:08<03:16, 2844.21 examples/s]Map (num_proc=32):  16%|█▌        | 106930/665298 [01:08<03:24, 2729.62 examples/s]Map (num_proc=32):  16%|█▌        | 107263/665298 [01:08<03:30, 2644.74 examples/s]Map (num_proc=32):  16%|█▌        | 107565/665298 [01:08<03:43, 2500.39 examples/s]Map (num_proc=32):  16%|█▌        | 107922/665298 [01:08<03:28, 2674.78 examples/s]Map (num_proc=32):  16%|█▋        | 108201/665298 [01:08<03:29, 2654.77 examples/s]Map (num_proc=32):  16%|█▋        | 108492/665298 [01:09<03:25, 2711.66 examples/s]Map (num_proc=32):  16%|█▋        | 108784/665298 [01:09<03:22, 2751.65 examples/s]Map (num_proc=32):  16%|█▋        | 109080/665298 [01:09<03:38, 2545.00 examples/s]Map (num_proc=32):  16%|█▋        | 109369/665298 [01:09<03:31, 2632.48 examples/s]Map (num_proc=32):  16%|█▋        | 109645/665298 [01:09<03:41, 2507.37 examples/s]Map (num_proc=32):  17%|█▋        | 109934/665298 [01:09<03:33, 2603.08 examples/s]Map (num_proc=32):  17%|█▋        | 110295/665298 [01:09<03:34, 2590.46 examples/s]Map (num_proc=32):  17%|█▋        | 110584/665298 [01:09<03:53, 2377.32 examples/s]Map (num_proc=32):  17%|█▋        | 110874/665298 [01:10<04:40, 1977.13 examples/s]Map (num_proc=32):  17%|█▋        | 111131/665298 [01:11<12:47, 721.90 examples/s] Map (num_proc=32):  17%|█▋        | 112023/665298 [01:11<06:17, 1466.41 examples/s]Map (num_proc=32):  17%|█▋        | 112360/665298 [01:11<06:25, 1434.77 examples/s]Map (num_proc=32):  17%|█▋        | 112650/665298 [01:11<06:24, 1437.17 examples/s]Map (num_proc=32):  17%|█▋        | 112938/665298 [01:11<06:24, 1436.62 examples/s]Map (num_proc=32):  17%|█▋        | 113166/665298 [01:12<07:00, 1314.53 examples/s]Map (num_proc=32):  17%|█▋        | 113362/665298 [01:12<07:00, 1311.67 examples/s]Map (num_proc=32):  17%|█▋        | 113580/665298 [01:12<06:50, 1342.66 examples/s]Map (num_proc=32):  17%|█▋        | 113801/665298 [01:12<06:41, 1373.28 examples/s]Map (num_proc=32):  17%|█▋        | 113956/665298 [01:12<07:09, 1282.78 examples/s]Map (num_proc=32):  17%|█▋        | 114158/665298 [01:12<07:04, 1298.25 examples/s]Map (num_proc=32):  17%|█▋        | 114307/665298 [01:12<06:52, 1335.72 examples/s]Map (num_proc=32):  17%|█▋        | 114455/665298 [01:13<06:43, 1365.18 examples/s]Map (num_proc=32):  17%|█▋        | 114600/665298 [01:13<06:37, 1384.48 examples/s]Map (num_proc=32):  17%|█▋        | 114743/665298 [01:13<06:34, 1395.55 examples/s]Map (num_proc=32):  17%|█▋        | 114944/665298 [01:13<06:41, 1372.20 examples/s]Map (num_proc=32):  17%|█▋        | 115096/665298 [01:13<07:22, 1242.02 examples/s]Map (num_proc=32):  17%|█▋        | 115238/665298 [01:13<07:08, 1282.74 examples/s]Map (num_proc=32):  17%|█▋        | 115379/665298 [01:13<06:58, 1313.28 examples/s]Map (num_proc=32):  17%|█▋        | 115522/665298 [01:13<07:33, 1211.35 examples/s]Map (num_proc=32):  17%|█▋        | 115738/665298 [01:14<06:41, 1367.08 examples/s]Map (num_proc=32):  17%|█▋        | 115955/665298 [01:14<07:13, 1268.07 examples/s]Map (num_proc=32):  17%|█▋        | 116159/665298 [01:14<07:04, 1293.61 examples/s]Map (num_proc=32):  17%|█▋        | 116299/665298 [01:14<06:58, 1313.33 examples/s]Map (num_proc=32):  18%|█▊        | 116516/665298 [01:14<06:39, 1372.97 examples/s]Map (num_proc=32):  18%|█▊        | 116661/665298 [01:14<06:35, 1387.56 examples/s]Map (num_proc=32):  18%|█▊        | 116805/665298 [01:14<06:52, 1329.82 examples/s]Map (num_proc=32):  18%|█▊        | 116955/665298 [01:15<07:32, 1212.54 examples/s]Map (num_proc=32):  18%|█▊        | 117090/665298 [01:15<07:23, 1235.76 examples/s]Map (num_proc=32):  18%|█▊        | 117299/665298 [01:15<07:03, 1295.40 examples/s]Map (num_proc=32):  18%|█▊        | 117514/665298 [01:15<06:44, 1353.31 examples/s]Map (num_proc=32):  18%|█▊        | 117655/665298 [01:15<06:46, 1346.46 examples/s]Map (num_proc=32):  18%|█▊        | 117792/665298 [01:16<14:14, 640.63 examples/s] Map (num_proc=32):  18%|█▊        | 118231/665298 [01:16<08:12, 1110.06 examples/s]Map (num_proc=32):  18%|█▊        | 118444/665298 [01:16<07:52, 1157.88 examples/s]Map (num_proc=32):  18%|█▊        | 118663/665298 [01:16<07:20, 1239.56 examples/s]Map (num_proc=32):  18%|█▊        | 118879/665298 [01:16<07:04, 1287.39 examples/s]Map (num_proc=32):  18%|█▊        | 119096/665298 [01:16<06:17, 1447.87 examples/s]Map (num_proc=32):  18%|█▊        | 119385/665298 [01:16<05:10, 1760.55 examples/s]Map (num_proc=32):  18%|█▊        | 119677/665298 [01:16<04:33, 1996.91 examples/s]Map (num_proc=32):  18%|█▊        | 120027/665298 [01:17<03:56, 2302.76 examples/s]Map (num_proc=32):  18%|█▊        | 120381/665298 [01:17<03:45, 2421.29 examples/s]Map (num_proc=32):  18%|█▊        | 120660/665298 [01:17<03:36, 2514.23 examples/s]Map (num_proc=32):  18%|█▊        | 120959/665298 [01:17<03:42, 2447.40 examples/s]Map (num_proc=32):  18%|█▊        | 121235/665298 [01:17<03:55, 2314.99 examples/s]Map (num_proc=32):  18%|█▊        | 121591/665298 [01:17<03:33, 2544.63 examples/s]Map (num_proc=32):  18%|█▊        | 121867/665298 [01:17<03:30, 2583.71 examples/s]Map (num_proc=32):  18%|█▊        | 122157/665298 [01:17<03:24, 2660.99 examples/s]Map (num_proc=32):  18%|█▊        | 122444/665298 [01:18<03:22, 2675.86 examples/s]Map (num_proc=32):  18%|█▊        | 122802/665298 [01:18<03:23, 2663.48 examples/s]Map (num_proc=32):  19%|█▊        | 123083/665298 [01:18<03:40, 2454.19 examples/s]Map (num_proc=32):  19%|█▊        | 123364/665298 [01:18<03:35, 2512.02 examples/s]Map (num_proc=32):  19%|█▊        | 123718/665298 [01:18<03:35, 2511.28 examples/s]Map (num_proc=32):  19%|█▊        | 124005/665298 [01:18<03:28, 2591.51 examples/s]Map (num_proc=32):  19%|█▊        | 124366/665298 [01:18<03:09, 2852.30 examples/s]Map (num_proc=32):  19%|█▊        | 124661/665298 [01:18<03:10, 2834.24 examples/s]Map (num_proc=32):  19%|█▉        | 124966/665298 [01:19<03:37, 2480.83 examples/s]Map (num_proc=32):  19%|█▉        | 125227/665298 [01:19<03:35, 2510.08 examples/s]Map (num_proc=32):  19%|█▉        | 125495/665298 [01:19<03:31, 2550.53 examples/s]Map (num_proc=32):  19%|█▉        | 125780/665298 [01:19<03:25, 2626.20 examples/s]Map (num_proc=32):  19%|█▉        | 126137/665298 [01:19<03:17, 2734.47 examples/s]Map (num_proc=32):  19%|█▉        | 126426/665298 [01:19<03:17, 2731.74 examples/s]Map (num_proc=32):  19%|█▉        | 126781/665298 [01:19<03:14, 2765.89 examples/s]Map (num_proc=32):  19%|█▉        | 127091/665298 [01:19<03:41, 2425.83 examples/s]Map (num_proc=32):  19%|█▉        | 127366/665298 [01:19<03:34, 2503.44 examples/s]Map (num_proc=32):  19%|█▉        | 127637/665298 [01:20<03:33, 2519.75 examples/s]Map (num_proc=32):  19%|█▉        | 127918/665298 [01:20<06:46, 1321.74 examples/s]Map (num_proc=32):  19%|█▉        | 128637/665298 [01:20<03:58, 2249.41 examples/s]Map (num_proc=32):  19%|█▉        | 129230/665298 [01:20<03:03, 2914.77 examples/s]Map (num_proc=32):  20%|█▉        | 129734/665298 [01:20<02:50, 3138.32 examples/s]Map (num_proc=32):  20%|█▉        | 130175/665298 [01:20<02:53, 3080.53 examples/s]Map (num_proc=32):  20%|█▉        | 130603/665298 [01:21<02:59, 2975.84 examples/s]Map (num_proc=32):  20%|█▉        | 130956/665298 [01:21<04:08, 2153.52 examples/s]Map (num_proc=32):  20%|█▉        | 131239/665298 [01:22<12:07, 734.43 examples/s] Map (num_proc=32):  20%|█▉        | 132242/665298 [01:22<06:20, 1399.50 examples/s]Map (num_proc=32):  20%|█▉        | 132660/665298 [01:23<06:21, 1394.77 examples/s]Map (num_proc=32):  20%|█▉        | 132956/665298 [01:23<06:36, 1342.24 examples/s]Map (num_proc=32):  20%|██        | 133252/665298 [01:23<06:28, 1368.50 examples/s]Map (num_proc=32):  20%|██        | 133466/665298 [01:23<06:25, 1378.52 examples/s]Map (num_proc=32):  20%|██        | 133692/665298 [01:23<06:18, 1403.13 examples/s]Map (num_proc=32):  20%|██        | 133878/665298 [01:24<06:57, 1272.64 examples/s]Map (num_proc=32):  20%|██        | 134093/665298 [01:24<06:46, 1307.96 examples/s]Map (num_proc=32):  20%|██        | 134310/665298 [01:24<06:36, 1339.17 examples/s]Map (num_proc=32):  20%|██        | 134524/665298 [01:24<06:30, 1360.25 examples/s]Map (num_proc=32):  20%|██        | 134743/665298 [01:24<06:23, 1384.81 examples/s]Map (num_proc=32):  20%|██        | 134892/665298 [01:24<06:57, 1271.94 examples/s]Map (num_proc=32):  20%|██        | 135035/665298 [01:24<06:46, 1303.15 examples/s]Map (num_proc=32):  20%|██        | 135180/665298 [01:25<06:37, 1333.26 examples/s]Map (num_proc=32):  20%|██        | 135324/665298 [01:25<06:30, 1357.26 examples/s]Map (num_proc=32):  20%|██        | 135469/665298 [01:25<06:24, 1379.66 examples/s]Map (num_proc=32):  20%|██        | 135612/665298 [01:25<06:21, 1390.04 examples/s]Map (num_proc=32):  20%|██        | 135815/665298 [01:25<07:02, 1253.10 examples/s]Map (num_proc=32):  20%|██        | 135957/665298 [01:25<06:55, 1275.44 examples/s]Map (num_proc=32):  20%|██        | 136095/665298 [01:25<06:46, 1300.93 examples/s]Map (num_proc=32):  20%|██        | 136240/665298 [01:25<06:35, 1336.81 examples/s]Map (num_proc=32):  21%|██        | 136454/665298 [01:26<06:26, 1367.13 examples/s]Map (num_proc=32):  21%|██        | 136603/665298 [01:26<06:18, 1396.56 examples/s]Map (num_proc=32):  21%|██        | 136745/665298 [01:26<06:17, 1401.05 examples/s]Map (num_proc=32):  21%|██        | 136938/665298 [01:26<07:18, 1204.76 examples/s]Map (num_proc=32):  21%|██        | 137148/665298 [01:26<06:53, 1276.03 examples/s]Map (num_proc=32):  21%|██        | 137289/665298 [01:26<06:44, 1305.82 examples/s]Map (num_proc=32):  21%|██        | 137430/665298 [01:26<06:36, 1330.09 examples/s]Map (num_proc=32):  21%|██        | 137574/665298 [01:26<06:29, 1355.37 examples/s]Map (num_proc=32):  21%|██        | 137715/665298 [01:26<06:30, 1349.47 examples/s]Map (num_proc=32):  21%|██        | 137878/665298 [01:27<07:21, 1195.70 examples/s]Map (num_proc=32):  21%|██        | 138020/665298 [01:27<07:02, 1248.90 examples/s]Map (num_proc=32):  21%|██        | 138164/665298 [01:27<06:46, 1296.82 examples/s]Map (num_proc=32):  21%|██        | 138308/665298 [01:27<06:35, 1331.98 examples/s]Map (num_proc=32):  21%|██        | 138504/665298 [01:27<06:34, 1334.44 examples/s]Map (num_proc=32):  21%|██        | 138652/665298 [01:27<09:16, 946.70 examples/s] Map (num_proc=32):  21%|██        | 138811/665298 [01:28<09:09, 958.17 examples/s]Map (num_proc=32):  21%|██        | 138946/665298 [01:28<08:27, 1037.44 examples/s]Map (num_proc=32):  21%|██        | 139146/665298 [01:28<07:04, 1240.70 examples/s]Map (num_proc=32):  21%|██        | 139434/665298 [01:28<05:21, 1633.42 examples/s]Map (num_proc=32):  21%|██        | 139721/665298 [01:28<04:29, 1948.74 examples/s]Map (num_proc=32):  21%|██        | 140004/665298 [01:28<04:00, 2182.20 examples/s]Map (num_proc=32):  21%|██        | 140302/665298 [01:28<05:23, 1624.54 examples/s]Map (num_proc=32):  21%|██        | 140942/665298 [01:28<03:22, 2583.13 examples/s]Map (num_proc=32):  21%|██        | 141300/665298 [01:29<03:19, 2621.91 examples/s]Map (num_proc=32):  21%|██▏       | 141660/665298 [01:29<03:19, 2628.03 examples/s]Map (num_proc=32):  21%|██▏       | 142012/665298 [01:29<03:11, 2733.73 examples/s]Map (num_proc=32):  21%|██▏       | 142357/665298 [01:29<03:12, 2715.39 examples/s]Map (num_proc=32):  21%|██▏       | 142649/665298 [01:29<03:21, 2596.94 examples/s]Map (num_proc=32):  21%|██▏       | 142930/665298 [01:29<03:20, 2606.55 examples/s]Map (num_proc=32):  22%|██▏       | 143210/665298 [01:29<03:33, 2450.59 examples/s]Map (num_proc=32):  22%|██▏       | 143496/665298 [01:29<03:26, 2526.41 examples/s]Map (num_proc=32):  22%|██▏       | 143790/665298 [01:30<03:21, 2592.72 examples/s]Map (num_proc=32):  22%|██▏       | 144138/665298 [01:30<03:09, 2745.63 examples/s]Map (num_proc=32):  22%|██▏       | 144508/665298 [01:30<03:11, 2712.47 examples/s]Map (num_proc=32):  22%|██▏       | 144807/665298 [01:30<03:15, 2666.89 examples/s]Map (num_proc=32):  22%|██▏       | 145099/665298 [01:30<03:35, 2410.09 examples/s]Map (num_proc=32):  22%|██▏       | 145382/665298 [01:30<03:27, 2508.73 examples/s]Map (num_proc=32):  22%|██▏       | 145669/665298 [01:30<03:20, 2594.99 examples/s]Map (num_proc=32):  22%|██▏       | 146033/665298 [01:30<03:03, 2837.20 examples/s]Map (num_proc=32):  22%|██▏       | 146384/665298 [01:31<03:14, 2664.82 examples/s]Map (num_proc=32):  22%|██▏       | 146662/665298 [01:31<03:12, 2689.61 examples/s]Map (num_proc=32):  22%|██▏       | 146939/665298 [01:31<03:13, 2683.16 examples/s]Map (num_proc=32):  22%|██▏       | 147234/665298 [01:31<03:25, 2521.36 examples/s]Map (num_proc=32):  22%|██▏       | 147519/665298 [01:31<03:21, 2570.34 examples/s]Map (num_proc=32):  22%|██▏       | 147879/665298 [01:31<03:12, 2689.16 examples/s]Map (num_proc=32):  22%|██▏       | 148155/665298 [01:31<03:11, 2706.06 examples/s]Map (num_proc=32):  22%|██▏       | 148444/665298 [01:31<03:21, 2567.48 examples/s]Map (num_proc=32):  22%|██▏       | 148733/665298 [01:31<03:17, 2612.92 examples/s]Map (num_proc=32):  22%|██▏       | 148998/665298 [01:32<03:17, 2615.71 examples/s]Map (num_proc=32):  22%|██▏       | 149313/665298 [01:32<03:31, 2442.92 examples/s]Map (num_proc=32):  22%|██▏       | 149602/665298 [01:32<03:21, 2555.87 examples/s]Map (num_proc=32):  23%|██▎       | 149876/665298 [01:32<03:18, 2597.36 examples/s]Map (num_proc=32):  23%|██▎       | 150162/665298 [01:32<03:48, 2258.98 examples/s]Map (num_proc=32):  23%|██▎       | 150667/665298 [01:32<03:05, 2776.55 examples/s]Map (num_proc=32):  23%|██▎       | 150960/665298 [01:32<03:05, 2777.09 examples/s]Map (num_proc=32):  23%|██▎       | 151245/665298 [01:32<03:04, 2789.67 examples/s]Map (num_proc=32):  23%|██▎       | 151533/665298 [01:32<03:02, 2808.48 examples/s]Map (num_proc=32):  23%|██▎       | 151887/665298 [01:33<03:11, 2686.87 examples/s]Map (num_proc=32):  23%|██▎       | 152174/665298 [01:33<03:32, 2413.00 examples/s]Map (num_proc=32):  23%|██▎       | 152464/665298 [01:33<04:13, 2024.53 examples/s]Map (num_proc=32):  23%|██▎       | 152715/665298 [01:34<10:56, 780.49 examples/s] Map (num_proc=32):  23%|██▎       | 153504/665298 [01:34<05:42, 1492.63 examples/s]Map (num_proc=32):  23%|██▎       | 153822/665298 [01:34<06:05, 1398.14 examples/s]Map (num_proc=32):  23%|██▎       | 154116/665298 [01:34<06:02, 1410.48 examples/s]Map (num_proc=32):  23%|██▎       | 154342/665298 [01:35<05:57, 1428.40 examples/s]Map (num_proc=32):  23%|██▎       | 154606/665298 [01:35<07:31, 1129.90 examples/s]Map (num_proc=32):  23%|██▎       | 154960/665298 [01:35<06:13, 1367.98 examples/s]Map (num_proc=32):  23%|██▎       | 155177/665298 [01:35<06:08, 1382.82 examples/s]Map (num_proc=32):  23%|██▎       | 155394/665298 [01:36<07:27, 1138.74 examples/s]Map (num_proc=32):  23%|██▎       | 155676/665298 [01:36<06:11, 1371.05 examples/s]Map (num_proc=32):  23%|██▎       | 155873/665298 [01:36<06:16, 1354.69 examples/s]Map (num_proc=32):  23%|██▎       | 156078/665298 [01:36<06:15, 1356.53 examples/s]Map (num_proc=32):  23%|██▎       | 156293/665298 [01:36<06:10, 1373.70 examples/s]Map (num_proc=32):  24%|██▎       | 156506/665298 [01:36<06:07, 1384.20 examples/s]Map (num_proc=32):  24%|██▎       | 156670/665298 [01:36<06:47, 1248.65 examples/s]Map (num_proc=32):  24%|██▎       | 156814/665298 [01:37<06:36, 1283.96 examples/s]Map (num_proc=32):  24%|██▎       | 157009/665298 [01:37<06:35, 1286.31 examples/s]Map (num_proc=32):  24%|██▎       | 157154/665298 [01:37<06:25, 1318.37 examples/s]Map (num_proc=32):  24%|██▎       | 157302/665298 [01:37<06:15, 1351.28 examples/s]Map (num_proc=32):  24%|██▎       | 157522/665298 [01:37<06:05, 1390.50 examples/s]Map (num_proc=32):  24%|██▎       | 157674/665298 [01:37<06:51, 1232.35 examples/s]Map (num_proc=32):  24%|██▎       | 157857/665298 [01:37<06:59, 1211.02 examples/s]Map (num_proc=32):  24%|██▍       | 158028/665298 [01:38<07:08, 1182.51 examples/s]Map (num_proc=32):  24%|██▍       | 158208/665298 [01:38<07:08, 1184.15 examples/s]Map (num_proc=32):  24%|██▍       | 158426/665298 [01:38<06:02, 1399.87 examples/s]Map (num_proc=32):  24%|██▍       | 158667/665298 [01:38<05:08, 1642.42 examples/s]Map (num_proc=32):  24%|██▍       | 158902/665298 [01:38<04:54, 1720.62 examples/s]Map (num_proc=32):  24%|██▍       | 159110/665298 [01:38<04:39, 1808.21 examples/s]Map (num_proc=32):  24%|██▍       | 159361/665298 [01:38<04:13, 1994.22 examples/s]Map (num_proc=32):  24%|██▍       | 159664/665298 [01:38<03:59, 2113.09 examples/s]Map (num_proc=32):  24%|██▍       | 159984/665298 [01:38<03:44, 2248.71 examples/s]Map (num_proc=32):  24%|██▍       | 160221/665298 [01:39<05:22, 1564.22 examples/s]Map (num_proc=32):  24%|██▍       | 160654/665298 [01:39<04:00, 2102.17 examples/s]Map (num_proc=32):  24%|██▍       | 160941/665298 [01:39<04:02, 2076.77 examples/s]Map (num_proc=32):  24%|██▍       | 161189/665298 [01:39<04:12, 1999.80 examples/s]Map (num_proc=32):  24%|██▍       | 161416/665298 [01:39<04:05, 2048.72 examples/s]Map (num_proc=32):  24%|██▍       | 161654/665298 [01:39<03:56, 2127.82 examples/s]Map (num_proc=32):  24%|██▍       | 161939/665298 [01:39<04:06, 2045.50 examples/s]Map (num_proc=32):  24%|██▍       | 162230/665298 [01:40<03:44, 2240.24 examples/s]Map (num_proc=32):  24%|██▍       | 162521/665298 [01:40<03:52, 2167.10 examples/s]Map (num_proc=32):  24%|██▍       | 162783/665298 [01:40<03:49, 2188.73 examples/s]Map (num_proc=32):  25%|██▍       | 163030/665298 [01:40<03:42, 2252.63 examples/s]Map (num_proc=32):  25%|██▍       | 163290/665298 [01:40<04:01, 2080.24 examples/s]Map (num_proc=32):  25%|██▍       | 163527/665298 [01:40<03:53, 2152.59 examples/s]Map (num_proc=32):  25%|██▍       | 163801/665298 [01:40<04:00, 2085.30 examples/s]Map (num_proc=32):  25%|██▍       | 164056/665298 [01:40<03:54, 2139.62 examples/s]Map (num_proc=32):  25%|██▍       | 164314/665298 [01:41<03:43, 2245.88 examples/s]Map (num_proc=32):  25%|██▍       | 164609/665298 [01:41<03:47, 2199.98 examples/s]Map (num_proc=32):  25%|██▍       | 164862/665298 [01:41<03:39, 2282.79 examples/s]Map (num_proc=32):  25%|██▍       | 165111/665298 [01:41<03:43, 2238.02 examples/s]Map (num_proc=32):  25%|██▍       | 165396/665298 [01:41<03:39, 2278.95 examples/s]Map (num_proc=32):  25%|██▍       | 165676/665298 [01:41<03:48, 2188.61 examples/s]Map (num_proc=32):  25%|██▍       | 165922/665298 [01:41<03:49, 2179.38 examples/s]Map (num_proc=32):  25%|██▍       | 166167/665298 [01:41<03:52, 2150.15 examples/s]Map (num_proc=32):  25%|██▌       | 166431/665298 [01:42<03:57, 2099.23 examples/s]Map (num_proc=32):  25%|██▌       | 166807/665298 [01:42<03:37, 2296.43 examples/s]Map (num_proc=32):  25%|██▌       | 167038/665298 [01:42<03:36, 2297.29 examples/s]Map (num_proc=32):  25%|██▌       | 167294/665298 [01:42<03:47, 2193.06 examples/s]Map (num_proc=32):  25%|██▌       | 167577/665298 [01:42<03:42, 2234.57 examples/s]Map (num_proc=32):  25%|██▌       | 167821/665298 [01:42<03:38, 2281.50 examples/s]Map (num_proc=32):  25%|██▌       | 168055/665298 [01:42<03:55, 2111.29 examples/s]Map (num_proc=32):  25%|██▌       | 168334/665298 [01:42<03:49, 2161.97 examples/s]Map (num_proc=32):  25%|██▌       | 168565/665298 [01:42<03:46, 2191.44 examples/s]Map (num_proc=32):  25%|██▌       | 168799/665298 [01:43<03:42, 2228.03 examples/s]Map (num_proc=32):  25%|██▌       | 169029/665298 [01:43<03:41, 2245.31 examples/s]Map (num_proc=32):  25%|██▌       | 169294/665298 [01:43<04:00, 2063.21 examples/s]Map (num_proc=32):  25%|██▌       | 169511/665298 [01:43<03:57, 2083.91 examples/s]Map (num_proc=32):  26%|██▌       | 169736/665298 [01:43<03:53, 2124.66 examples/s]Map (num_proc=32):  26%|██▌       | 170008/665298 [01:43<03:44, 2203.60 examples/s]Map (num_proc=32):  26%|██▌       | 170287/665298 [01:43<03:53, 2121.24 examples/s]Map (num_proc=32):  26%|██▌       | 170527/665298 [01:43<03:45, 2189.33 examples/s]Map (num_proc=32):  26%|██▌       | 170755/665298 [01:43<03:43, 2212.84 examples/s]Map (num_proc=32):  26%|██▌       | 170983/665298 [01:44<03:42, 2226.42 examples/s]Map (num_proc=32):  26%|██▌       | 171235/665298 [01:44<03:42, 2221.18 examples/s]Map (num_proc=32):  26%|██▌       | 171476/665298 [01:44<03:39, 2248.21 examples/s]Map (num_proc=32):  26%|██▌       | 171712/665298 [01:44<03:36, 2276.70 examples/s]Map (num_proc=32):  26%|██▌       | 171955/665298 [01:44<04:05, 2012.37 examples/s]Map (num_proc=32):  26%|██▌       | 172194/665298 [01:44<03:54, 2107.15 examples/s]Map (num_proc=32):  26%|██▌       | 172429/665298 [01:44<03:46, 2171.49 examples/s]Map (num_proc=32):  26%|██▌       | 172655/665298 [01:44<03:44, 2194.50 examples/s]Map (num_proc=32):  26%|██▌       | 172894/665298 [01:44<03:39, 2241.64 examples/s]Map (num_proc=32):  26%|██▌       | 173173/665298 [01:45<04:49, 1697.16 examples/s]Map (num_proc=32):  26%|██▌       | 173591/665298 [01:45<03:46, 2173.46 examples/s]Map (num_proc=32):  26%|██▌       | 173879/665298 [01:45<03:42, 2211.22 examples/s]Map (num_proc=32):  26%|██▌       | 174152/665298 [01:45<03:46, 2170.74 examples/s]Map (num_proc=32):  26%|██▌       | 174469/665298 [01:45<03:36, 2270.26 examples/s]Map (num_proc=32):  26%|██▋       | 174762/665298 [01:45<03:31, 2320.14 examples/s]Map (num_proc=32):  26%|██▋       | 175020/665298 [01:45<04:08, 1970.81 examples/s]Map (num_proc=32):  26%|██▋       | 175272/665298 [01:46<06:08, 1330.36 examples/s]Map (num_proc=32):  26%|██▋       | 175479/665298 [01:47<14:01, 581.75 examples/s] Map (num_proc=32):  26%|██▋       | 175991/665298 [01:47<08:17, 983.03 examples/s]Map (num_proc=32):  27%|██▋       | 176451/665298 [01:47<05:53, 1381.86 examples/s]Map (num_proc=32):  27%|██▋       | 176806/665298 [01:47<06:11, 1315.00 examples/s]Map (num_proc=32):  27%|██▋       | 177063/665298 [01:48<06:13, 1306.54 examples/s]Map (num_proc=32):  27%|██▋       | 177312/665298 [01:48<06:16, 1296.79 examples/s]Map (num_proc=32):  27%|██▋       | 177552/665298 [01:48<06:57, 1167.46 examples/s]Map (num_proc=32):  27%|██▋       | 177714/665298 [01:48<07:01, 1155.49 examples/s]Map (num_proc=32):  27%|██▋       | 178011/665298 [01:48<05:36, 1449.28 examples/s]Map (num_proc=32):  27%|██▋       | 178260/665298 [01:48<05:05, 1593.27 examples/s]Map (num_proc=32):  27%|██▋       | 178565/665298 [01:48<04:35, 1765.39 examples/s]Map (num_proc=32):  27%|██▋       | 178811/665298 [01:49<04:14, 1909.89 examples/s]Map (num_proc=32):  27%|██▋       | 179092/665298 [01:49<04:06, 1975.25 examples/s]Map (num_proc=32):  27%|██▋       | 179394/665298 [01:49<03:48, 2129.12 examples/s]Map (num_proc=32):  27%|██▋       | 179624/665298 [01:49<03:45, 2156.64 examples/s]Map (num_proc=32):  27%|██▋       | 179854/665298 [01:49<03:59, 2031.05 examples/s]Map (num_proc=32):  27%|██▋       | 180082/665298 [01:49<06:12, 1303.40 examples/s]Map (num_proc=32):  27%|██▋       | 180577/665298 [01:50<04:09, 1945.36 examples/s]Map (num_proc=32):  27%|██▋       | 180998/665298 [01:50<03:31, 2292.34 examples/s]Map (num_proc=32):  27%|██▋       | 181309/665298 [01:50<03:39, 2202.73 examples/s]Map (num_proc=32):  27%|██▋       | 181616/665298 [01:50<03:49, 2111.49 examples/s]Map (num_proc=32):  27%|██▋       | 181890/665298 [01:50<03:44, 2152.70 examples/s]Map (num_proc=32):  27%|██▋       | 182195/665298 [01:50<03:43, 2165.45 examples/s]Map (num_proc=32):  27%|██▋       | 182498/665298 [01:50<03:26, 2340.90 examples/s]Map (num_proc=32):  27%|██▋       | 182756/665298 [01:50<03:22, 2388.70 examples/s]Map (num_proc=32):  28%|██▊       | 183010/665298 [01:51<03:26, 2331.69 examples/s]Map (num_proc=32):  28%|██▊       | 183259/665298 [01:51<03:42, 2167.82 examples/s]Map (num_proc=32):  28%|██▊       | 183544/665298 [01:51<03:44, 2150.28 examples/s]Map (num_proc=32):  28%|██▊       | 183819/665298 [01:51<03:42, 2165.04 examples/s]Map (num_proc=32):  28%|██▊       | 184058/665298 [01:51<03:36, 2218.05 examples/s]Map (num_proc=32):  28%|██▊       | 184314/665298 [01:51<03:28, 2305.59 examples/s]Map (num_proc=32):  28%|██▊       | 184604/665298 [01:51<03:27, 2315.61 examples/s]Map (num_proc=32):  28%|██▊       | 184887/665298 [01:51<03:27, 2317.12 examples/s]Map (num_proc=32):  28%|██▊       | 185182/665298 [01:52<03:43, 2152.22 examples/s]Map (num_proc=32):  28%|██▊       | 185412/665298 [01:52<03:39, 2182.09 examples/s]Map (num_proc=32):  28%|██▊       | 185664/665298 [01:52<04:31, 1767.68 examples/s]Map (num_proc=32):  28%|██▊       | 186138/665298 [01:52<03:24, 2343.81 examples/s]Map (num_proc=32):  28%|██▊       | 186447/665298 [01:52<03:25, 2331.35 examples/s]Map (num_proc=32):  28%|██▊       | 186744/665298 [01:52<03:26, 2319.16 examples/s]Map (num_proc=32):  28%|██▊       | 186987/665298 [01:52<03:24, 2343.80 examples/s]Map (num_proc=32):  28%|██▊       | 187292/665298 [01:52<03:33, 2234.64 examples/s]Map (num_proc=32):  28%|██▊       | 187536/665298 [01:53<03:33, 2232.67 examples/s]Map (num_proc=32):  28%|██▊       | 187811/665298 [01:53<03:41, 2156.18 examples/s]Map (num_proc=32):  28%|██▊       | 188056/665298 [01:53<03:34, 2225.44 examples/s]Map (num_proc=32):  28%|██▊       | 188288/665298 [01:53<03:32, 2244.92 examples/s]Map (num_proc=32):  28%|██▊       | 188517/665298 [01:53<03:31, 2253.05 examples/s]Map (num_proc=32):  28%|██▊       | 188794/665298 [01:53<03:27, 2291.65 examples/s]Map (num_proc=32):  28%|██▊       | 189077/665298 [01:53<03:38, 2179.43 examples/s]Map (num_proc=32):  28%|██▊       | 189332/665298 [01:53<03:29, 2269.96 examples/s]Map (num_proc=32):  29%|██▊       | 189617/665298 [01:54<03:36, 2192.23 examples/s]Map (num_proc=32):  29%|██▊       | 189863/665298 [01:54<03:37, 2184.02 examples/s]Map (num_proc=32):  29%|██▊       | 190111/665298 [01:54<03:30, 2258.70 examples/s]Map (num_proc=32):  29%|██▊       | 190341/665298 [01:54<03:29, 2266.45 examples/s]Map (num_proc=32):  29%|██▊       | 190573/665298 [01:54<03:28, 2277.17 examples/s]Map (num_proc=32):  29%|██▊       | 190820/665298 [01:54<03:24, 2317.30 examples/s]Map (num_proc=32):  29%|██▊       | 191070/665298 [01:54<03:48, 2071.79 examples/s]Map (num_proc=32):  29%|██▉       | 191310/665298 [01:54<03:40, 2151.11 examples/s]Map (num_proc=32):  29%|██▉       | 191549/665298 [01:54<03:34, 2213.25 examples/s]Map (num_proc=32):  29%|██▉       | 191787/665298 [01:55<03:30, 2246.44 examples/s]Map (num_proc=32):  29%|██▉       | 192039/665298 [01:55<03:24, 2319.61 examples/s]Map (num_proc=32):  29%|██▉       | 192280/665298 [01:55<03:21, 2342.53 examples/s]Map (num_proc=32):  29%|██▉       | 192576/665298 [01:55<03:35, 2198.61 examples/s]Map (num_proc=32):  29%|██▉       | 192833/665298 [01:55<03:26, 2287.04 examples/s]Map (num_proc=32):  29%|██▉       | 193106/665298 [01:55<03:43, 2114.16 examples/s]Map (num_proc=32):  29%|██▉       | 193338/665298 [01:55<03:38, 2162.49 examples/s]Map (num_proc=32):  29%|██▉       | 193588/665298 [01:55<03:43, 2107.05 examples/s]Map (num_proc=32):  29%|██▉       | 193811/665298 [01:56<04:20, 1810.89 examples/s]Map (num_proc=32):  29%|██▉       | 194116/665298 [01:56<03:48, 2062.43 examples/s]Map (num_proc=32):  29%|██▉       | 194473/665298 [01:56<03:30, 2234.64 examples/s]Map (num_proc=32):  29%|██▉       | 194752/665298 [01:56<03:21, 2336.53 examples/s]Map (num_proc=32):  29%|██▉       | 194996/665298 [01:56<03:21, 2330.48 examples/s]Map (num_proc=32):  29%|██▉       | 195247/665298 [01:56<03:44, 2094.31 examples/s]Map (num_proc=32):  29%|██▉       | 195477/665298 [01:56<03:39, 2137.13 examples/s]Map (num_proc=32):  29%|██▉       | 195730/665298 [01:56<03:35, 2175.94 examples/s]Map (num_proc=32):  29%|██▉       | 195977/665298 [01:56<03:31, 2219.81 examples/s]Map (num_proc=32):  29%|██▉       | 196204/665298 [01:57<03:32, 2211.35 examples/s]Map (num_proc=32):  30%|██▉       | 196434/665298 [01:57<03:30, 2224.99 examples/s]Map (num_proc=32):  30%|██▉       | 196702/665298 [01:57<03:47, 2058.14 examples/s]Map (num_proc=32):  30%|██▉       | 196935/665298 [01:57<04:38, 1679.76 examples/s]Map (num_proc=32):  30%|██▉       | 197119/665298 [01:58<14:37, 533.44 examples/s] Map (num_proc=32):  30%|██▉       | 197681/665298 [01:58<07:51, 991.82 examples/s]Map (num_proc=32):  30%|██▉       | 198119/665298 [01:58<05:54, 1316.46 examples/s]Map (num_proc=32):  30%|██▉       | 198437/665298 [01:59<06:17, 1237.58 examples/s]Map (num_proc=32):  30%|██▉       | 198681/665298 [01:59<06:18, 1232.14 examples/s]Map (num_proc=32):  30%|██▉       | 198892/665298 [01:59<06:33, 1185.73 examples/s]Map (num_proc=32):  30%|██▉       | 199085/665298 [01:59<06:27, 1204.04 examples/s]Map (num_proc=32):  30%|██▉       | 199240/665298 [01:59<06:31, 1190.65 examples/s]Map (num_proc=32):  30%|██▉       | 199551/665298 [01:59<05:20, 1451.81 examples/s]Map (num_proc=32):  30%|███       | 199786/665298 [02:00<04:46, 1626.06 examples/s]Map (num_proc=32):  30%|███       | 200027/665298 [02:00<04:20, 1788.78 examples/s]Map (num_proc=32):  30%|███       | 200257/665298 [02:00<04:08, 1871.74 examples/s]Map (num_proc=32):  30%|███       | 200505/665298 [02:00<04:48, 1613.30 examples/s]Map (num_proc=32):  30%|███       | 200725/665298 [02:00<04:28, 1732.89 examples/s]Map (num_proc=32):  30%|███       | 200918/665298 [02:00<04:37, 1672.13 examples/s]Map (num_proc=32):  30%|███       | 201162/665298 [02:00<04:10, 1855.37 examples/s]Map (num_proc=32):  30%|███       | 201366/665298 [02:00<04:05, 1892.54 examples/s]Map (num_proc=32):  30%|███       | 201662/665298 [02:01<03:53, 1981.69 examples/s]Map (num_proc=32):  30%|███       | 201887/665298 [02:01<03:47, 2039.29 examples/s]Map (num_proc=32):  30%|███       | 202146/665298 [02:01<03:41, 2095.19 examples/s]Map (num_proc=32):  30%|███       | 202381/665298 [02:01<03:38, 2117.72 examples/s]Map (num_proc=32):  30%|███       | 202687/665298 [02:01<03:30, 2202.43 examples/s]Map (num_proc=32):  31%|███       | 202916/665298 [02:01<05:36, 1375.59 examples/s]Map (num_proc=32):  31%|███       | 203467/665298 [02:01<03:38, 2117.17 examples/s]Map (num_proc=32):  31%|███       | 203885/665298 [02:02<03:16, 2352.67 examples/s]Map (num_proc=32):  31%|███       | 204186/665298 [02:02<03:11, 2412.06 examples/s]Map (num_proc=32):  31%|███       | 204507/665298 [02:02<03:13, 2386.32 examples/s]Map (num_proc=32):  31%|███       | 204800/665298 [02:02<03:09, 2434.46 examples/s]Map (num_proc=32):  31%|███       | 205080/665298 [02:02<03:16, 2347.42 examples/s]Map (num_proc=32):  31%|███       | 205344/665298 [02:02<03:31, 2176.33 examples/s]Map (num_proc=32):  31%|███       | 205583/665298 [02:02<03:40, 2083.62 examples/s]Map (num_proc=32):  31%|███       | 205822/665298 [02:02<03:33, 2153.92 examples/s]Map (num_proc=32):  31%|███       | 206108/665298 [02:03<03:27, 2209.76 examples/s]Map (num_proc=32):  31%|███       | 206343/665298 [02:03<03:36, 2123.88 examples/s]Map (num_proc=32):  31%|███       | 206705/665298 [02:03<03:11, 2391.50 examples/s]Map (num_proc=32):  31%|███       | 206971/665298 [02:03<03:23, 2255.94 examples/s]Map (num_proc=32):  31%|███       | 207216/665298 [02:03<03:19, 2300.95 examples/s]Map (num_proc=32):  31%|███       | 207462/665298 [02:03<03:41, 2064.35 examples/s]Map (num_proc=32):  31%|███       | 207708/665298 [02:03<03:32, 2158.03 examples/s]Map (num_proc=32):  31%|███▏      | 207938/665298 [02:03<03:30, 2174.49 examples/s]Map (num_proc=32):  31%|███▏      | 208170/665298 [02:03<03:27, 2207.47 examples/s]Map (num_proc=32):  31%|███▏      | 208415/665298 [02:04<03:20, 2273.27 examples/s]Map (num_proc=32):  31%|███▏      | 208689/665298 [02:04<03:14, 2348.29 examples/s]Map (num_proc=32):  31%|███▏      | 208945/665298 [02:04<03:37, 2102.13 examples/s]Map (num_proc=32):  31%|███▏      | 209174/665298 [02:04<03:32, 2148.26 examples/s]Map (num_proc=32):  31%|███▏      | 209416/665298 [02:04<03:33, 2137.91 examples/s]Map (num_proc=32):  32%|███▏      | 209642/665298 [02:04<03:31, 2158.19 examples/s]Map (num_proc=32):  32%|███▏      | 209890/665298 [02:04<03:27, 2197.62 examples/s]Map (num_proc=32):  32%|███▏      | 210142/665298 [02:04<03:20, 2274.69 examples/s]Map (num_proc=32):  32%|███▏      | 210381/665298 [02:04<03:17, 2301.51 examples/s]Map (num_proc=32):  32%|███▏      | 210664/665298 [02:05<03:17, 2297.55 examples/s]Map (num_proc=32):  32%|███▏      | 210898/665298 [02:05<03:36, 2102.55 examples/s]Map (num_proc=32):  32%|███▏      | 211142/665298 [02:05<03:27, 2187.37 examples/s]Map (num_proc=32):  32%|███▏      | 211416/665298 [02:05<03:40, 2059.78 examples/s]Map (num_proc=32):  32%|███▏      | 211642/665298 [02:05<03:35, 2106.13 examples/s]Map (num_proc=32):  32%|███▏      | 211867/665298 [02:05<03:31, 2140.11 examples/s]Map (num_proc=32):  32%|███▏      | 212098/665298 [02:05<03:29, 2164.67 examples/s]Map (num_proc=32):  32%|███▏      | 212326/665298 [02:05<03:26, 2193.41 examples/s]Map (num_proc=32):  32%|███▏      | 212577/665298 [02:06<03:19, 2274.26 examples/s]Map (num_proc=32):  32%|███▏      | 212820/665298 [02:06<03:15, 2316.35 examples/s]Map (num_proc=32):  32%|███▏      | 213066/665298 [02:06<03:16, 2301.79 examples/s]Map (num_proc=32):  32%|███▏      | 213342/665298 [02:06<03:34, 2107.40 examples/s]Map (num_proc=32):  32%|███▏      | 213573/665298 [02:06<03:29, 2158.34 examples/s]Map (num_proc=32):  32%|███▏      | 213799/665298 [02:06<03:27, 2180.62 examples/s]Map (num_proc=32):  32%|███▏      | 214048/665298 [02:06<03:24, 2208.15 examples/s]Map (num_proc=32):  32%|███▏      | 214282/665298 [02:06<03:21, 2239.91 examples/s]Map (num_proc=32):  32%|███▏      | 214513/665298 [02:06<04:21, 1723.47 examples/s]Map (num_proc=32):  32%|███▏      | 215005/665298 [02:07<03:17, 2278.76 examples/s]Map (num_proc=32):  32%|███▏      | 215253/665298 [02:07<03:19, 2256.66 examples/s]Map (num_proc=32):  32%|███▏      | 215531/665298 [02:07<03:14, 2310.82 examples/s]Map (num_proc=32):  32%|███▏      | 215817/665298 [02:07<03:22, 2219.10 examples/s]Map (num_proc=32):  32%|███▏      | 216111/665298 [02:07<03:10, 2355.73 examples/s]Map (num_proc=32):  33%|███▎      | 216353/665298 [02:07<03:11, 2343.72 examples/s]Map (num_proc=32):  33%|███▎      | 216596/665298 [02:07<03:11, 2348.01 examples/s]Map (num_proc=32):  33%|███▎      | 216885/665298 [02:07<03:19, 2246.17 examples/s]Map (num_proc=32):  33%|███▎      | 217148/665298 [02:08<04:31, 1648.30 examples/s]Map (num_proc=32):  33%|███▎      | 217371/665298 [02:09<10:47, 691.75 examples/s] Map (num_proc=32):  33%|███▎      | 217894/665298 [02:09<06:28, 1151.26 examples/s]Map (num_proc=32):  33%|███▎      | 218247/665298 [02:09<05:09, 1442.21 examples/s]Map (num_proc=32):  33%|███▎      | 218557/665298 [02:09<05:27, 1363.97 examples/s]Map (num_proc=32):  33%|███▎      | 218815/665298 [02:09<05:30, 1349.24 examples/s]Map (num_proc=32):  33%|███▎      | 219017/665298 [02:10<06:46, 1097.15 examples/s]Map (num_proc=32):  33%|███▎      | 219197/665298 [02:10<06:40, 1113.76 examples/s]Map (num_proc=32):  33%|███▎      | 219384/665298 [02:10<06:30, 1141.70 examples/s]Map (num_proc=32):  33%|███▎      | 219557/665298 [02:10<08:05, 917.98 examples/s] Map (num_proc=32):  33%|███▎      | 219969/665298 [02:10<05:14, 1417.36 examples/s]Map (num_proc=32):  33%|███▎      | 220200/665298 [02:10<05:05, 1454.83 examples/s]Map (num_proc=32):  33%|███▎      | 220453/665298 [02:10<04:27, 1662.04 examples/s]Map (num_proc=32):  33%|███▎      | 220666/665298 [02:11<04:15, 1741.19 examples/s]Map (num_proc=32):  33%|███▎      | 220897/665298 [02:11<04:53, 1515.39 examples/s]Map (num_proc=32):  33%|███▎      | 221110/665298 [02:11<05:49, 1269.57 examples/s]Map (num_proc=32):  33%|███▎      | 221291/665298 [02:11<05:24, 1369.64 examples/s]Map (num_proc=32):  33%|███▎      | 221538/665298 [02:11<04:37, 1601.46 examples/s]Map (num_proc=32):  33%|███▎      | 221793/665298 [02:11<04:04, 1813.15 examples/s]Map (num_proc=32):  33%|███▎      | 222004/665298 [02:11<04:17, 1718.45 examples/s]Map (num_proc=32):  33%|███▎      | 222242/665298 [02:12<03:59, 1853.77 examples/s]Map (num_proc=32):  33%|███▎      | 222493/665298 [02:12<03:40, 2006.79 examples/s]Map (num_proc=32):  33%|███▎      | 222792/665298 [02:12<03:31, 2087.49 examples/s]Map (num_proc=32):  34%|███▎      | 223027/665298 [02:12<03:25, 2149.76 examples/s]Map (num_proc=32):  34%|███▎      | 223274/665298 [02:12<03:17, 2233.98 examples/s]Map (num_proc=32):  34%|███▎      | 223512/665298 [02:12<03:16, 2252.62 examples/s]Map (num_proc=32):  34%|███▎      | 223824/665298 [02:12<03:06, 2371.98 examples/s]Map (num_proc=32):  34%|███▎      | 224080/665298 [02:12<03:34, 2058.67 examples/s]Map (num_proc=32):  34%|███▎      | 224326/665298 [02:13<03:25, 2149.38 examples/s]Map (num_proc=32):  34%|███▍      | 224576/665298 [02:13<03:17, 2236.75 examples/s]Map (num_proc=32):  34%|███▍      | 224825/665298 [02:13<03:11, 2299.28 examples/s]Map (num_proc=32):  34%|███▍      | 225099/665298 [02:13<03:10, 2315.40 examples/s]Map (num_proc=32):  34%|███▍      | 225374/665298 [02:13<03:18, 2212.78 examples/s]Map (num_proc=32):  34%|███▍      | 225603/665298 [02:13<03:17, 2229.94 examples/s]Map (num_proc=32):  34%|███▍      | 225836/665298 [02:13<03:14, 2254.72 examples/s]Map (num_proc=32):  34%|███▍      | 226083/665298 [02:13<03:34, 2051.13 examples/s]Map (num_proc=32):  34%|███▍      | 226335/665298 [02:13<03:26, 2123.52 examples/s]Map (num_proc=32):  34%|███▍      | 226580/665298 [02:14<03:20, 2189.38 examples/s]Map (num_proc=32):  34%|███▍      | 226803/665298 [02:14<03:19, 2196.96 examples/s]Map (num_proc=32):  34%|███▍      | 227036/665298 [02:14<03:17, 2222.30 examples/s]Map (num_proc=32):  34%|███▍      | 227282/665298 [02:14<03:27, 2107.29 examples/s]Map (num_proc=32):  34%|███▍      | 227648/665298 [02:14<02:57, 2460.26 examples/s]Map (num_proc=32):  34%|███▍      | 227910/665298 [02:14<03:25, 2130.74 examples/s]Map (num_proc=32):  34%|███▍      | 228178/665298 [02:14<03:14, 2244.04 examples/s]Map (num_proc=32):  34%|███▍      | 228431/665298 [02:14<03:09, 2310.69 examples/s]Map (num_proc=32):  34%|███▍      | 228683/665298 [02:14<03:04, 2362.95 examples/s]Map (num_proc=32):  34%|███▍      | 228953/665298 [02:15<03:21, 2167.18 examples/s]Map (num_proc=32):  34%|███▍      | 229195/665298 [02:15<03:17, 2207.56 examples/s]Map (num_proc=32):  34%|███▍      | 229424/665298 [02:15<03:15, 2225.14 examples/s]Map (num_proc=32):  35%|███▍      | 229655/665298 [02:15<03:14, 2245.52 examples/s]Map (num_proc=32):  35%|███▍      | 229910/665298 [02:15<03:28, 2087.95 examples/s]Map (num_proc=32):  35%|███▍      | 230148/665298 [02:15<03:21, 2163.83 examples/s]Map (num_proc=32):  35%|███▍      | 230391/665298 [02:15<03:14, 2231.44 examples/s]Map (num_proc=32):  35%|███▍      | 230623/665298 [02:15<03:13, 2250.37 examples/s]Map (num_proc=32):  35%|███▍      | 230876/665298 [02:15<03:07, 2312.12 examples/s]Map (num_proc=32):  35%|███▍      | 231123/665298 [02:16<03:04, 2352.39 examples/s]Map (num_proc=32):  35%|███▍      | 231417/665298 [02:16<03:01, 2396.91 examples/s]Map (num_proc=32):  35%|███▍      | 231700/665298 [02:16<03:08, 2295.55 examples/s]Map (num_proc=32):  35%|███▍      | 231977/665298 [02:16<03:31, 2049.31 examples/s]Map (num_proc=32):  35%|███▍      | 232230/665298 [02:16<03:20, 2163.30 examples/s]Map (num_proc=32):  35%|███▍      | 232477/665298 [02:16<03:13, 2236.27 examples/s]Map (num_proc=32):  35%|███▍      | 232735/665298 [02:16<03:06, 2315.44 examples/s]Map (num_proc=32):  35%|███▌      | 233006/665298 [02:16<02:59, 2413.95 examples/s]Map (num_proc=32):  35%|███▌      | 233304/665298 [02:17<03:11, 2250.77 examples/s]Map (num_proc=32):  35%|███▌      | 233555/665298 [02:17<03:07, 2308.51 examples/s]Map (num_proc=32):  35%|███▌      | 233834/665298 [02:17<02:58, 2421.84 examples/s]Map (num_proc=32):  35%|███▌      | 234134/665298 [02:17<03:26, 2090.37 examples/s]Map (num_proc=32):  35%|███▌      | 234378/665298 [02:17<03:18, 2173.13 examples/s]Map (num_proc=32):  35%|███▌      | 234619/665298 [02:17<03:13, 2222.06 examples/s]Map (num_proc=32):  35%|███▌      | 234859/665298 [02:17<03:10, 2257.80 examples/s]Map (num_proc=32):  35%|███▌      | 235136/665298 [02:17<03:23, 2110.46 examples/s]Map (num_proc=32):  35%|███▌      | 235379/665298 [02:17<03:16, 2191.18 examples/s]Map (num_proc=32):  35%|███▌      | 235609/665298 [02:18<03:13, 2215.50 examples/s]Map (num_proc=32):  35%|███▌      | 235861/665298 [02:18<03:24, 2099.86 examples/s]Map (num_proc=32):  35%|███▌      | 236076/665298 [02:18<03:23, 2108.70 examples/s]Map (num_proc=32):  36%|███▌      | 236312/665298 [02:18<03:17, 2174.53 examples/s]Map (num_proc=32):  36%|███▌      | 236559/665298 [02:18<03:53, 1837.83 examples/s]Map (num_proc=32):  36%|███▌      | 236994/665298 [02:18<02:58, 2399.88 examples/s]Map (num_proc=32):  36%|███▌      | 237280/665298 [02:18<02:58, 2400.93 examples/s]Map (num_proc=32):  36%|███▌      | 237537/665298 [02:18<02:58, 2391.19 examples/s]Map (num_proc=32):  36%|███▌      | 237827/665298 [02:19<05:51, 1215.72 examples/s]Map (num_proc=32):  36%|███▌      | 238043/665298 [02:20<09:59, 712.40 examples/s] Map (num_proc=32):  36%|███▌      | 238591/665298 [02:20<05:57, 1193.42 examples/s]Map (num_proc=32):  36%|███▌      | 239050/665298 [02:20<04:34, 1555.30 examples/s]Map (num_proc=32):  36%|███▌      | 239360/665298 [02:20<04:53, 1449.66 examples/s]Map (num_proc=32):  36%|███▌      | 239590/665298 [02:20<05:10, 1370.06 examples/s]Map (num_proc=32):  36%|███▌      | 239807/665298 [02:21<05:53, 1205.18 examples/s]Map (num_proc=32):  36%|███▌      | 239992/665298 [02:21<05:50, 1213.62 examples/s]Map (num_proc=32):  36%|███▌      | 240166/665298 [02:21<05:57, 1188.06 examples/s]Map (num_proc=32):  36%|███▌      | 240354/665298 [02:21<05:53, 1200.59 examples/s]Map (num_proc=32):  36%|███▌      | 240581/665298 [02:21<05:33, 1275.30 examples/s]Map (num_proc=32):  36%|███▌      | 240859/665298 [02:21<04:42, 1502.18 examples/s]Map (num_proc=32):  36%|███▋      | 241189/665298 [02:21<03:47, 1863.16 examples/s]Map (num_proc=32):  36%|███▋      | 241522/665298 [02:22<03:12, 2201.03 examples/s]Map (num_proc=32):  36%|███▋      | 241827/665298 [02:22<03:27, 2043.42 examples/s]Map (num_proc=32):  36%|███▋      | 242137/665298 [02:22<03:05, 2287.22 examples/s]Map (num_proc=32):  37%|███▋      | 242920/665298 [02:22<02:04, 3379.82 examples/s]Map (num_proc=32):  37%|███▋      | 243515/665298 [02:22<01:45, 4010.59 examples/s]Map (num_proc=32):  37%|███▋      | 243982/665298 [02:22<02:54, 2417.09 examples/s]Map (num_proc=32):  37%|███▋      | 245184/665298 [02:23<01:42, 4080.25 examples/s]Map (num_proc=32):  37%|███▋      | 246795/665298 [02:23<01:05, 6378.69 examples/s]Map (num_proc=32):  37%|███▋      | 247697/665298 [02:23<01:04, 6491.40 examples/s]Map (num_proc=32):  37%|███▋      | 248579/665298 [02:23<01:05, 6372.80 examples/s]Map (num_proc=32):  37%|███▋      | 249366/665298 [02:23<01:05, 6363.30 examples/s]Map (num_proc=32):  38%|███▊      | 250164/665298 [02:23<01:09, 5953.88 examples/s]Map (num_proc=32):  38%|███▊      | 250970/665298 [02:23<01:08, 6038.36 examples/s]Map (num_proc=32):  38%|███▊      | 251701/665298 [02:23<01:10, 5898.38 examples/s]Map (num_proc=32):  38%|███▊      | 252495/665298 [02:24<01:10, 5894.28 examples/s]Map (num_proc=32):  38%|███▊      | 253136/665298 [02:24<01:09, 5939.76 examples/s]Map (num_proc=32):  38%|███▊      | 253864/665298 [02:24<01:09, 5896.92 examples/s]Map (num_proc=32):  38%|███▊      | 254658/665298 [02:24<01:10, 5829.76 examples/s]Map (num_proc=32):  38%|███▊      | 255269/665298 [02:24<01:09, 5892.74 examples/s]Map (num_proc=32):  38%|███▊      | 255926/665298 [02:24<01:10, 5811.29 examples/s]Map (num_proc=32):  39%|███▊      | 256742/665298 [02:24<01:09, 5886.75 examples/s]Map (num_proc=32):  39%|███▊      | 257397/665298 [02:24<01:17, 5284.21 examples/s]Map (num_proc=32):  39%|███▉      | 257980/665298 [02:25<03:29, 1942.71 examples/s]Map (num_proc=32):  39%|███▉      | 259083/665298 [02:25<02:18, 2927.79 examples/s]Map (num_proc=32):  39%|███▉      | 260101/665298 [02:26<01:48, 3741.31 examples/s]Map (num_proc=32):  39%|███▉      | 260791/665298 [02:26<02:11, 3081.24 examples/s]Map (num_proc=32):  39%|███▉      | 261390/665298 [02:26<01:57, 3428.33 examples/s]Map (num_proc=32):  39%|███▉      | 261935/665298 [02:26<02:06, 3189.95 examples/s]Map (num_proc=32):  39%|███▉      | 262405/665298 [02:26<02:06, 3174.22 examples/s]Map (num_proc=32):  40%|███▉      | 262946/665298 [02:27<02:13, 3008.31 examples/s]Map (num_proc=32):  40%|███▉      | 263404/665298 [02:27<02:50, 2358.76 examples/s]Map (num_proc=32):  40%|███▉      | 263762/665298 [02:27<02:53, 2309.64 examples/s]Map (num_proc=32):  40%|███▉      | 264070/665298 [02:27<02:44, 2435.44 examples/s]Map (num_proc=32):  40%|███▉      | 264376/665298 [02:27<02:36, 2554.47 examples/s]Map (num_proc=32):  40%|███▉      | 264797/665298 [02:27<02:36, 2559.66 examples/s]Map (num_proc=32):  40%|███▉      | 265108/665298 [02:27<02:29, 2673.67 examples/s]Map (num_proc=32):  40%|███▉      | 265407/665298 [02:28<02:25, 2744.92 examples/s]Map (num_proc=32):  40%|███▉      | 265797/665298 [02:28<02:29, 2676.06 examples/s]Map (num_proc=32):  40%|███▉      | 266087/665298 [02:28<02:26, 2726.49 examples/s]Map (num_proc=32):  40%|████      | 266393/665298 [02:28<02:21, 2809.87 examples/s]Map (num_proc=32):  40%|████      | 266798/665298 [02:28<02:26, 2713.33 examples/s]Map (num_proc=32):  40%|████      | 267230/665298 [02:28<02:24, 2763.76 examples/s]Map (num_proc=32):  40%|████      | 267642/665298 [02:28<02:28, 2686.43 examples/s]Map (num_proc=32):  40%|████      | 267954/665298 [02:29<02:22, 2783.85 examples/s]Map (num_proc=32):  40%|████      | 268266/665298 [02:29<02:18, 2865.08 examples/s]Map (num_proc=32):  40%|████      | 268642/665298 [02:29<02:25, 2719.94 examples/s]Map (num_proc=32):  40%|████      | 268949/665298 [02:29<02:21, 2802.64 examples/s]Map (num_proc=32):  40%|████      | 269263/665298 [02:29<02:17, 2888.42 examples/s]Map (num_proc=32):  41%|████      | 269642/665298 [02:29<02:24, 2745.73 examples/s]Map (num_proc=32):  41%|████      | 269950/665298 [02:29<02:19, 2828.60 examples/s]Map (num_proc=32):  41%|████      | 270260/665298 [02:29<02:16, 2898.95 examples/s]Map (num_proc=32):  41%|████      | 270577/665298 [02:34<29:23, 223.87 examples/s] Map (num_proc=32):  41%|████      | 271038/665298 [02:34<19:03, 344.90 examples/s]Map (num_proc=32):  41%|████      | 271283/665298 [02:34<15:29, 424.09 examples/s]Map (num_proc=32):  41%|████      | 271731/665298 [02:34<10:32, 622.22 examples/s]Map (num_proc=32):  41%|████      | 272184/665298 [02:34<07:37, 859.09 examples/s]Map (num_proc=32):  41%|████      | 272577/665298 [02:35<07:10, 911.25 examples/s]Map (num_proc=32):  41%|████      | 273432/665298 [02:35<04:07, 1585.70 examples/s]Map (num_proc=32):  41%|████      | 273892/665298 [02:35<03:36, 1809.12 examples/s]Map (num_proc=32):  41%|████      | 274283/665298 [02:35<03:21, 1943.24 examples/s]Map (num_proc=32):  41%|████▏     | 274712/665298 [02:35<03:03, 2133.66 examples/s]Map (num_proc=32):  41%|████▏     | 275167/665298 [02:36<02:45, 2356.67 examples/s]Map (num_proc=32):  41%|████▏     | 275573/665298 [02:36<02:44, 2367.38 examples/s]Map (num_proc=32):  41%|████▏     | 275876/665298 [02:36<02:36, 2489.58 examples/s]Map (num_proc=32):  42%|████▏     | 276176/665298 [02:36<02:29, 2596.14 examples/s]Map (num_proc=32):  42%|████▏     | 276583/665298 [02:36<02:31, 2557.80 examples/s]Map (num_proc=32):  42%|████▏     | 277034/665298 [02:36<02:24, 2690.72 examples/s]Map (num_proc=32):  42%|████▏     | 277432/665298 [02:36<02:30, 2582.34 examples/s]Map (num_proc=32):  42%|████▏     | 277729/665298 [02:37<02:27, 2634.08 examples/s]Map (num_proc=32):  42%|████▏     | 278035/665298 [02:37<02:21, 2730.89 examples/s]Map (num_proc=32):  42%|████▏     | 278433/665298 [02:37<02:25, 2652.27 examples/s]Map (num_proc=32):  42%|████▏     | 278859/665298 [02:37<02:22, 2707.98 examples/s]Map (num_proc=32):  42%|████▏     | 279161/665298 [02:37<02:18, 2779.66 examples/s]Map (num_proc=32):  42%|████▏     | 279561/665298 [02:37<02:26, 2629.28 examples/s]Map (num_proc=32):  42%|████▏     | 280016/665298 [02:37<02:26, 2621.49 examples/s]Map (num_proc=32):  42%|████▏     | 280436/665298 [02:37<02:17, 2808.32 examples/s]Map (num_proc=32):  42%|████▏     | 280747/665298 [02:38<02:13, 2874.96 examples/s]Map (num_proc=32):  42%|████▏     | 281191/665298 [02:38<02:12, 2900.86 examples/s]Map (num_proc=32):  42%|████▏     | 281589/665298 [02:38<02:17, 2798.63 examples/s]Map (num_proc=32):  42%|████▏     | 281900/665298 [02:38<02:13, 2869.43 examples/s]Map (num_proc=32):  42%|████▏     | 282204/665298 [02:38<02:11, 2906.60 examples/s]Map (num_proc=32):  42%|████▏     | 282588/665298 [02:38<02:18, 2754.98 examples/s]Map (num_proc=32):  43%|████▎     | 282896/665298 [02:38<02:15, 2831.87 examples/s]Map (num_proc=32):  43%|████▎     | 283199/665298 [02:38<02:12, 2879.81 examples/s]Map (num_proc=32):  43%|████▎     | 283586/665298 [02:39<02:18, 2753.18 examples/s]Map (num_proc=32):  43%|████▎     | 283892/665298 [02:39<02:14, 2829.37 examples/s]Map (num_proc=32):  43%|████▎     | 284197/665298 [02:39<03:10, 1997.96 examples/s]Map (num_proc=32):  43%|████▎     | 284433/665298 [02:39<03:09, 2010.95 examples/s]Map (num_proc=32):  43%|████▎     | 284712/665298 [02:39<02:54, 2182.17 examples/s]Map (num_proc=32):  43%|████▎     | 285022/665298 [02:39<02:38, 2401.46 examples/s]Map (num_proc=32):  43%|████▎     | 285428/665298 [02:39<02:35, 2445.55 examples/s]Map (num_proc=32):  43%|████▎     | 285730/665298 [02:40<02:26, 2583.30 examples/s]Map (num_proc=32):  43%|████▎     | 286037/665298 [02:40<02:20, 2704.47 examples/s]Map (num_proc=32):  43%|████▎     | 286428/665298 [02:40<02:23, 2634.86 examples/s]Map (num_proc=32):  43%|████▎     | 286717/665298 [02:40<02:20, 2695.24 examples/s]Map (num_proc=32):  43%|████▎     | 287027/665298 [02:40<02:15, 2799.33 examples/s]Map (num_proc=32):  43%|████▎     | 287430/665298 [02:40<02:19, 2701.78 examples/s]Map (num_proc=32):  43%|████▎     | 287743/665298 [02:40<02:14, 2806.46 examples/s]Map (num_proc=32):  43%|████▎     | 288057/665298 [02:40<02:10, 2891.92 examples/s]Map (num_proc=32):  43%|████▎     | 288435/665298 [02:41<02:18, 2725.06 examples/s]Map (num_proc=32):  43%|████▎     | 288750/665298 [02:41<02:13, 2830.63 examples/s]Map (num_proc=32):  43%|████▎     | 289056/665298 [02:41<02:10, 2889.74 examples/s]Map (num_proc=32):  44%|████▎     | 289439/665298 [02:41<02:16, 2760.66 examples/s]Map (num_proc=32):  44%|████▎     | 289753/665298 [02:41<02:11, 2855.96 examples/s]Map (num_proc=32):  44%|████▎     | 290068/665298 [02:41<02:07, 2933.06 examples/s]Map (num_proc=32):  44%|████▎     | 290428/665298 [02:41<02:55, 2139.77 examples/s]Map (num_proc=32):  44%|████▎     | 291047/665298 [02:41<02:04, 2996.11 examples/s]Map (num_proc=32):  44%|████▍     | 291525/665298 [02:45<17:01, 365.77 examples/s] Map (num_proc=32):  44%|████▍     | 291982/665298 [02:45<12:23, 501.97 examples/s]Map (num_proc=32):  44%|████▍     | 292385/665298 [02:45<09:36, 647.01 examples/s]Map (num_proc=32):  44%|████▍     | 292698/665298 [02:46<07:47, 796.75 examples/s]Map (num_proc=32):  44%|████▍     | 293077/665298 [02:46<06:12, 998.33 examples/s]Map (num_proc=32):  44%|████▍     | 293383/665298 [02:46<05:11, 1195.07 examples/s]Map (num_proc=32):  44%|████▍     | 293692/665298 [02:46<04:23, 1410.46 examples/s]Map (num_proc=32):  44%|████▍     | 294079/665298 [02:46<03:40, 1684.72 examples/s]Map (num_proc=32):  44%|████▍     | 294373/665298 [02:46<03:15, 1893.65 examples/s]Map (num_proc=32):  44%|████▍     | 294684/665298 [02:46<02:54, 2126.54 examples/s]Map (num_proc=32):  44%|████▍     | 294991/665298 [02:46<02:40, 2303.31 examples/s]Map (num_proc=32):  44%|████▍     | 295377/665298 [02:46<02:35, 2373.49 examples/s]Map (num_proc=32):  44%|████▍     | 295825/665298 [02:47<02:22, 2587.30 examples/s]Map (num_proc=32):  45%|████▍     | 296227/665298 [02:47<02:22, 2581.86 examples/s]Map (num_proc=32):  45%|████▍     | 296532/665298 [02:47<02:17, 2684.52 examples/s]Map (num_proc=32):  45%|████▍     | 296839/665298 [02:47<02:14, 2741.30 examples/s]Map (num_proc=32):  45%|████▍     | 297226/665298 [02:47<02:17, 2674.41 examples/s]Map (num_proc=32):  45%|████▍     | 297680/665298 [02:47<02:11, 2785.35 examples/s]Map (num_proc=32):  45%|████▍     | 298074/665298 [02:48<03:51, 1583.49 examples/s]Map (num_proc=32):  45%|████▍     | 299379/665298 [02:48<01:53, 3224.17 examples/s]Map (num_proc=32):  45%|████▌     | 299990/665298 [02:48<01:56, 3134.77 examples/s]Map (num_proc=32):  45%|████▌     | 300532/665298 [02:48<02:00, 3020.23 examples/s]Map (num_proc=32):  45%|████▌     | 300990/665298 [02:48<02:00, 3025.52 examples/s]Map (num_proc=32):  45%|████▌     | 301379/665298 [02:49<02:06, 2879.55 examples/s]Map (num_proc=32):  45%|████▌     | 301822/665298 [02:49<02:04, 2922.23 examples/s]Map (num_proc=32):  45%|████▌     | 302226/665298 [02:49<02:11, 2762.71 examples/s]Map (num_proc=32):  45%|████▌     | 302697/665298 [02:49<02:04, 2912.88 examples/s]Map (num_proc=32):  46%|████▌     | 303010/665298 [02:49<02:02, 2949.22 examples/s]Map (num_proc=32):  46%|████▌     | 303399/665298 [02:49<02:07, 2839.28 examples/s]Map (num_proc=32):  46%|████▌     | 303707/665298 [02:49<02:05, 2885.33 examples/s]Map (num_proc=32):  46%|████▌     | 304093/665298 [02:50<02:07, 2826.70 examples/s]Map (num_proc=32):  46%|████▌     | 304404/665298 [02:50<02:04, 2892.68 examples/s]Map (num_proc=32):  46%|████▌     | 304718/665298 [02:50<02:02, 2953.84 examples/s]Map (num_proc=32):  46%|████▌     | 305074/665298 [02:50<03:06, 1927.30 examples/s]Map (num_proc=32):  46%|████▌     | 305360/665298 [02:50<02:51, 2104.08 examples/s]Map (num_proc=32):  46%|████▌     | 305641/665298 [02:50<02:39, 2252.99 examples/s]Map (num_proc=32):  46%|████▌     | 305924/665298 [02:50<02:30, 2384.55 examples/s]Map (num_proc=32):  46%|████▌     | 306269/665298 [02:51<02:32, 2353.74 examples/s]Map (num_proc=32):  46%|████▌     | 306557/665298 [02:51<02:24, 2478.26 examples/s]Map (num_proc=32):  46%|████▌     | 306853/665298 [02:51<02:17, 2598.72 examples/s]Map (num_proc=32):  46%|████▌     | 307217/665298 [02:51<02:22, 2517.27 examples/s]Map (num_proc=32):  46%|████▌     | 307502/665298 [02:51<02:17, 2598.94 examples/s]Map (num_proc=32):  46%|████▋     | 307799/665298 [02:51<02:12, 2695.04 examples/s]Map (num_proc=32):  46%|████▋     | 308224/665298 [02:51<02:16, 2623.93 examples/s]Map (num_proc=32):  46%|████▋     | 308530/665298 [02:51<02:10, 2729.84 examples/s]Map (num_proc=32):  46%|████▋     | 308826/665298 [02:51<02:07, 2788.38 examples/s]Map (num_proc=32):  46%|████▋     | 309214/665298 [02:52<02:14, 2655.40 examples/s]Map (num_proc=32):  47%|████▋     | 309495/665298 [02:52<02:12, 2691.34 examples/s]Map (num_proc=32):  47%|████▋     | 309910/665298 [02:52<02:10, 2715.81 examples/s]Map (num_proc=32):  47%|████▋     | 310266/665298 [02:52<02:16, 2598.81 examples/s]Map (num_proc=32):  47%|████▋     | 310557/665298 [02:52<02:12, 2672.61 examples/s]Map (num_proc=32):  47%|████▋     | 310857/665298 [02:52<02:08, 2754.19 examples/s]Map (num_proc=32):  47%|████▋     | 311227/665298 [02:52<02:13, 2649.56 examples/s]Map (num_proc=32):  47%|████▋     | 311522/665298 [02:52<02:09, 2724.43 examples/s]Map (num_proc=32):  47%|████▋     | 311827/665298 [02:53<02:05, 2808.41 examples/s]Map (num_proc=32):  47%|████▋     | 312156/665298 [02:57<24:41, 238.43 examples/s] Map (num_proc=32):  47%|████▋     | 312457/665298 [02:57<18:11, 323.28 examples/s]Map (num_proc=32):  47%|████▋     | 312865/665298 [02:57<12:31, 468.85 examples/s]Map (num_proc=32):  47%|████▋     | 313169/665298 [02:57<09:36, 610.83 examples/s]Map (num_proc=32):  47%|████▋     | 313473/665298 [02:57<07:26, 788.17 examples/s]Map (num_proc=32):  47%|████▋     | 313769/665298 [02:57<05:56, 984.96 examples/s]Map (num_proc=32):  47%|████▋     | 314171/665298 [02:58<04:36, 1269.01 examples/s]Map (num_proc=32):  47%|████▋     | 314614/665298 [02:58<03:40, 1588.86 examples/s]Map (num_proc=32):  47%|████▋     | 315016/665298 [02:58<03:14, 1805.52 examples/s]Map (num_proc=32):  47%|████▋     | 315326/665298 [02:58<02:53, 2022.60 examples/s]Map (num_proc=32):  47%|████▋     | 315635/665298 [02:58<02:36, 2227.98 examples/s]Map (num_proc=32):  47%|████▋     | 315944/665298 [02:58<04:04, 1428.49 examples/s]Map (num_proc=32):  48%|████▊     | 316172/665298 [02:59<04:54, 1186.25 examples/s]Map (num_proc=32):  48%|████▊     | 316376/665298 [02:59<05:25, 1072.52 examples/s]Map (num_proc=32):  48%|████▊     | 316530/665298 [02:59<05:50, 994.31 examples/s] Map (num_proc=32):  48%|████▊     | 316687/665298 [02:59<06:15, 928.35 examples/s]Map (num_proc=32):  48%|████▊     | 316802/665298 [03:00<06:29, 894.98 examples/s]Map (num_proc=32):  48%|████▊     | 316933/665298 [03:00<07:50, 740.59 examples/s]Map (num_proc=32):  48%|████▊     | 317055/665298 [03:00<07:35, 764.79 examples/s]Map (num_proc=32):  48%|████▊     | 317172/665298 [03:00<07:32, 769.50 examples/s]Map (num_proc=32):  48%|████▊     | 317289/665298 [03:00<07:32, 769.65 examples/s]Map (num_proc=32):  48%|████▊     | 317411/665298 [03:00<07:26, 778.43 examples/s]Map (num_proc=32):  48%|████▊     | 317522/665298 [03:01<07:33, 767.57 examples/s]Map (num_proc=32):  48%|████▊     | 317602/665298 [03:01<07:29, 773.11 examples/s]Map (num_proc=32):  48%|████▊     | 317713/665298 [03:01<07:36, 762.07 examples/s]Map (num_proc=32):  48%|████▊     | 317791/665298 [03:01<07:34, 764.46 examples/s]Map (num_proc=32):  48%|████▊     | 317903/665298 [03:01<09:04, 637.66 examples/s]Map (num_proc=32):  48%|████▊     | 317983/665298 [03:01<08:38, 669.60 examples/s]Map (num_proc=32):  48%|████▊     | 318068/665298 [03:01<08:08, 710.23 examples/s]Map (num_proc=32):  48%|████▊     | 318148/665298 [03:02<07:57, 727.51 examples/s]Map (num_proc=32):  48%|████▊     | 318226/665298 [03:02<07:49, 739.83 examples/s]Map (num_proc=32):  48%|████▊     | 318312/665298 [03:02<07:31, 768.21 examples/s]Map (num_proc=32):  48%|████▊     | 318427/665298 [03:02<07:35, 761.58 examples/s]Map (num_proc=32):  48%|████▊     | 318508/665298 [03:02<07:30, 770.34 examples/s]Map (num_proc=32):  48%|████▊     | 318590/665298 [03:02<07:24, 779.82 examples/s]Map (num_proc=32):  48%|████▊     | 318671/665298 [03:02<07:20, 786.66 examples/s]Map (num_proc=32):  48%|████▊     | 318788/665298 [03:02<07:23, 781.19 examples/s]Map (num_proc=32):  48%|████▊     | 318905/665298 [03:03<08:45, 659.15 examples/s]Map (num_proc=32):  48%|████▊     | 318980/665298 [03:03<08:30, 678.29 examples/s]Map (num_proc=32):  48%|████▊     | 319056/665298 [03:03<08:17, 696.33 examples/s]Map (num_proc=32):  48%|████▊     | 319134/665298 [03:03<08:04, 715.16 examples/s]Map (num_proc=32):  48%|████▊     | 319217/665298 [03:03<07:46, 742.55 examples/s]Map (num_proc=32):  48%|████▊     | 319333/665298 [03:03<07:43, 746.75 examples/s]Map (num_proc=32):  48%|████▊     | 319411/665298 [03:03<07:38, 753.65 examples/s]Map (num_proc=32):  48%|████▊     | 319526/665298 [03:03<07:38, 753.41 examples/s]Map (num_proc=32):  48%|████▊     | 319616/665298 [03:03<07:18, 787.96 examples/s]Map (num_proc=32):  48%|████▊     | 319731/665298 [03:04<08:04, 713.36 examples/s]Map (num_proc=32):  48%|████▊     | 319856/665298 [03:04<07:15, 793.43 examples/s]Map (num_proc=32):  48%|████▊     | 319940/665298 [03:04<08:39, 664.27 examples/s]Map (num_proc=32):  48%|████▊     | 320020/665298 [03:04<08:18, 692.88 examples/s]Map (num_proc=32):  48%|████▊     | 320095/665298 [03:04<08:16, 695.46 examples/s]Map (num_proc=32):  48%|████▊     | 320173/665298 [03:04<08:02, 715.88 examples/s]Map (num_proc=32):  48%|████▊     | 320253/665298 [03:04<07:49, 734.36 examples/s]Map (num_proc=32):  48%|████▊     | 320337/665298 [03:05<12:12, 471.10 examples/s]Map (num_proc=32):  48%|████▊     | 320607/665298 [03:05<06:38, 864.60 examples/s]Map (num_proc=32):  48%|████▊     | 320727/665298 [03:05<06:52, 835.97 examples/s]Map (num_proc=32):  48%|████▊     | 320834/665298 [03:05<07:12, 797.17 examples/s]Map (num_proc=32):  48%|████▊     | 320943/665298 [03:05<08:39, 663.03 examples/s]Map (num_proc=32):  48%|████▊     | 321022/665298 [03:05<08:22, 685.64 examples/s]Map (num_proc=32):  48%|████▊     | 321100/665298 [03:06<08:15, 694.49 examples/s]Map (num_proc=32):  48%|████▊     | 321181/665298 [03:06<07:58, 719.46 examples/s]Map (num_proc=32):  48%|████▊     | 321262/665298 [03:06<07:44, 741.33 examples/s]Map (num_proc=32):  48%|████▊     | 321377/665298 [03:06<07:34, 757.12 examples/s]Map (num_proc=32):  48%|████▊     | 321494/665298 [03:06<07:31, 761.74 examples/s]Map (num_proc=32):  48%|████▊     | 321612/665298 [03:06<07:42, 743.10 examples/s]Map (num_proc=32):  48%|████▊     | 321734/665298 [03:06<07:31, 760.70 examples/s]Map (num_proc=32):  48%|████▊     | 321850/665298 [03:07<07:26, 768.59 examples/s]Map (num_proc=32):  48%|████▊     | 321935/665298 [03:07<09:07, 626.64 examples/s]Map (num_proc=32):  48%|████▊     | 322011/665298 [03:07<08:46, 651.65 examples/s]Map (num_proc=32):  48%|████▊     | 322135/665298 [03:07<08:04, 708.17 examples/s]Map (num_proc=32):  48%|████▊     | 322215/665298 [03:07<07:51, 727.25 examples/s]Map (num_proc=32):  48%|████▊     | 322367/665298 [03:07<06:18, 905.36 examples/s]Map (num_proc=32):  48%|████▊     | 322568/665298 [03:07<05:15, 1087.68 examples/s]Map (num_proc=32):  49%|████▊     | 322775/665298 [03:07<04:35, 1241.76 examples/s]Map (num_proc=32):  49%|████▊     | 322978/665298 [03:08<04:15, 1342.09 examples/s]Map (num_proc=32):  49%|████▊     | 323170/665298 [03:08<03:57, 1439.83 examples/s]Map (num_proc=32):  49%|████▊     | 323335/665298 [03:08<03:49, 1490.52 examples/s]Map (num_proc=32):  49%|████▊     | 323495/665298 [03:08<06:48, 836.45 examples/s] Map (num_proc=32):  49%|████▊     | 323713/665298 [03:08<05:20, 1067.05 examples/s]Map (num_proc=32):  49%|████▊     | 324029/665298 [03:08<03:52, 1465.16 examples/s]Map (num_proc=32):  49%|████▉     | 324346/665298 [03:09<03:08, 1811.15 examples/s]Map (num_proc=32):  49%|████▉     | 324592/665298 [03:09<03:17, 1728.93 examples/s]Map (num_proc=32):  49%|████▉     | 324832/665298 [03:09<03:22, 1684.93 examples/s]Map (num_proc=32):  49%|████▉     | 325054/665298 [03:09<03:30, 1617.63 examples/s]Map (num_proc=32):  49%|████▉     | 325258/665298 [03:09<03:35, 1574.41 examples/s]Map (num_proc=32):  49%|████▉     | 325453/665298 [03:09<03:46, 1501.21 examples/s]Map (num_proc=32):  49%|████▉     | 325627/665298 [03:10<05:13, 1082.36 examples/s]Map (num_proc=32):  49%|████▉     | 326018/665298 [03:10<03:33, 1591.00 examples/s]Map (num_proc=32):  49%|████▉     | 326255/665298 [03:10<03:43, 1519.01 examples/s]Map (num_proc=32):  49%|████▉     | 326445/665298 [03:10<03:49, 1478.37 examples/s]Map (num_proc=32):  49%|████▉     | 326637/665298 [03:10<03:40, 1532.90 examples/s]Map (num_proc=32):  49%|████▉     | 326843/665298 [03:10<03:39, 1538.61 examples/s]Map (num_proc=32):  49%|████▉     | 327038/665298 [03:10<03:37, 1558.21 examples/s]Map (num_proc=32):  49%|████▉     | 327235/665298 [03:10<03:40, 1532.39 examples/s]Map (num_proc=32):  49%|████▉     | 327431/665298 [03:11<04:20, 1297.21 examples/s]Map (num_proc=32):  49%|████▉     | 327607/665298 [03:11<04:19, 1302.02 examples/s]Map (num_proc=32):  49%|████▉     | 327768/665298 [03:11<04:30, 1249.77 examples/s]Map (num_proc=32):  49%|████▉     | 327932/665298 [03:11<04:13, 1331.20 examples/s]Map (num_proc=32):  49%|████▉     | 328102/665298 [03:11<04:31, 1241.18 examples/s]Map (num_proc=32):  49%|████▉     | 328260/665298 [03:11<05:00, 1120.50 examples/s]Map (num_proc=32):  49%|████▉     | 328569/665298 [03:12<03:41, 1518.28 examples/s]Map (num_proc=32):  49%|████▉     | 328769/665298 [03:12<03:39, 1535.25 examples/s]Map (num_proc=32):  49%|████▉     | 328961/665298 [03:12<03:41, 1520.73 examples/s]Map (num_proc=32):  49%|████▉     | 329121/665298 [03:12<03:39, 1534.53 examples/s]Map (num_proc=32):  49%|████▉     | 329286/665298 [03:12<03:35, 1558.48 examples/s]Map (num_proc=32):  50%|████▉     | 329446/665298 [03:12<03:34, 1563.20 examples/s]Map (num_proc=32):  50%|████▉     | 329642/665298 [03:12<03:32, 1575.98 examples/s]Map (num_proc=32):  50%|████▉     | 329802/665298 [03:12<03:32, 1580.50 examples/s]Map (num_proc=32):  50%|████▉     | 329979/665298 [03:13<04:21, 1279.95 examples/s]Map (num_proc=32):  50%|████▉     | 330138/665298 [03:13<04:08, 1350.21 examples/s]Map (num_proc=32):  50%|████▉     | 330300/665298 [03:13<03:56, 1413.68 examples/s]Map (num_proc=32):  50%|████▉     | 330450/665298 [03:13<03:53, 1432.46 examples/s]Map (num_proc=32):  50%|████▉     | 330615/665298 [03:13<03:44, 1487.99 examples/s]Map (num_proc=32):  50%|████▉     | 330781/665298 [03:13<03:39, 1526.88 examples/s]Map (num_proc=32):  50%|████▉     | 330942/665298 [03:13<03:36, 1542.56 examples/s]Map (num_proc=32):  50%|████▉     | 331109/665298 [03:13<03:32, 1569.88 examples/s]Map (num_proc=32):  50%|████▉     | 331301/665298 [03:13<03:34, 1557.01 examples/s]Map (num_proc=32):  50%|████▉     | 331500/665298 [03:13<03:34, 1556.66 examples/s]Map (num_proc=32):  50%|████▉     | 331662/665298 [03:14<03:32, 1568.05 examples/s]Map (num_proc=32):  50%|████▉     | 331825/665298 [03:14<03:31, 1580.36 examples/s]Map (num_proc=32):  50%|████▉     | 332023/665298 [03:14<04:19, 1285.97 examples/s]Map (num_proc=32):  50%|████▉     | 332189/665298 [03:14<04:03, 1370.38 examples/s]Map (num_proc=32):  50%|████▉     | 332361/665298 [03:14<03:49, 1452.36 examples/s]Map (num_proc=32):  50%|████▉     | 332520/665298 [03:14<03:44, 1481.82 examples/s]Map (num_proc=32):  50%|█████     | 332684/665298 [03:14<03:39, 1517.84 examples/s]Map (num_proc=32):  50%|█████     | 332850/665298 [03:14<03:34, 1553.09 examples/s]Map (num_proc=32):  50%|█████     | 333015/665298 [03:15<03:31, 1572.19 examples/s]Map (num_proc=32):  50%|█████     | 333210/665298 [03:15<03:32, 1565.25 examples/s]Map (num_proc=32):  50%|█████     | 333376/665298 [03:15<03:29, 1584.41 examples/s]Map (num_proc=32):  50%|█████     | 333575/665298 [03:15<03:30, 1578.47 examples/s]Map (num_proc=32):  50%|█████     | 333735/665298 [03:15<03:29, 1582.19 examples/s]Map (num_proc=32):  50%|█████     | 333905/665298 [03:15<04:38, 1188.51 examples/s]Map (num_proc=32):  50%|█████     | 334070/665298 [03:15<04:17, 1283.92 examples/s]Map (num_proc=32):  50%|█████     | 334263/665298 [03:15<03:50, 1435.88 examples/s]Map (num_proc=32):  50%|█████     | 334421/665298 [03:16<03:51, 1428.88 examples/s]Map (num_proc=32):  50%|█████     | 334622/665298 [03:16<03:49, 1439.38 examples/s]Map (num_proc=32):  50%|█████     | 334785/665298 [03:16<03:45, 1463.74 examples/s]Map (num_proc=32):  50%|█████     | 334975/665298 [03:16<03:36, 1527.46 examples/s]Map (num_proc=32):  50%|█████     | 335137/665298 [03:16<03:33, 1547.01 examples/s]Map (num_proc=32):  50%|█████     | 335352/665298 [03:16<03:34, 1535.61 examples/s]Map (num_proc=32):  50%|█████     | 335516/665298 [03:16<03:31, 1556.01 examples/s]Map (num_proc=32):  50%|█████     | 335679/665298 [03:16<03:30, 1567.28 examples/s]Map (num_proc=32):  50%|█████     | 335857/665298 [03:16<03:57, 1389.88 examples/s]Map (num_proc=32):  51%|█████     | 336025/665298 [03:17<04:06, 1334.97 examples/s]Map (num_proc=32):  51%|█████     | 336183/665298 [03:17<06:26, 852.11 examples/s] Map (num_proc=32):  51%|█████     | 336594/665298 [03:17<03:50, 1423.02 examples/s]Map (num_proc=32):  51%|█████     | 336947/665298 [03:17<03:03, 1789.86 examples/s]Map (num_proc=32):  51%|█████     | 337192/665298 [03:17<03:20, 1638.61 examples/s]Map (num_proc=32):  51%|█████     | 337427/665298 [03:18<03:23, 1612.19 examples/s]Map (num_proc=32):  51%|█████     | 337642/665298 [03:18<03:20, 1634.33 examples/s]Map (num_proc=32):  51%|█████     | 337834/665298 [03:18<03:22, 1618.53 examples/s]Map (num_proc=32):  51%|█████     | 338021/665298 [03:18<04:05, 1332.43 examples/s]Map (num_proc=32):  51%|█████     | 338176/665298 [03:18<03:58, 1372.93 examples/s]Map (num_proc=32):  51%|█████     | 338328/665298 [03:18<03:52, 1404.61 examples/s]Map (num_proc=32):  51%|█████     | 338485/665298 [03:18<03:48, 1431.48 examples/s]Map (num_proc=32):  51%|█████     | 338650/665298 [03:18<04:01, 1352.90 examples/s]Map (num_proc=32):  51%|█████     | 338828/665298 [03:19<03:53, 1398.81 examples/s]Map (num_proc=32):  51%|█████     | 338990/665298 [03:19<03:45, 1448.65 examples/s]Map (num_proc=32):  51%|█████     | 339192/665298 [03:19<03:43, 1460.27 examples/s]Map (num_proc=32):  51%|█████     | 339433/665298 [03:19<03:14, 1674.22 examples/s]Map (num_proc=32):  51%|█████     | 339674/665298 [03:19<02:54, 1862.60 examples/s]Map (num_proc=32):  51%|█████     | 339965/665298 [03:19<02:32, 2137.79 examples/s]Map (num_proc=32):  51%|█████     | 340204/665298 [03:19<02:31, 2145.32 examples/s]Map (num_proc=32):  51%|█████     | 340463/665298 [03:19<02:54, 1856.61 examples/s]Map (num_proc=32):  51%|█████     | 340748/665298 [03:19<02:37, 2060.97 examples/s]Map (num_proc=32):  51%|█████▏    | 341052/665298 [03:20<02:23, 2254.21 examples/s]Map (num_proc=32):  51%|█████▏    | 341307/665298 [03:20<02:19, 2319.46 examples/s]Map (num_proc=32):  51%|█████▏    | 341551/665298 [03:20<02:20, 2305.59 examples/s]Map (num_proc=32):  51%|█████▏    | 341814/665298 [03:20<02:15, 2389.85 examples/s]Map (num_proc=32):  51%|█████▏    | 342088/665298 [03:20<02:22, 2263.66 examples/s]Map (num_proc=32):  51%|█████▏    | 342333/665298 [03:20<02:21, 2278.38 examples/s]Map (num_proc=32):  52%|█████▏    | 342637/665298 [03:20<02:13, 2416.88 examples/s]Map (num_proc=32):  52%|█████▏    | 342890/665298 [03:20<02:13, 2407.79 examples/s]Map (num_proc=32):  52%|█████▏    | 343145/665298 [03:20<02:12, 2438.27 examples/s]Map (num_proc=32):  52%|█████▏    | 343405/665298 [03:21<04:30, 1189.73 examples/s]Map (num_proc=32):  52%|█████▏    | 343936/665298 [03:21<02:53, 1848.93 examples/s]Map (num_proc=32):  52%|█████▏    | 344351/665298 [03:21<02:21, 2266.62 examples/s]Map (num_proc=32):  52%|█████▏    | 344800/665298 [03:21<01:59, 2684.27 examples/s]Map (num_proc=32):  52%|█████▏    | 345153/665298 [03:21<01:52, 2853.53 examples/s]Map (num_proc=32):  52%|█████▏    | 345543/665298 [03:22<01:54, 2790.73 examples/s]Map (num_proc=32):  52%|█████▏    | 345871/665298 [03:22<02:02, 2602.98 examples/s]Map (num_proc=32):  52%|█████▏    | 346167/665298 [03:22<02:04, 2572.24 examples/s]Map (num_proc=32):  52%|█████▏    | 346467/665298 [03:22<02:35, 2051.81 examples/s]Map (num_proc=32):  52%|█████▏    | 346707/665298 [03:22<02:32, 2095.35 examples/s]Map (num_proc=32):  52%|█████▏    | 346943/665298 [03:23<09:09, 579.46 examples/s] Map (num_proc=32):  52%|█████▏    | 348569/665298 [03:24<02:54, 1812.59 examples/s]Map (num_proc=32):  52%|█████▏    | 349115/665298 [03:24<03:08, 1679.98 examples/s]Map (num_proc=32):  53%|█████▎    | 349559/665298 [03:24<03:07, 1685.62 examples/s]Map (num_proc=32):  53%|█████▎    | 349940/665298 [03:24<03:02, 1723.59 examples/s]Map (num_proc=32):  53%|█████▎    | 350262/665298 [03:25<02:55, 1795.33 examples/s]Map (num_proc=32):  53%|█████▎    | 350580/665298 [03:25<03:01, 1732.68 examples/s]Map (num_proc=32):  53%|█████▎    | 350823/665298 [03:25<03:03, 1712.85 examples/s]Map (num_proc=32):  53%|█████▎    | 351095/665298 [03:25<03:03, 1708.29 examples/s]Map (num_proc=32):  53%|█████▎    | 351300/665298 [03:25<02:57, 1765.78 examples/s]Map (num_proc=32):  53%|█████▎    | 351517/665298 [03:25<02:49, 1846.48 examples/s]Map (num_proc=32):  53%|█████▎    | 351727/665298 [03:25<02:44, 1902.94 examples/s]Map (num_proc=32):  53%|█████▎    | 351973/665298 [03:25<02:51, 1827.87 examples/s]Map (num_proc=32):  53%|█████▎    | 352204/665298 [03:26<03:00, 1730.52 examples/s]Map (num_proc=32):  53%|█████▎    | 352413/665298 [03:26<02:53, 1806.26 examples/s]Map (num_proc=32):  53%|█████▎    | 352641/665298 [03:26<02:43, 1915.60 examples/s]Map (num_proc=32):  53%|█████▎    | 352851/665298 [03:26<02:39, 1958.94 examples/s]Map (num_proc=32):  53%|█████▎    | 353090/665298 [03:26<02:30, 2069.82 examples/s]Map (num_proc=32):  53%|█████▎    | 353316/665298 [03:26<02:27, 2119.23 examples/s]Map (num_proc=32):  53%|█████▎    | 353581/665298 [03:26<02:37, 1978.02 examples/s]Map (num_proc=32):  53%|█████▎    | 353785/665298 [03:26<02:48, 1853.13 examples/s]Map (num_proc=32):  53%|█████▎    | 354005/665298 [03:27<03:27, 1497.71 examples/s]Map (num_proc=32):  53%|█████▎    | 354523/665298 [03:27<02:25, 2142.59 examples/s]Map (num_proc=32):  53%|█████▎    | 354835/665298 [03:27<02:14, 2301.94 examples/s]Map (num_proc=32):  53%|█████▎    | 355079/665298 [03:27<02:13, 2325.61 examples/s]Map (num_proc=32):  53%|█████▎    | 355332/665298 [03:27<02:11, 2361.55 examples/s]Map (num_proc=32):  53%|█████▎    | 355636/665298 [03:27<02:18, 2240.42 examples/s]Map (num_proc=32):  53%|█████▎    | 355890/665298 [03:27<02:14, 2307.07 examples/s]Map (num_proc=32):  54%|█████▎    | 356227/665298 [03:27<02:13, 2323.08 examples/s]Map (num_proc=32):  54%|█████▎    | 356475/665298 [03:28<02:12, 2326.94 examples/s]Map (num_proc=32):  54%|█████▎    | 356735/665298 [03:28<02:12, 2332.37 examples/s]Map (num_proc=32):  54%|█████▎    | 356987/665298 [03:28<02:24, 2136.44 examples/s]Map (num_proc=32):  54%|█████▎    | 357236/665298 [03:28<02:19, 2215.21 examples/s]Map (num_proc=32):  54%|█████▎    | 357506/665298 [03:28<02:11, 2341.19 examples/s]Map (num_proc=32):  54%|█████▍    | 357784/665298 [03:28<02:05, 2450.03 examples/s]Map (num_proc=32):  54%|█████▍    | 358052/665298 [03:28<02:02, 2507.35 examples/s]Map (num_proc=32):  54%|█████▍    | 358319/665298 [03:28<02:02, 2497.09 examples/s]Map (num_proc=32):  54%|█████▍    | 358593/665298 [03:28<01:59, 2560.63 examples/s]Map (num_proc=32):  54%|█████▍    | 358874/665298 [03:29<01:56, 2627.20 examples/s]Map (num_proc=32):  54%|█████▍    | 359149/665298 [03:29<01:56, 2617.40 examples/s]Map (num_proc=32):  54%|█████▍    | 359443/665298 [03:29<02:33, 1992.42 examples/s]Map (num_proc=32):  54%|█████▍    | 359677/665298 [03:29<02:52, 1768.91 examples/s]Map (num_proc=32):  54%|█████▍    | 359884/665298 [03:29<03:05, 1645.76 examples/s]Map (num_proc=32):  54%|█████▍    | 360071/665298 [03:29<03:10, 1601.37 examples/s]Map (num_proc=32):  54%|█████▍    | 360255/665298 [03:30<03:28, 1466.03 examples/s]Map (num_proc=32):  54%|█████▍    | 360423/665298 [03:30<05:07, 991.55 examples/s] Map (num_proc=32):  54%|█████▍    | 360752/665298 [03:30<03:41, 1371.91 examples/s]Map (num_proc=32):  54%|█████▍    | 360999/665298 [03:30<03:14, 1566.51 examples/s]Map (num_proc=32):  54%|█████▍    | 361222/665298 [03:30<03:36, 1401.69 examples/s]Map (num_proc=32):  54%|█████▍    | 361452/665298 [03:30<03:16, 1543.32 examples/s]Map (num_proc=32):  54%|█████▍    | 361688/665298 [03:30<03:00, 1685.31 examples/s]Map (num_proc=32):  54%|█████▍    | 361910/665298 [03:31<03:00, 1682.47 examples/s]Map (num_proc=32):  54%|█████▍    | 362104/665298 [03:31<02:54, 1735.35 examples/s]Map (num_proc=32):  54%|█████▍    | 362302/665298 [03:31<02:49, 1790.76 examples/s]Map (num_proc=32):  54%|█████▍    | 362529/665298 [03:31<02:44, 1840.69 examples/s]Map (num_proc=32):  55%|█████▍    | 362724/665298 [03:31<02:44, 1838.69 examples/s]Map (num_proc=32):  55%|█████▍    | 362924/665298 [03:31<02:40, 1878.78 examples/s]Map (num_proc=32):  55%|█████▍    | 363125/665298 [03:31<02:54, 1728.89 examples/s]Map (num_proc=32):  55%|█████▍    | 363316/665298 [03:31<03:01, 1660.12 examples/s]Map (num_proc=32):  55%|█████▍    | 363512/665298 [03:32<02:54, 1727.61 examples/s]Map (num_proc=32):  55%|█████▍    | 363715/665298 [03:32<02:48, 1794.43 examples/s]Map (num_proc=32):  55%|█████▍    | 363907/665298 [03:32<02:46, 1815.08 examples/s]Map (num_proc=32):  55%|█████▍    | 364112/665298 [03:32<02:41, 1869.77 examples/s]Map (num_proc=32):  55%|█████▍    | 364315/665298 [03:32<02:37, 1905.84 examples/s]Map (num_proc=32):  55%|█████▍    | 364512/665298 [03:32<02:37, 1913.36 examples/s]Map (num_proc=32):  55%|█████▍    | 364711/665298 [03:32<02:35, 1929.34 examples/s]Map (num_proc=32):  55%|█████▍    | 364906/665298 [03:32<02:47, 1795.76 examples/s]Map (num_proc=32):  55%|█████▍    | 365122/665298 [03:32<02:40, 1868.85 examples/s]Map (num_proc=32):  55%|█████▍    | 365319/665298 [03:32<02:41, 1860.42 examples/s]Map (num_proc=32):  55%|█████▍    | 365521/665298 [03:33<02:57, 1689.67 examples/s]Map (num_proc=32):  55%|█████▍    | 365709/665298 [03:33<02:53, 1728.78 examples/s]Map (num_proc=32):  55%|█████▍    | 365904/665298 [03:33<02:48, 1776.20 examples/s]Map (num_proc=32):  55%|█████▌    | 366145/665298 [03:33<02:39, 1878.60 examples/s]Map (num_proc=32):  55%|█████▌    | 366336/665298 [03:33<02:39, 1875.90 examples/s]Map (num_proc=32):  55%|█████▌    | 366535/665298 [03:33<02:43, 1831.43 examples/s]Map (num_proc=32):  55%|█████▌    | 366741/665298 [03:33<02:52, 1733.86 examples/s]Map (num_proc=32):  55%|█████▌    | 366930/665298 [03:33<02:50, 1745.35 examples/s]Map (num_proc=32):  55%|█████▌    | 367157/665298 [03:33<02:39, 1869.37 examples/s]Map (num_proc=32):  55%|█████▌    | 367359/665298 [03:34<03:21, 1477.58 examples/s]Map (num_proc=32):  55%|█████▌    | 367784/665298 [03:34<02:21, 2107.64 examples/s]Map (num_proc=32):  55%|█████▌    | 368022/665298 [03:34<02:51, 1733.17 examples/s]Map (num_proc=32):  55%|█████▌    | 368239/665298 [03:34<02:45, 1797.25 examples/s]Map (num_proc=32):  55%|█████▌    | 368458/665298 [03:34<02:56, 1680.91 examples/s]Map (num_proc=32):  55%|█████▌    | 368659/665298 [03:34<02:49, 1752.88 examples/s]Map (num_proc=32):  55%|█████▌    | 368856/665298 [03:34<02:44, 1800.46 examples/s]Map (num_proc=32):  55%|█████▌    | 369052/665298 [03:35<02:40, 1840.27 examples/s]Map (num_proc=32):  56%|█████▌    | 369252/665298 [03:35<02:38, 1872.76 examples/s]Map (num_proc=32):  56%|█████▌    | 369455/665298 [03:35<02:34, 1911.26 examples/s]Map (num_proc=32):  56%|█████▌    | 369657/665298 [03:35<02:55, 1686.84 examples/s]Map (num_proc=32):  56%|█████▌    | 369854/665298 [03:35<03:17, 1495.21 examples/s]Map (num_proc=32):  56%|█████▌    | 370017/665298 [03:35<04:01, 1222.59 examples/s]Map (num_proc=32):  56%|█████▌    | 370155/665298 [03:36<07:02, 698.44 examples/s] Map (num_proc=32):  56%|█████▌    | 370279/665298 [03:37<16:43, 293.95 examples/s]Map (num_proc=32):  56%|█████▌    | 371253/665298 [03:37<05:00, 977.72 examples/s]Map (num_proc=32):  56%|█████▌    | 371584/665298 [03:38<05:20, 915.38 examples/s]Map (num_proc=32):  56%|█████▌    | 371834/665298 [03:38<04:47, 1019.81 examples/s]Map (num_proc=32):  56%|█████▌    | 372080/665298 [03:38<05:45, 849.65 examples/s] Map (num_proc=32):  56%|█████▌    | 372281/665298 [03:38<05:07, 951.73 examples/s]Map (num_proc=32):  56%|█████▌    | 372477/665298 [03:38<04:32, 1076.27 examples/s]Map (num_proc=32):  56%|█████▌    | 372668/665298 [03:38<04:19, 1126.58 examples/s]Map (num_proc=32):  56%|█████▌    | 372839/665298 [03:39<04:19, 1125.00 examples/s]Map (num_proc=32):  56%|█████▌    | 373004/665298 [03:39<04:22, 1112.40 examples/s]Map (num_proc=32):  56%|█████▌    | 373152/665298 [03:39<04:56, 985.33 examples/s] Map (num_proc=32):  56%|█████▌    | 373290/665298 [03:39<04:44, 1026.77 examples/s]Map (num_proc=32):  56%|█████▌    | 373429/665298 [03:39<04:44, 1024.90 examples/s]Map (num_proc=32):  56%|█████▌    | 373566/665298 [03:39<04:36, 1054.73 examples/s]Map (num_proc=32):  56%|█████▌    | 373703/665298 [03:40<04:38, 1046.46 examples/s]Map (num_proc=32):  56%|█████▌    | 373816/665298 [03:40<04:46, 1016.75 examples/s]Map (num_proc=32):  56%|█████▌    | 373934/665298 [03:40<05:09, 940.63 examples/s] Map (num_proc=32):  56%|█████▌    | 374043/665298 [03:40<05:01, 967.30 examples/s]Map (num_proc=32):  56%|█████▌    | 374157/665298 [03:40<04:48, 1008.51 examples/s]Map (num_proc=32):  56%|█████▋    | 374274/665298 [03:40<04:39, 1041.49 examples/s]Map (num_proc=32):  56%|█████▋    | 374383/665298 [03:40<04:36, 1052.37 examples/s]Map (num_proc=32):  56%|█████▋    | 374498/665298 [03:40<04:29, 1077.15 examples/s]Map (num_proc=32):  56%|█████▋    | 374639/665298 [03:40<04:13, 1148.53 examples/s]Map (num_proc=32):  56%|█████▋    | 374780/665298 [03:41<04:29, 1077.65 examples/s]Map (num_proc=32):  56%|█████▋    | 374895/665298 [03:41<04:26, 1089.51 examples/s]Map (num_proc=32):  56%|█████▋    | 375034/665298 [03:41<04:12, 1149.42 examples/s]Map (num_proc=32):  56%|█████▋    | 375170/665298 [03:41<05:08, 940.78 examples/s] Map (num_proc=32):  56%|█████▋    | 375285/665298 [03:41<04:54, 984.11 examples/s]Map (num_proc=32):  56%|█████▋    | 375402/665298 [03:41<07:46, 621.18 examples/s]Map (num_proc=32):  56%|█████▋    | 375603/665298 [03:42<05:34, 864.90 examples/s]Map (num_proc=32):  56%|█████▋    | 375800/665298 [03:42<04:29, 1073.99 examples/s]Map (num_proc=32):  57%|█████▋    | 375971/665298 [03:42<04:01, 1199.53 examples/s]Map (num_proc=32):  57%|█████▋    | 376259/665298 [03:42<03:01, 1588.65 examples/s]Map (num_proc=32):  57%|█████▋    | 376461/665298 [03:42<02:52, 1671.83 examples/s]Map (num_proc=32):  57%|█████▋    | 376661/665298 [03:42<02:44, 1755.19 examples/s]Map (num_proc=32):  57%|█████▋    | 376886/665298 [03:42<02:34, 1869.26 examples/s]Map (num_proc=32):  57%|█████▋    | 377137/665298 [03:42<02:22, 2028.90 examples/s]Map (num_proc=32):  57%|█████▋    | 377366/665298 [03:42<02:28, 1937.96 examples/s]Map (num_proc=32):  57%|█████▋    | 377568/665298 [03:42<02:33, 1871.50 examples/s]Map (num_proc=32):  57%|█████▋    | 377762/665298 [03:43<02:38, 1813.01 examples/s]Map (num_proc=32):  57%|█████▋    | 377956/665298 [03:43<02:46, 1728.40 examples/s]Map (num_proc=32):  57%|█████▋    | 378155/665298 [03:43<03:02, 1571.42 examples/s]Map (num_proc=32):  57%|█████▋    | 378321/665298 [03:43<03:00, 1588.51 examples/s]Map (num_proc=32):  57%|█████▋    | 378490/665298 [03:43<03:08, 1522.51 examples/s]Map (num_proc=32):  57%|█████▋    | 378646/665298 [03:43<03:09, 1516.10 examples/s]Map (num_proc=32):  57%|█████▋    | 378804/665298 [03:43<03:25, 1392.40 examples/s]Map (num_proc=32):  57%|█████▋    | 378969/665298 [03:43<03:33, 1342.70 examples/s]Map (num_proc=32):  57%|█████▋    | 379169/665298 [03:44<03:09, 1506.70 examples/s]Map (num_proc=32):  57%|█████▋    | 379365/665298 [03:44<03:03, 1554.81 examples/s]Map (num_proc=32):  57%|█████▋    | 379535/665298 [03:44<03:00, 1580.41 examples/s]Map (num_proc=32):  57%|█████▋    | 379707/665298 [03:44<02:56, 1618.59 examples/s]Map (num_proc=32):  57%|█████▋    | 379880/665298 [03:44<02:53, 1644.47 examples/s]Map (num_proc=32):  57%|█████▋    | 380052/665298 [03:44<02:51, 1661.44 examples/s]Map (num_proc=32):  57%|█████▋    | 380221/665298 [03:44<02:51, 1666.63 examples/s]Map (num_proc=32):  57%|█████▋    | 380419/665298 [03:44<02:42, 1747.77 examples/s]Map (num_proc=32):  57%|█████▋    | 380621/665298 [03:44<02:50, 1670.90 examples/s]Map (num_proc=32):  57%|█████▋    | 380815/665298 [03:45<02:54, 1629.27 examples/s]Map (num_proc=32):  57%|█████▋    | 380985/665298 [03:45<02:53, 1643.10 examples/s]Map (num_proc=32):  57%|█████▋    | 381152/665298 [03:45<02:53, 1639.64 examples/s]Map (num_proc=32):  57%|█████▋    | 381327/665298 [03:45<03:57, 1197.06 examples/s]Map (num_proc=32):  57%|█████▋    | 381637/665298 [03:45<03:07, 1514.38 examples/s]Map (num_proc=32):  57%|█████▋    | 381807/665298 [03:45<03:09, 1492.98 examples/s]Map (num_proc=32):  57%|█████▋    | 381975/665298 [03:45<03:04, 1532.36 examples/s]Map (num_proc=32):  57%|█████▋    | 382144/665298 [03:45<03:00, 1566.10 examples/s]Map (num_proc=32):  57%|█████▋    | 382316/665298 [03:46<02:57, 1598.75 examples/s]Map (num_proc=32):  57%|█████▋    | 382485/665298 [03:46<02:55, 1614.27 examples/s]Map (num_proc=32):  58%|█████▊    | 382652/665298 [03:46<02:53, 1626.08 examples/s]Map (num_proc=32):  58%|█████▊    | 382822/665298 [03:46<02:52, 1637.83 examples/s]Map (num_proc=32):  58%|█████▊    | 382993/665298 [03:46<02:50, 1654.04 examples/s]Map (num_proc=32):  58%|█████▊    | 383161/665298 [03:46<02:54, 1617.75 examples/s]Map (num_proc=32):  58%|█████▊    | 383333/665298 [03:46<02:51, 1642.12 examples/s]Map (num_proc=32):  58%|█████▊    | 383499/665298 [03:46<02:51, 1641.72 examples/s]Map (num_proc=32):  58%|█████▊    | 383676/665298 [03:46<03:12, 1460.40 examples/s]Map (num_proc=32):  58%|█████▊    | 383831/665298 [03:47<03:25, 1367.01 examples/s]Map (num_proc=32):  58%|█████▊    | 383997/665298 [03:47<04:06, 1143.37 examples/s]Map (num_proc=32):  58%|█████▊    | 384144/665298 [03:47<04:01, 1162.89 examples/s]Map (num_proc=32):  58%|█████▊    | 384283/665298 [03:47<04:38, 1010.02 examples/s]Map (num_proc=32):  58%|█████▊    | 384446/665298 [03:47<04:05, 1143.57 examples/s]Map (num_proc=32):  58%|█████▊    | 384619/665298 [03:47<03:39, 1277.28 examples/s]Map (num_proc=32):  58%|█████▊    | 384791/665298 [03:47<03:22, 1385.40 examples/s]Map (num_proc=32):  58%|█████▊    | 384963/665298 [03:47<03:10, 1468.91 examples/s]Map (num_proc=32):  58%|█████▊    | 385131/665298 [03:48<03:04, 1518.06 examples/s]Map (num_proc=32):  58%|█████▊    | 385299/665298 [03:48<03:00, 1547.47 examples/s]Map (num_proc=32):  58%|█████▊    | 385466/665298 [03:48<03:05, 1504.67 examples/s]Map (num_proc=32):  58%|█████▊    | 385634/665298 [03:48<03:17, 1414.23 examples/s]Map (num_proc=32):  58%|█████▊    | 385801/665298 [03:48<03:09, 1478.62 examples/s]Map (num_proc=32):  58%|█████▊    | 385969/665298 [03:48<03:03, 1522.72 examples/s]Map (num_proc=32):  58%|█████▊    | 386128/665298 [03:48<03:01, 1539.30 examples/s]Map (num_proc=32):  58%|█████▊    | 386295/665298 [03:48<02:57, 1569.49 examples/s]Map (num_proc=32):  58%|█████▊    | 386463/665298 [03:48<03:03, 1521.17 examples/s]Map (num_proc=32):  58%|█████▊    | 386628/665298 [03:49<04:36, 1006.17 examples/s]Map (num_proc=32):  58%|█████▊    | 386874/665298 [03:49<03:46, 1227.82 examples/s]Map (num_proc=32):  58%|█████▊    | 387035/665298 [03:49<03:56, 1177.46 examples/s]Map (num_proc=32):  58%|█████▊    | 387176/665298 [03:49<06:29, 713.89 examples/s] Map (num_proc=32):  58%|█████▊    | 387283/665298 [03:50<06:12, 746.20 examples/s]Map (num_proc=32):  58%|█████▊    | 387404/665298 [03:51<18:30, 250.27 examples/s]Map (num_proc=32):  58%|█████▊    | 387576/665298 [03:51<13:09, 351.80 examples/s]Map (num_proc=32):  58%|█████▊    | 387753/665298 [03:51<09:38, 479.53 examples/s]Map (num_proc=32):  58%|█████▊    | 387967/665298 [03:51<06:55, 667.75 examples/s]Map (num_proc=32):  58%|█████▊    | 388189/665298 [03:51<05:16, 876.31 examples/s]Map (num_proc=32):  58%|█████▊    | 388441/665298 [03:52<04:02, 1142.71 examples/s]Map (num_proc=32):  58%|█████▊    | 388714/665298 [03:52<03:14, 1419.62 examples/s]Map (num_proc=32):  58%|█████▊    | 388925/665298 [03:52<03:02, 1514.91 examples/s]Map (num_proc=32):  58%|█████▊    | 389144/665298 [03:52<02:47, 1653.31 examples/s]Map (num_proc=32):  59%|█████▊    | 389395/665298 [03:52<02:29, 1843.78 examples/s]Map (num_proc=32):  59%|█████▊    | 389665/665298 [03:52<02:15, 2039.07 examples/s]Map (num_proc=32):  59%|█████▊    | 389918/665298 [03:52<02:36, 1757.43 examples/s]Map (num_proc=32):  59%|█████▊    | 390133/665298 [03:52<02:40, 1714.72 examples/s]Map (num_proc=32):  59%|█████▊    | 390322/665298 [03:53<02:50, 1615.04 examples/s]Map (num_proc=32):  59%|█████▊    | 390521/665298 [03:53<03:14, 1416.08 examples/s]Map (num_proc=32):  59%|█████▊    | 390676/665298 [03:53<03:20, 1368.32 examples/s]Map (num_proc=32):  59%|█████▊    | 390848/665298 [03:53<03:34, 1276.67 examples/s]Map (num_proc=32):  59%|█████▉    | 390988/665298 [03:53<03:35, 1274.97 examples/s]Map (num_proc=32):  59%|█████▉    | 391137/665298 [03:53<03:32, 1289.27 examples/s]Map (num_proc=32):  59%|█████▉    | 391307/665298 [03:53<03:19, 1373.89 examples/s]Map (num_proc=32):  59%|█████▉    | 391475/665298 [03:53<03:08, 1449.61 examples/s]Map (num_proc=32):  59%|█████▉    | 391664/665298 [03:54<03:09, 1440.40 examples/s]Map (num_proc=32):  59%|█████▉    | 391860/665298 [03:54<02:58, 1530.43 examples/s]Map (num_proc=32):  59%|█████▉    | 392056/665298 [03:54<02:47, 1631.14 examples/s]Map (num_proc=32):  59%|█████▉    | 392247/665298 [03:54<05:05, 894.38 examples/s] Map (num_proc=32):  59%|█████▉    | 392544/665298 [03:54<03:41, 1230.21 examples/s]Map (num_proc=32):  59%|█████▉    | 392791/665298 [03:54<03:06, 1458.47 examples/s]Map (num_proc=32):  59%|█████▉    | 393051/665298 [03:55<02:41, 1689.51 examples/s]Map (num_proc=32):  59%|█████▉    | 393274/665298 [03:55<02:45, 1639.73 examples/s]Map (num_proc=32):  59%|█████▉    | 393467/665298 [03:55<02:47, 1626.05 examples/s]Map (num_proc=32):  59%|█████▉    | 393662/665298 [03:55<02:51, 1580.78 examples/s]Map (num_proc=32):  59%|█████▉    | 393834/665298 [03:55<02:48, 1607.56 examples/s]Map (num_proc=32):  59%|█████▉    | 394034/665298 [03:55<02:43, 1655.14 examples/s]Map (num_proc=32):  59%|█████▉    | 394210/665298 [03:55<02:58, 1521.14 examples/s]Map (num_proc=32):  59%|█████▉    | 394374/665298 [03:55<02:55, 1540.47 examples/s]Map (num_proc=32):  59%|█████▉    | 394539/665298 [03:55<02:53, 1564.20 examples/s]Map (num_proc=32):  59%|█████▉    | 394736/665298 [03:56<02:48, 1607.60 examples/s]Map (num_proc=32):  59%|█████▉    | 394904/665298 [03:56<02:55, 1539.60 examples/s]Map (num_proc=32):  59%|█████▉    | 395075/665298 [03:56<03:14, 1389.14 examples/s]Map (num_proc=32):  59%|█████▉    | 395241/665298 [03:56<03:51, 1167.08 examples/s]Map (num_proc=32):  59%|█████▉    | 395386/665298 [03:56<03:42, 1214.53 examples/s]Map (num_proc=32):  59%|█████▉    | 395555/665298 [03:56<03:24, 1321.83 examples/s]Map (num_proc=32):  59%|█████▉    | 395753/665298 [03:56<03:13, 1389.97 examples/s]Map (num_proc=32):  60%|█████▉    | 395946/665298 [03:57<03:03, 1471.86 examples/s]Map (num_proc=32):  60%|█████▉    | 396112/665298 [03:57<04:30, 996.22 examples/s] Map (num_proc=32):  60%|█████▉    | 396308/665298 [03:57<03:53, 1151.00 examples/s]Map (num_proc=32):  60%|█████▉    | 396561/665298 [03:57<03:09, 1419.64 examples/s]Map (num_proc=32):  60%|█████▉    | 396829/665298 [03:57<02:38, 1689.46 examples/s]Map (num_proc=32):  60%|█████▉    | 397041/665298 [03:57<02:58, 1503.83 examples/s]Map (num_proc=32):  60%|█████▉    | 397342/665298 [03:57<02:29, 1795.76 examples/s]Map (num_proc=32):  60%|█████▉    | 397558/665298 [03:58<02:34, 1734.29 examples/s]Map (num_proc=32):  60%|█████▉    | 397753/665298 [03:58<02:36, 1704.62 examples/s]Map (num_proc=32):  60%|█████▉    | 397947/665298 [03:58<02:40, 1670.67 examples/s]Map (num_proc=32):  60%|█████▉    | 398144/665298 [03:58<02:53, 1539.51 examples/s]Map (num_proc=32):  60%|█████▉    | 398305/665298 [03:58<02:52, 1548.69 examples/s]Map (num_proc=32):  60%|█████▉    | 398472/665298 [03:58<02:49, 1570.33 examples/s]Map (num_proc=32):  60%|█████▉    | 398639/665298 [03:58<02:47, 1594.99 examples/s]Map (num_proc=32):  60%|█████▉    | 398808/665298 [03:58<02:45, 1612.71 examples/s]Map (num_proc=32):  60%|█████▉    | 398981/665298 [03:58<02:43, 1630.71 examples/s]Map (num_proc=32):  60%|█████▉    | 399149/665298 [03:59<02:42, 1637.46 examples/s]Map (num_proc=32):  60%|██████    | 399320/665298 [03:59<02:41, 1644.40 examples/s]Map (num_proc=32):  60%|██████    | 399492/665298 [03:59<02:40, 1657.46 examples/s]Map (num_proc=32):  60%|██████    | 399664/665298 [03:59<02:46, 1596.55 examples/s]Map (num_proc=32):  60%|██████    | 399833/665298 [03:59<03:07, 1413.79 examples/s]Map (num_proc=32):  60%|██████    | 399989/665298 [03:59<03:14, 1362.05 examples/s]Map (num_proc=32):  60%|██████    | 400158/665298 [03:59<03:03, 1441.60 examples/s]Map (num_proc=32):  60%|██████    | 400326/665298 [03:59<02:56, 1497.72 examples/s]Map (num_proc=32):  60%|██████    | 400493/665298 [03:59<02:52, 1539.53 examples/s]Map (num_proc=32):  60%|██████    | 400662/665298 [04:00<02:48, 1575.08 examples/s]Map (num_proc=32):  60%|██████    | 400829/665298 [04:00<02:45, 1597.82 examples/s]Map (num_proc=32):  60%|██████    | 400991/665298 [04:00<02:45, 1594.59 examples/s]Map (num_proc=32):  60%|██████    | 401178/665298 [04:00<03:02, 1449.73 examples/s]Map (num_proc=32):  60%|██████    | 401345/665298 [04:00<02:55, 1501.41 examples/s]Map (num_proc=32):  60%|██████    | 401515/665298 [04:00<02:50, 1548.85 examples/s]Map (num_proc=32):  60%|██████    | 401683/665298 [04:00<02:46, 1582.05 examples/s]Map (num_proc=32):  60%|██████    | 401852/665298 [04:00<02:44, 1604.22 examples/s]Map (num_proc=32):  60%|██████    | 402019/665298 [04:00<02:42, 1615.48 examples/s]Map (num_proc=32):  60%|██████    | 402189/665298 [04:01<03:25, 1281.69 examples/s]Map (num_proc=32):  60%|██████    | 402413/665298 [04:01<02:56, 1488.95 examples/s]Map (num_proc=32):  61%|██████    | 402630/665298 [04:01<02:39, 1642.83 examples/s]Map (num_proc=32):  61%|██████    | 402820/665298 [04:01<02:47, 1563.03 examples/s]Map (num_proc=32):  61%|██████    | 402989/665298 [04:01<02:57, 1481.36 examples/s]Map (num_proc=32):  61%|██████    | 403152/665298 [04:01<02:59, 1457.96 examples/s]Map (num_proc=32):  61%|██████    | 403321/665298 [04:01<02:56, 1486.55 examples/s]Map (num_proc=32):  61%|██████    | 403519/665298 [04:01<02:43, 1600.63 examples/s]Map (num_proc=32):  61%|██████    | 403687/665298 [04:02<02:44, 1588.87 examples/s]Map (num_proc=32):  61%|██████    | 403858/665298 [04:02<02:43, 1599.73 examples/s]Map (num_proc=32):  61%|██████    | 404025/665298 [04:02<02:47, 1555.70 examples/s]Map (num_proc=32):  61%|██████    | 404186/665298 [04:02<03:00, 1447.54 examples/s]Map (num_proc=32):  61%|██████    | 404358/665298 [04:02<02:55, 1488.81 examples/s]Map (num_proc=32):  61%|██████    | 404529/665298 [04:02<02:49, 1538.90 examples/s]Map (num_proc=32):  61%|██████    | 404702/665298 [04:02<02:44, 1585.33 examples/s]Map (num_proc=32):  61%|██████    | 404874/665298 [04:02<02:41, 1612.29 examples/s]Map (num_proc=32):  61%|██████    | 405046/665298 [04:03<04:42, 920.02 examples/s] Map (num_proc=32):  61%|██████    | 405331/665298 [04:03<03:24, 1271.55 examples/s]Map (num_proc=32):  61%|██████    | 405586/665298 [04:03<02:48, 1539.07 examples/s]Map (num_proc=32):  61%|██████    | 405858/665298 [04:03<02:24, 1794.15 examples/s]Map (num_proc=32):  61%|██████    | 406097/665298 [04:03<02:25, 1784.45 examples/s]Map (num_proc=32):  61%|██████    | 406318/665298 [04:03<02:23, 1806.77 examples/s]Map (num_proc=32):  61%|██████    | 406544/665298 [04:03<02:30, 1719.07 examples/s]Map (num_proc=32):  61%|██████    | 406743/665298 [04:04<02:29, 1726.62 examples/s]Map (num_proc=32):  61%|██████    | 406941/665298 [04:04<02:33, 1679.91 examples/s]Map (num_proc=32):  61%|██████    | 407134/665298 [04:04<02:49, 1527.58 examples/s]Map (num_proc=32):  61%|██████    | 407330/665298 [04:04<02:45, 1560.15 examples/s]Map (num_proc=32):  61%|██████▏   | 407553/665298 [04:04<02:31, 1700.29 examples/s]Map (num_proc=32):  61%|██████▏   | 407803/665298 [04:04<02:19, 1850.87 examples/s]Map (num_proc=32):  61%|██████▏   | 408059/665298 [04:04<02:12, 1938.38 examples/s]Map (num_proc=32):  61%|██████▏   | 408315/665298 [04:04<02:07, 2010.63 examples/s]Map (num_proc=32):  61%|██████▏   | 408544/665298 [04:04<02:03, 2079.09 examples/s]Map (num_proc=32):  61%|██████▏   | 408798/665298 [04:05<01:58, 2166.97 examples/s]Map (num_proc=32):  61%|██████▏   | 409022/665298 [04:05<02:02, 2090.10 examples/s]Map (num_proc=32):  62%|██████▏   | 409250/665298 [04:05<02:10, 1968.05 examples/s]Map (num_proc=32):  62%|██████▏   | 409477/665298 [04:05<02:06, 2026.55 examples/s]Map (num_proc=32):  62%|██████▏   | 409699/665298 [04:05<02:16, 1878.65 examples/s]Map (num_proc=32):  62%|██████▏   | 409901/665298 [04:05<02:18, 1845.92 examples/s]Map (num_proc=32):  62%|██████▏   | 410127/665298 [04:05<02:11, 1938.97 examples/s]Map (num_proc=32):  62%|██████▏   | 410325/665298 [04:06<03:14, 1309.50 examples/s]Map (num_proc=32):  62%|██████▏   | 410548/665298 [04:06<02:52, 1473.90 examples/s]Map (num_proc=32):  62%|██████▏   | 410829/665298 [04:06<02:26, 1739.76 examples/s]Map (num_proc=32):  62%|██████▏   | 411049/665298 [04:06<02:18, 1833.46 examples/s]Map (num_proc=32):  62%|██████▏   | 411277/665298 [04:06<02:11, 1931.45 examples/s]Map (num_proc=32):  62%|██████▏   | 411510/665298 [04:06<02:07, 1995.38 examples/s]Map (num_proc=32):  62%|██████▏   | 411790/665298 [04:06<01:56, 2176.27 examples/s]Map (num_proc=32):  62%|██████▏   | 412037/665298 [04:06<01:56, 2170.48 examples/s]Map (num_proc=32):  62%|██████▏   | 412286/665298 [04:06<01:53, 2228.13 examples/s]Map (num_proc=32):  62%|██████▏   | 412539/665298 [04:07<01:49, 2298.09 examples/s]Map (num_proc=32):  62%|██████▏   | 412793/665298 [04:07<01:51, 2272.32 examples/s]Map (num_proc=32):  62%|██████▏   | 413040/665298 [04:07<03:51, 1091.34 examples/s]Map (num_proc=32):  62%|██████▏   | 413288/665298 [04:07<03:14, 1293.22 examples/s]Map (num_proc=32):  62%|██████▏   | 413539/665298 [04:07<02:48, 1490.43 examples/s]Map (num_proc=32):  62%|██████▏   | 413788/665298 [04:07<02:29, 1683.08 examples/s]Map (num_proc=32):  62%|██████▏   | 414043/665298 [04:08<02:15, 1856.84 examples/s]Map (num_proc=32):  62%|██████▏   | 414293/665298 [04:08<02:07, 1974.44 examples/s]Map (num_proc=32):  62%|██████▏   | 414542/665298 [04:08<02:00, 2084.96 examples/s]Map (num_proc=32):  62%|██████▏   | 414790/665298 [04:08<01:54, 2180.36 examples/s]Map (num_proc=32):  62%|██████▏   | 415045/665298 [04:08<01:51, 2253.78 examples/s]Map (num_proc=32):  62%|██████▏   | 415305/665298 [04:08<01:50, 2260.87 examples/s]Map (num_proc=32):  62%|██████▏   | 415565/665298 [04:08<01:50, 2265.88 examples/s]Map (num_proc=32):  63%|██████▎   | 415863/665298 [04:08<01:42, 2431.73 examples/s]Map (num_proc=32):  63%|██████▎   | 416145/665298 [04:08<01:40, 2471.92 examples/s]Map (num_proc=32):  63%|██████▎   | 416479/665298 [04:09<01:33, 2649.11 examples/s]Map (num_proc=32):  63%|██████▎   | 416761/665298 [04:09<01:35, 2595.38 examples/s]Map (num_proc=32):  63%|██████▎   | 417040/665298 [04:09<01:34, 2618.59 examples/s]Map (num_proc=32):  63%|██████▎   | 417314/665298 [04:09<01:54, 2174.20 examples/s]Map (num_proc=32):  63%|██████▎   | 417629/665298 [04:09<01:45, 2345.91 examples/s]Map (num_proc=32):  63%|██████▎   | 417879/665298 [04:09<01:45, 2350.73 examples/s]Map (num_proc=32):  63%|██████▎   | 418129/665298 [04:09<01:44, 2372.08 examples/s]Map (num_proc=32):  63%|██████▎   | 418374/665298 [04:09<01:52, 2201.93 examples/s]Map (num_proc=32):  63%|██████▎   | 418601/665298 [04:10<01:51, 2214.70 examples/s]Map (num_proc=32):  63%|██████▎   | 418827/665298 [04:10<01:51, 2207.02 examples/s]Map (num_proc=32):  63%|██████▎   | 419051/665298 [04:10<01:51, 2202.34 examples/s]Map (num_proc=32):  63%|██████▎   | 419275/665298 [04:10<01:57, 2085.49 examples/s]Map (num_proc=32):  63%|██████▎   | 419493/665298 [04:10<02:11, 1871.40 examples/s]Map (num_proc=32):  63%|██████▎   | 419686/665298 [04:10<02:10, 1879.96 examples/s]Map (num_proc=32):  63%|██████▎   | 419904/665298 [04:10<02:05, 1951.54 examples/s]Map (num_proc=32):  63%|██████▎   | 420134/665298 [04:10<02:00, 2036.59 examples/s]Map (num_proc=32):  63%|██████▎   | 420360/665298 [04:10<01:57, 2085.06 examples/s]Map (num_proc=32):  63%|██████▎   | 420587/665298 [04:11<01:58, 2072.67 examples/s]Map (num_proc=32):  63%|██████▎   | 420800/665298 [04:11<01:58, 2056.37 examples/s]Map (num_proc=32):  63%|██████▎   | 421029/665298 [04:11<01:55, 2118.69 examples/s]Map (num_proc=32):  63%|██████▎   | 421255/665298 [04:11<02:08, 1898.16 examples/s]Map (num_proc=32):  63%|██████▎   | 421473/665298 [04:11<02:04, 1959.39 examples/s]Map (num_proc=32):  63%|██████▎   | 421694/665298 [04:11<02:00, 2016.84 examples/s]Map (num_proc=32):  63%|██████▎   | 421921/665298 [04:11<01:56, 2081.46 examples/s]Map (num_proc=32):  63%|██████▎   | 422147/665298 [04:11<01:54, 2130.05 examples/s]Map (num_proc=32):  63%|██████▎   | 422373/665298 [04:11<01:52, 2158.10 examples/s]Map (num_proc=32):  64%|██████▎   | 422598/665298 [04:11<01:51, 2168.83 examples/s]Map (num_proc=32):  64%|██████▎   | 422819/665298 [04:12<01:52, 2159.50 examples/s]Map (num_proc=32):  64%|██████▎   | 423049/665298 [04:12<01:50, 2191.72 examples/s]Map (num_proc=32):  64%|██████▎   | 423272/665298 [04:12<01:50, 2191.18 examples/s]Map (num_proc=32):  64%|██████▎   | 423496/665298 [04:12<01:58, 2041.47 examples/s]Map (num_proc=32):  64%|██████▎   | 423717/665298 [04:12<02:11, 1834.85 examples/s]Map (num_proc=32):  64%|██████▎   | 423928/665298 [04:12<02:06, 1902.50 examples/s]Map (num_proc=32):  64%|██████▍   | 424144/665298 [04:12<02:36, 1537.87 examples/s]Map (num_proc=32):  64%|██████▍   | 424340/665298 [04:12<02:28, 1628.01 examples/s]Map (num_proc=32):  64%|██████▍   | 424552/665298 [04:13<02:17, 1748.35 examples/s]Map (num_proc=32):  64%|██████▍   | 424789/665298 [04:13<02:06, 1895.42 examples/s]Map (num_proc=32):  64%|██████▍   | 425013/665298 [04:13<02:11, 1831.78 examples/s]Map (num_proc=32):  64%|██████▍   | 425230/665298 [04:13<02:13, 1792.76 examples/s]Map (num_proc=32):  64%|██████▍   | 425424/665298 [04:13<02:18, 1736.77 examples/s]Map (num_proc=32):  64%|██████▍   | 425621/665298 [04:13<02:22, 1682.64 examples/s]Map (num_proc=32):  64%|██████▍   | 425809/665298 [04:14<06:34, 607.32 examples/s] Map (num_proc=32):  64%|██████▍   | 425962/665298 [04:14<05:49, 684.51 examples/s]Map (num_proc=32):  64%|██████▍   | 426093/665298 [04:16<16:31, 241.38 examples/s]Map (num_proc=32):  64%|██████▍   | 426273/665298 [04:16<13:21, 298.30 examples/s]Map (num_proc=32):  64%|██████▍   | 426494/665298 [04:16<09:17, 428.64 examples/s]Map (num_proc=32):  64%|██████▍   | 426697/665298 [04:16<06:58, 569.88 examples/s]Map (num_proc=32):  64%|██████▍   | 426906/665298 [04:16<05:22, 738.44 examples/s]Map (num_proc=32):  64%|██████▍   | 427150/665298 [04:17<04:10, 951.48 examples/s]Map (num_proc=32):  64%|██████▍   | 427350/665298 [04:17<03:34, 1107.36 examples/s]Map (num_proc=32):  64%|██████▍   | 427546/665298 [04:17<03:14, 1223.15 examples/s]Map (num_proc=32):  64%|██████▍   | 427737/665298 [04:17<02:54, 1363.33 examples/s]Map (num_proc=32):  64%|██████▍   | 428002/665298 [04:17<02:24, 1640.67 examples/s]Map (num_proc=32):  64%|██████▍   | 428239/665298 [04:17<02:11, 1798.96 examples/s]Map (num_proc=32):  64%|██████▍   | 428454/665298 [04:17<02:06, 1876.72 examples/s]Map (num_proc=32):  64%|██████▍   | 428687/665298 [04:17<02:32, 1550.14 examples/s]Map (num_proc=32):  64%|██████▍   | 428946/665298 [04:18<02:15, 1749.18 examples/s]Map (num_proc=32):  65%|██████▍   | 429182/665298 [04:18<02:05, 1880.09 examples/s]Map (num_proc=32):  65%|██████▍   | 429438/665298 [04:18<01:57, 2008.65 examples/s]Map (num_proc=32):  65%|██████▍   | 429692/665298 [04:18<01:52, 2101.24 examples/s]Map (num_proc=32):  65%|██████▍   | 429920/665298 [04:18<01:50, 2130.19 examples/s]Map (num_proc=32):  65%|██████▍   | 430151/665298 [04:18<01:53, 2065.84 examples/s]Map (num_proc=32):  65%|██████▍   | 430375/665298 [04:18<01:52, 2084.91 examples/s]Map (num_proc=32):  65%|██████▍   | 430643/665298 [04:18<01:45, 2217.24 examples/s]Map (num_proc=32):  65%|██████▍   | 430930/665298 [04:18<01:40, 2333.59 examples/s]Map (num_proc=32):  65%|██████▍   | 431181/665298 [04:18<01:39, 2356.28 examples/s]Map (num_proc=32):  65%|██████▍   | 431443/665298 [04:19<01:37, 2405.45 examples/s]Map (num_proc=32):  65%|██████▍   | 431692/665298 [04:19<01:38, 2370.31 examples/s]Map (num_proc=32):  65%|██████▍   | 432020/665298 [04:19<01:30, 2565.05 examples/s]Map (num_proc=32):  65%|██████▍   | 432297/665298 [04:19<01:32, 2527.75 examples/s]Map (num_proc=32):  65%|██████▌   | 432573/665298 [04:19<01:30, 2582.17 examples/s]Map (num_proc=32):  65%|██████▌   | 432853/665298 [04:19<01:28, 2632.12 examples/s]Map (num_proc=32):  65%|██████▌   | 433137/665298 [04:19<01:29, 2605.29 examples/s]Map (num_proc=32):  65%|██████▌   | 433421/665298 [04:19<01:30, 2575.75 examples/s]Map (num_proc=32):  65%|██████▌   | 433698/665298 [04:19<01:29, 2601.54 examples/s]Map (num_proc=32):  65%|██████▌   | 433962/665298 [04:20<01:31, 2518.81 examples/s]Map (num_proc=32):  65%|██████▌   | 434221/665298 [04:20<01:32, 2506.08 examples/s]Map (num_proc=32):  65%|██████▌   | 434485/665298 [04:20<01:32, 2505.73 examples/s]Map (num_proc=32):  65%|██████▌   | 434762/665298 [04:20<01:31, 2515.51 examples/s]Map (num_proc=32):  65%|██████▌   | 435016/665298 [04:20<01:34, 2428.07 examples/s]Map (num_proc=32):  65%|██████▌   | 435263/665298 [04:20<01:37, 2356.19 examples/s]Map (num_proc=32):  65%|██████▌   | 435526/665298 [04:20<01:43, 2230.14 examples/s]Map (num_proc=32):  65%|██████▌   | 435756/665298 [04:20<01:55, 1980.21 examples/s]Map (num_proc=32):  66%|██████▌   | 437297/665298 [04:21<00:45, 5034.87 examples/s]Map (num_proc=32):  66%|██████▌   | 437820/665298 [04:21<01:07, 3381.53 examples/s]Map (num_proc=32):  66%|██████▌   | 438239/665298 [04:21<01:16, 2949.37 examples/s]Map (num_proc=32):  66%|██████▌   | 438608/665298 [04:21<01:23, 2704.19 examples/s]Map (num_proc=32):  66%|██████▌   | 438943/665298 [04:21<01:41, 2231.83 examples/s]Map (num_proc=32):  66%|██████▌   | 439211/665298 [04:22<01:45, 2134.17 examples/s]Map (num_proc=32):  66%|██████▌   | 439472/665298 [04:22<01:44, 2167.55 examples/s]Map (num_proc=32):  66%|██████▌   | 439707/665298 [04:22<01:44, 2152.03 examples/s]Map (num_proc=32):  66%|██████▌   | 439945/665298 [04:22<01:49, 2066.06 examples/s]Map (num_proc=32):  66%|██████▌   | 440166/665298 [04:22<01:50, 2041.81 examples/s]Map (num_proc=32):  66%|██████▌   | 440398/665298 [04:22<01:48, 2078.78 examples/s]Map (num_proc=32):  66%|██████▌   | 440632/665298 [04:22<01:47, 2093.76 examples/s]Map (num_proc=32):  66%|██████▋   | 440847/665298 [04:22<01:47, 2092.98 examples/s]Map (num_proc=32):  66%|██████▋   | 441065/665298 [04:23<02:09, 1737.71 examples/s]Map (num_proc=32):  66%|██████▋   | 441266/665298 [04:23<02:04, 1792.89 examples/s]Map (num_proc=32):  66%|██████▋   | 441466/665298 [04:23<02:01, 1838.18 examples/s]Map (num_proc=32):  66%|██████▋   | 441672/665298 [04:23<01:58, 1888.61 examples/s]Map (num_proc=32):  66%|██████▋   | 441878/665298 [04:23<01:56, 1925.25 examples/s]Map (num_proc=32):  66%|██████▋   | 442084/665298 [04:23<01:54, 1948.97 examples/s]Map (num_proc=32):  66%|██████▋   | 442287/665298 [04:23<01:53, 1964.20 examples/s]Map (num_proc=32):  67%|██████▋   | 442505/665298 [04:23<02:01, 1837.57 examples/s]Map (num_proc=32):  67%|██████▋   | 442715/665298 [04:23<02:00, 1854.66 examples/s]Map (num_proc=32):  67%|██████▋   | 442924/665298 [04:24<02:34, 1441.75 examples/s]Map (num_proc=32):  67%|██████▋   | 443208/665298 [04:24<02:06, 1751.61 examples/s]Map (num_proc=32):  67%|██████▋   | 443516/665298 [04:24<01:47, 2070.44 examples/s]Map (num_proc=32):  67%|██████▋   | 443784/665298 [04:24<01:40, 2199.73 examples/s]Map (num_proc=32):  67%|██████▋   | 444040/665298 [04:24<02:42, 1361.43 examples/s]Map (num_proc=32):  67%|██████▋   | 444317/665298 [04:24<02:18, 1591.13 examples/s]Map (num_proc=32):  67%|██████▋   | 444541/665298 [04:25<02:15, 1633.87 examples/s]Map (num_proc=32):  67%|██████▋   | 444765/665298 [04:25<02:19, 1581.47 examples/s]Map (num_proc=32):  67%|██████▋   | 444958/665298 [04:25<03:00, 1218.84 examples/s]Map (num_proc=32):  67%|██████▋   | 445112/665298 [04:25<04:44, 774.25 examples/s] Map (num_proc=32):  67%|██████▋   | 445242/665298 [04:27<14:56, 245.51 examples/s]Map (num_proc=32):  67%|██████▋   | 445439/665298 [04:27<10:50, 337.86 examples/s]Map (num_proc=32):  67%|██████▋   | 445617/665298 [04:27<08:20, 438.49 examples/s]Map (num_proc=32):  67%|██████▋   | 445818/665298 [04:28<06:18, 579.12 examples/s]Map (num_proc=32):  67%|██████▋   | 446081/665298 [04:28<04:31, 808.41 examples/s]Map (num_proc=32):  67%|██████▋   | 446335/665298 [04:28<03:29, 1045.42 examples/s]Map (num_proc=32):  67%|██████▋   | 446567/665298 [04:28<02:55, 1249.60 examples/s]Map (num_proc=32):  67%|██████▋   | 446789/665298 [04:28<03:35, 1014.36 examples/s]Map (num_proc=32):  67%|██████▋   | 447004/665298 [04:28<03:02, 1196.81 examples/s]Map (num_proc=32):  67%|██████▋   | 447238/665298 [04:28<02:37, 1384.89 examples/s]Map (num_proc=32):  67%|██████▋   | 447439/665298 [04:29<02:24, 1510.35 examples/s]Map (num_proc=32):  67%|██████▋   | 447640/665298 [04:29<02:21, 1533.08 examples/s]Map (num_proc=32):  67%|██████▋   | 447884/665298 [04:29<02:07, 1710.41 examples/s]Map (num_proc=32):  67%|██████▋   | 448138/665298 [04:29<01:54, 1894.26 examples/s]Map (num_proc=32):  67%|██████▋   | 448405/665298 [04:29<01:44, 2078.11 examples/s]Map (num_proc=32):  67%|██████▋   | 448648/665298 [04:29<01:48, 1995.09 examples/s]Map (num_proc=32):  67%|██████▋   | 448887/665298 [04:29<01:44, 2069.85 examples/s]Map (num_proc=32):  68%|██████▊   | 449137/665298 [04:29<01:40, 2158.76 examples/s]Map (num_proc=32):  68%|██████▊   | 449362/665298 [04:29<01:39, 2163.18 examples/s]Map (num_proc=32):  68%|██████▊   | 449597/665298 [04:30<01:38, 2194.93 examples/s]Map (num_proc=32):  68%|██████▊   | 449838/665298 [04:30<02:41, 1333.20 examples/s]Map (num_proc=32):  68%|██████▊   | 450079/665298 [04:30<02:20, 1527.73 examples/s]Map (num_proc=32):  68%|██████▊   | 450281/665298 [04:30<02:13, 1616.43 examples/s]Map (num_proc=32):  68%|██████▊   | 450519/665298 [04:30<02:02, 1753.29 examples/s]Map (num_proc=32):  68%|██████▊   | 450746/665298 [04:30<01:55, 1854.70 examples/s]Map (num_proc=32):  68%|██████▊   | 450951/665298 [04:30<01:53, 1883.58 examples/s]Map (num_proc=32):  68%|██████▊   | 451171/665298 [04:31<01:50, 1944.70 examples/s]Map (num_proc=32):  68%|██████▊   | 451392/665298 [04:31<01:54, 1867.74 examples/s]Map (num_proc=32):  68%|██████▊   | 451636/665298 [04:31<01:48, 1974.72 examples/s]Map (num_proc=32):  68%|██████▊   | 451888/665298 [04:31<01:41, 2094.64 examples/s]Map (num_proc=32):  68%|██████▊   | 452120/665298 [04:31<01:39, 2151.84 examples/s]Map (num_proc=32):  68%|██████▊   | 452364/665298 [04:31<01:39, 2134.64 examples/s]Map (num_proc=32):  68%|██████▊   | 452628/665298 [04:31<01:34, 2248.78 examples/s]Map (num_proc=32):  68%|██████▊   | 452879/665298 [04:31<01:32, 2294.52 examples/s]Map (num_proc=32):  68%|██████▊   | 453138/665298 [04:31<01:30, 2352.49 examples/s]Map (num_proc=32):  68%|██████▊   | 453425/665298 [04:31<01:27, 2433.68 examples/s]Map (num_proc=32):  68%|██████▊   | 453671/665298 [04:32<01:28, 2402.26 examples/s]Map (num_proc=32):  68%|██████▊   | 453972/665298 [04:32<01:23, 2544.49 examples/s]Map (num_proc=32):  68%|██████▊   | 454297/665298 [04:32<01:19, 2665.51 examples/s]Map (num_proc=32):  68%|██████▊   | 454597/665298 [04:32<01:18, 2693.02 examples/s]Map (num_proc=32):  68%|██████▊   | 454878/665298 [04:32<01:19, 2642.98 examples/s]Map (num_proc=32):  68%|██████▊   | 455151/665298 [04:32<01:38, 2132.92 examples/s]Map (num_proc=32):  69%|██████▊   | 457051/665298 [04:32<00:33, 6146.15 examples/s]Map (num_proc=32):  69%|██████▉   | 457772/665298 [04:33<00:54, 3778.04 examples/s]Map (num_proc=32):  69%|██████▉   | 458338/665298 [04:33<01:10, 2955.17 examples/s]Map (num_proc=32):  69%|██████▉   | 458792/665298 [04:33<01:21, 2545.64 examples/s]Map (num_proc=32):  69%|██████▉   | 459170/665298 [04:34<01:40, 2047.08 examples/s]Map (num_proc=32):  69%|██████▉   | 459457/665298 [04:34<01:45, 1942.52 examples/s]Map (num_proc=32):  69%|██████▉   | 459708/665298 [04:34<01:45, 1957.05 examples/s]Map (num_proc=32):  69%|██████▉   | 459952/665298 [04:34<01:47, 1917.36 examples/s]Map (num_proc=32):  69%|██████▉   | 460174/665298 [04:34<01:53, 1805.80 examples/s]Map (num_proc=32):  69%|██████▉   | 460375/665298 [04:34<01:52, 1829.53 examples/s]Map (num_proc=32):  69%|██████▉   | 460588/665298 [04:34<01:49, 1865.84 examples/s]Map (num_proc=32):  69%|██████▉   | 460806/665298 [04:35<01:48, 1885.62 examples/s]Map (num_proc=32):  69%|██████▉   | 461011/665298 [04:35<01:56, 1749.52 examples/s]Map (num_proc=32):  69%|██████▉   | 461193/665298 [04:35<01:55, 1762.84 examples/s]Map (num_proc=32):  69%|██████▉   | 461398/665298 [04:35<01:54, 1784.03 examples/s]Map (num_proc=32):  69%|██████▉   | 461598/665298 [04:35<02:06, 1609.95 examples/s]Map (num_proc=32):  69%|██████▉   | 461771/665298 [04:35<02:20, 1445.62 examples/s]Map (num_proc=32):  69%|██████▉   | 461936/665298 [04:35<03:15, 1042.19 examples/s]Map (num_proc=32):  69%|██████▉   | 462080/665298 [04:36<03:06, 1088.92 examples/s]Map (num_proc=32):  69%|██████▉   | 462283/665298 [04:36<02:39, 1271.98 examples/s]Map (num_proc=32):  70%|██████▉   | 462429/665298 [04:36<04:42, 719.09 examples/s] Map (num_proc=32):  70%|██████▉   | 462546/665298 [04:36<04:55, 685.18 examples/s]Map (num_proc=32):  70%|██████▉   | 462648/665298 [04:39<24:37, 137.16 examples/s]Map (num_proc=32):  70%|██████▉   | 462846/665298 [04:39<15:59, 210.89 examples/s]Map (num_proc=32):  70%|██████▉   | 463020/665298 [04:39<11:31, 292.69 examples/s]Map (num_proc=32):  70%|██████▉   | 463154/665298 [04:40<09:10, 367.13 examples/s]Map (num_proc=32):  70%|██████▉   | 463304/665298 [04:40<07:11, 468.63 examples/s]Map (num_proc=32):  70%|██████▉   | 463432/665298 [04:40<05:58, 562.92 examples/s]Map (num_proc=32):  70%|██████▉   | 463581/665298 [04:40<04:51, 691.32 examples/s]Map (num_proc=32):  70%|██████▉   | 463788/665298 [04:40<03:38, 922.96 examples/s]Map (num_proc=32):  70%|██████▉   | 463968/665298 [04:40<03:07, 1072.32 examples/s]Map (num_proc=32):  70%|██████▉   | 464157/665298 [04:40<02:43, 1233.58 examples/s]Map (num_proc=32):  70%|██████▉   | 464326/665298 [04:40<03:49, 875.22 examples/s] Map (num_proc=32):  70%|██████▉   | 464471/665298 [04:41<03:30, 955.71 examples/s]Map (num_proc=32):  70%|██████▉   | 464624/665298 [04:41<03:07, 1069.70 examples/s]Map (num_proc=32):  70%|██████▉   | 464840/665298 [04:41<02:35, 1286.35 examples/s]Map (num_proc=32):  70%|██████▉   | 465017/665298 [04:41<02:23, 1391.13 examples/s]Map (num_proc=32):  70%|██████▉   | 465222/665298 [04:41<02:11, 1524.62 examples/s]Map (num_proc=32):  70%|██████▉   | 465411/665298 [04:41<02:07, 1566.76 examples/s]Map (num_proc=32):  70%|██████▉   | 465640/665298 [04:41<01:56, 1714.96 examples/s]Map (num_proc=32):  70%|███████   | 465867/665298 [04:41<01:48, 1845.30 examples/s]Map (num_proc=32):  70%|███████   | 466090/665298 [04:41<01:42, 1943.40 examples/s]Map (num_proc=32):  70%|███████   | 466352/665298 [04:42<01:35, 2076.56 examples/s]Map (num_proc=32):  70%|███████   | 466583/665298 [04:42<01:36, 2059.51 examples/s]Map (num_proc=32):  70%|███████   | 466825/665298 [04:42<01:35, 2075.63 examples/s]Map (num_proc=32):  70%|███████   | 467060/665298 [04:42<01:32, 2132.35 examples/s]Map (num_proc=32):  70%|███████   | 467305/665298 [04:42<01:31, 2153.12 examples/s]Map (num_proc=32):  70%|███████   | 467550/665298 [04:42<01:30, 2176.53 examples/s]Map (num_proc=32):  70%|███████   | 467794/665298 [04:42<01:33, 2116.27 examples/s]Map (num_proc=32):  70%|███████   | 468027/665298 [04:42<02:03, 1603.55 examples/s]Map (num_proc=32):  70%|███████   | 468235/665298 [04:43<01:57, 1675.63 examples/s]Map (num_proc=32):  70%|███████   | 468439/665298 [04:43<01:53, 1727.99 examples/s]Map (num_proc=32):  70%|███████   | 468651/665298 [04:43<01:48, 1812.07 examples/s]Map (num_proc=32):  70%|███████   | 468853/665298 [04:43<01:46, 1848.11 examples/s]Map (num_proc=32):  71%|███████   | 469047/665298 [04:43<01:50, 1777.87 examples/s]Map (num_proc=32):  71%|███████   | 469289/665298 [04:43<01:42, 1903.10 examples/s]Map (num_proc=32):  71%|███████   | 469532/665298 [04:43<01:38, 1992.74 examples/s]Map (num_proc=32):  71%|███████   | 469747/665298 [04:43<01:36, 2035.91 examples/s]Map (num_proc=32):  71%|███████   | 469979/665298 [04:43<01:34, 2058.03 examples/s]Map (num_proc=32):  71%|███████   | 470211/665298 [04:44<01:59, 1630.08 examples/s]Map (num_proc=32):  71%|███████▏  | 474206/665298 [04:44<00:17, 10636.84 examples/s]Map (num_proc=32):  71%|███████▏  | 475545/665298 [04:44<00:43, 4340.66 examples/s] Map (num_proc=32):  72%|███████▏  | 476543/665298 [04:45<00:56, 3329.85 examples/s]Map (num_proc=32):  72%|███████▏  | 477290/665298 [04:45<01:06, 2839.48 examples/s]Map (num_proc=32):  72%|███████▏  | 477867/665298 [04:46<01:11, 2631.21 examples/s]Map (num_proc=32):  72%|███████▏  | 478334/665298 [04:46<01:14, 2517.07 examples/s]Map (num_proc=32):  72%|███████▏  | 478740/665298 [04:46<01:20, 2329.11 examples/s]Map (num_proc=32):  72%|███████▏  | 479071/665298 [04:46<01:21, 2287.87 examples/s]Map (num_proc=32):  72%|███████▏  | 479368/665298 [04:46<01:25, 2163.96 examples/s]Map (num_proc=32):  72%|███████▏  | 479625/665298 [04:47<01:28, 2108.01 examples/s]Map (num_proc=32):  72%|███████▏  | 479890/665298 [04:47<01:33, 1984.59 examples/s]Map (num_proc=32):  72%|███████▏  | 480116/665298 [04:47<01:49, 1693.71 examples/s]Map (num_proc=32):  72%|███████▏  | 480350/665298 [04:47<01:42, 1806.89 examples/s]Map (num_proc=32):  72%|███████▏  | 480584/665298 [04:47<01:37, 1894.18 examples/s]Map (num_proc=32):  72%|███████▏  | 480800/665298 [04:47<01:34, 1944.02 examples/s]Map (num_proc=32):  72%|███████▏  | 481044/665298 [04:47<01:29, 2052.77 examples/s]Map (num_proc=32):  72%|███████▏  | 481276/665298 [04:48<01:27, 2110.21 examples/s]Map (num_proc=32):  72%|███████▏  | 481508/665298 [04:48<01:26, 2114.05 examples/s]Map (num_proc=32):  72%|███████▏  | 481745/665298 [04:48<01:31, 2005.60 examples/s]Map (num_proc=32):  72%|███████▏  | 481991/665298 [04:48<01:29, 2045.56 examples/s]Map (num_proc=32):  72%|███████▏  | 482221/665298 [04:48<01:29, 2039.23 examples/s]Map (num_proc=32):  73%|███████▎  | 482446/665298 [04:48<01:30, 2009.97 examples/s]Map (num_proc=32):  73%|███████▎  | 482656/665298 [04:48<01:39, 1844.40 examples/s]Map (num_proc=32):  73%|███████▎  | 482860/665298 [04:48<01:36, 1881.38 examples/s]Map (num_proc=32):  73%|███████▎  | 483058/665298 [04:48<01:41, 1803.34 examples/s]Map (num_proc=32):  73%|███████▎  | 483273/665298 [04:49<01:43, 1754.29 examples/s]Map (num_proc=32):  73%|███████▎  | 483465/665298 [04:49<02:40, 1131.35 examples/s]Map (num_proc=32):  73%|███████▎  | 483677/665298 [04:49<02:18, 1310.86 examples/s]Map (num_proc=32):  73%|███████▎  | 483876/665298 [04:49<02:07, 1426.32 examples/s]Map (num_proc=32):  73%|███████▎  | 484075/665298 [04:49<01:58, 1527.21 examples/s]Map (num_proc=32):  73%|███████▎  | 484349/665298 [04:49<01:41, 1781.28 examples/s]Map (num_proc=32):  73%|███████▎  | 484586/665298 [04:49<01:34, 1908.44 examples/s]Map (num_proc=32):  73%|███████▎  | 484809/665298 [04:50<01:32, 1945.44 examples/s]Map (num_proc=32):  73%|███████▎  | 485055/665298 [04:50<01:28, 2030.24 examples/s]Map (num_proc=32):  73%|███████▎  | 485329/665298 [04:50<01:21, 2198.71 examples/s]Map (num_proc=32):  73%|███████▎  | 485591/665298 [04:50<01:19, 2260.89 examples/s]Map (num_proc=32):  73%|███████▎  | 485840/665298 [04:50<01:19, 2266.88 examples/s]Map (num_proc=32):  73%|███████▎  | 486074/665298 [04:50<01:21, 2203.45 examples/s]Map (num_proc=32):  73%|███████▎  | 486419/665298 [04:50<01:11, 2516.18 examples/s]Map (num_proc=32):  73%|███████▎  | 486816/665298 [04:50<01:02, 2859.27 examples/s]Map (num_proc=32):  73%|███████▎  | 487180/665298 [04:50<00:58, 3052.65 examples/s]Map (num_proc=32):  73%|███████▎  | 487553/665298 [04:51<00:56, 3170.17 examples/s]Map (num_proc=32):  73%|███████▎  | 487973/665298 [04:51<00:52, 3385.15 examples/s]Map (num_proc=32):  73%|███████▎  | 488336/665298 [04:51<00:54, 3272.21 examples/s]Map (num_proc=32):  73%|███████▎  | 488805/665298 [04:51<00:49, 3586.31 examples/s]Map (num_proc=32):  74%|███████▎  | 489209/665298 [04:51<00:53, 3270.47 examples/s]Map (num_proc=32):  74%|███████▎  | 489666/665298 [04:51<00:49, 3534.22 examples/s]Map (num_proc=32):  74%|███████▎  | 490140/665298 [04:51<00:47, 3692.11 examples/s]Map (num_proc=32):  74%|███████▎  | 490617/665298 [04:51<00:45, 3873.15 examples/s]Map (num_proc=32):  74%|███████▍  | 491097/665298 [04:51<00:43, 3974.83 examples/s]Map (num_proc=32):  74%|███████▍  | 491566/665298 [04:52<00:43, 3957.93 examples/s]Map (num_proc=32):  74%|███████▍  | 492012/665298 [04:52<00:47, 3657.37 examples/s]Map (num_proc=32):  74%|███████▍  | 492417/665298 [04:52<00:50, 3419.84 examples/s]Map (num_proc=32):  74%|███████▍  | 492800/665298 [04:52<00:50, 3446.77 examples/s]Map (num_proc=32):  74%|███████▍  | 493212/665298 [04:52<00:52, 3267.74 examples/s]Map (num_proc=32):  74%|███████▍  | 493569/665298 [04:52<00:57, 3005.60 examples/s]Map (num_proc=32):  74%|███████▍  | 493898/665298 [04:53<01:35, 1799.91 examples/s]Map (num_proc=32):  74%|███████▍  | 494138/665298 [04:53<01:30, 1888.78 examples/s]Map (num_proc=32):  74%|███████▍  | 494440/665298 [04:53<01:21, 2095.76 examples/s]Map (num_proc=32):  74%|███████▍  | 494703/665298 [04:53<01:18, 2171.87 examples/s]Map (num_proc=32):  74%|███████▍  | 494955/665298 [04:53<01:16, 2235.62 examples/s]Map (num_proc=32):  74%|███████▍  | 495253/665298 [04:53<01:10, 2405.00 examples/s]Map (num_proc=32):  74%|███████▍  | 495603/665298 [04:53<01:04, 2646.47 examples/s]Map (num_proc=32):  75%|███████▍  | 495948/665298 [04:53<00:59, 2835.79 examples/s]Map (num_proc=32):  75%|███████▍  | 496286/665298 [04:54<00:58, 2880.92 examples/s]Map (num_proc=32):  75%|███████▍  | 496599/665298 [04:54<00:58, 2881.31 examples/s]Map (num_proc=32):  75%|███████▍  | 496922/665298 [04:54<00:58, 2898.49 examples/s]Map (num_proc=32):  75%|███████▍  | 497238/665298 [04:54<01:02, 2705.43 examples/s]Map (num_proc=32):  75%|███████▍  | 497570/665298 [04:54<00:59, 2804.83 examples/s]Map (num_proc=32):  75%|███████▍  | 497904/665298 [04:54<00:57, 2921.15 examples/s]Map (num_proc=32):  75%|███████▍  | 498260/665298 [04:54<01:07, 2457.02 examples/s]Map (num_proc=32):  75%|███████▌  | 501129/665298 [04:54<00:18, 8645.55 examples/s]Map (num_proc=32):  75%|███████▌  | 502150/665298 [04:55<00:28, 5788.53 examples/s]Map (num_proc=32):  76%|███████▌  | 503000/665298 [04:55<00:36, 4476.40 examples/s]Map (num_proc=32):  76%|███████▌  | 503652/665298 [04:55<00:39, 4124.68 examples/s]Map (num_proc=32):  76%|███████▌  | 504221/665298 [04:56<01:03, 2524.08 examples/s]Map (num_proc=32):  76%|███████▌  | 504654/665298 [04:56<01:03, 2513.89 examples/s]Map (num_proc=32):  76%|███████▌  | 505031/665298 [04:58<03:10, 840.72 examples/s] Map (num_proc=32):  76%|███████▌  | 505362/665298 [04:58<02:43, 980.41 examples/s]Map (num_proc=32):  76%|███████▌  | 505655/665298 [04:58<02:22, 1120.63 examples/s]Map (num_proc=32):  76%|███████▌  | 506023/665298 [04:58<01:56, 1365.55 examples/s]Map (num_proc=32):  76%|███████▌  | 506375/665298 [04:58<01:40, 1577.46 examples/s]Map (num_proc=32):  76%|███████▌  | 506718/665298 [04:58<01:26, 1834.60 examples/s]Map (num_proc=32):  76%|███████▌  | 507044/665298 [04:58<01:17, 2054.17 examples/s]Map (num_proc=32):  76%|███████▋  | 507365/665298 [04:58<01:11, 2219.85 examples/s]Map (num_proc=32):  76%|███████▋  | 507668/665298 [04:59<01:06, 2353.67 examples/s]Map (num_proc=32):  76%|███████▋  | 507984/665298 [04:59<01:05, 2402.71 examples/s]Map (num_proc=32):  76%|███████▋  | 508278/665298 [04:59<01:03, 2483.27 examples/s]Map (num_proc=32):  76%|███████▋  | 508581/665298 [04:59<01:01, 2539.63 examples/s]Map (num_proc=32):  76%|███████▋  | 508904/665298 [04:59<00:59, 2628.85 examples/s]Map (num_proc=32):  77%|███████▋  | 509436/665298 [04:59<00:47, 3302.51 examples/s]Map (num_proc=32):  77%|███████▋  | 509786/665298 [04:59<00:50, 3078.48 examples/s]Map (num_proc=32):  77%|███████▋  | 510123/665298 [04:59<00:50, 3046.07 examples/s]Map (num_proc=32):  77%|███████▋  | 510521/665298 [04:59<00:47, 3264.03 examples/s]Map (num_proc=32):  77%|███████▋  | 510980/665298 [05:00<00:43, 3578.57 examples/s]Map (num_proc=32):  77%|███████▋  | 511390/665298 [05:00<00:41, 3720.79 examples/s]Map (num_proc=32):  77%|███████▋  | 511819/665298 [05:00<00:39, 3837.17 examples/s]Map (num_proc=32):  77%|███████▋  | 512240/665298 [05:00<00:39, 3835.06 examples/s]Map (num_proc=32):  77%|███████▋  | 512659/665298 [05:00<00:39, 3830.22 examples/s]Map (num_proc=32):  77%|███████▋  | 513094/665298 [05:00<00:38, 3949.73 examples/s]Map (num_proc=32):  77%|███████▋  | 513507/665298 [05:00<00:39, 3847.29 examples/s]Map (num_proc=32):  77%|███████▋  | 513979/665298 [05:00<00:38, 3979.99 examples/s]Map (num_proc=32):  77%|███████▋  | 514438/665298 [05:00<00:36, 4100.28 examples/s]Map (num_proc=32):  77%|███████▋  | 514863/665298 [05:01<01:01, 2430.25 examples/s]Map (num_proc=32):  77%|███████▋  | 515254/665298 [05:01<00:56, 2673.29 examples/s]Map (num_proc=32):  78%|███████▊  | 515645/665298 [05:01<00:51, 2887.11 examples/s]Map (num_proc=32):  78%|███████▊  | 516042/665298 [05:01<00:50, 2975.23 examples/s]Map (num_proc=32):  78%|███████▊  | 516437/665298 [05:01<00:48, 3087.77 examples/s]Map (num_proc=32):  78%|███████▊  | 516796/665298 [05:01<00:48, 3089.69 examples/s]Map (num_proc=32):  78%|███████▊  | 517173/665298 [05:01<00:47, 3124.52 examples/s]Map (num_proc=32):  78%|███████▊  | 517529/665298 [05:02<00:46, 3183.76 examples/s]Map (num_proc=32):  78%|███████▊  | 517993/665298 [05:02<00:42, 3492.00 examples/s]Map (num_proc=32):  78%|███████▊  | 518465/665298 [05:02<00:39, 3725.31 examples/s]Map (num_proc=32):  78%|███████▊  | 518853/665298 [05:02<00:39, 3748.14 examples/s]Map (num_proc=32):  78%|███████▊  | 519366/665298 [05:02<00:36, 3998.04 examples/s]Map (num_proc=32):  78%|███████▊  | 519782/665298 [05:02<00:38, 3783.40 examples/s]Map (num_proc=32):  78%|███████▊  | 520180/665298 [05:02<00:38, 3791.64 examples/s]Map (num_proc=32):  78%|███████▊  | 520654/665298 [05:02<00:36, 3963.76 examples/s]Map (num_proc=32):  78%|███████▊  | 521067/665298 [05:02<00:38, 3764.76 examples/s]Map (num_proc=32):  78%|███████▊  | 521493/665298 [05:03<00:37, 3807.39 examples/s]Map (num_proc=32):  78%|███████▊  | 521936/665298 [05:03<00:37, 3833.74 examples/s]Map (num_proc=32):  79%|███████▊  | 522359/665298 [05:03<00:38, 3718.58 examples/s]Map (num_proc=32):  79%|███████▊  | 522773/665298 [05:03<00:37, 3813.26 examples/s]Map (num_proc=32):  79%|███████▊  | 523161/665298 [05:03<00:37, 3785.84 examples/s]Map (num_proc=32):  79%|███████▊  | 523545/665298 [05:03<00:38, 3663.58 examples/s]Map (num_proc=32):  79%|███████▉  | 524045/665298 [05:03<00:35, 3943.38 examples/s]Map (num_proc=32):  79%|███████▉  | 524510/665298 [05:03<00:34, 4104.69 examples/s]Map (num_proc=32):  79%|███████▉  | 524945/665298 [05:03<00:36, 3827.18 examples/s]Map (num_proc=32):  79%|███████▉  | 525339/665298 [05:04<00:38, 3609.53 examples/s]Map (num_proc=32):  79%|███████▉  | 525796/665298 [05:04<00:36, 3807.15 examples/s]Map (num_proc=32):  79%|███████▉  | 526220/665298 [05:04<00:36, 3766.12 examples/s]Map (num_proc=32):  79%|███████▉  | 526653/665298 [05:04<00:36, 3833.80 examples/s]Map (num_proc=32):  79%|███████▉  | 527155/665298 [05:04<00:33, 4118.48 examples/s]Map (num_proc=32):  79%|███████▉  | 527849/665298 [05:04<00:28, 4772.04 examples/s]Map (num_proc=32):  79%|███████▉  | 528335/665298 [05:04<00:31, 4317.89 examples/s]Map (num_proc=32):  79%|███████▉  | 528814/665298 [05:04<00:33, 4040.77 examples/s]Map (num_proc=32):  80%|███████▉  | 529318/665298 [05:04<00:32, 4202.85 examples/s]Map (num_proc=32):  80%|███████▉  | 529769/665298 [05:05<00:32, 4158.94 examples/s]Map (num_proc=32):  80%|███████▉  | 530206/665298 [05:05<00:36, 3751.87 examples/s]Map (num_proc=32):  81%|████████  | 537558/665298 [05:05<00:06, 20789.66 examples/s]Map (num_proc=32):  81%|████████  | 539793/665298 [05:06<00:23, 5240.19 examples/s] Map (num_proc=32):  81%|████████▏ | 541669/665298 [05:06<00:19, 6370.29 examples/s]Map (num_proc=32):  82%|████████▏ | 543367/665298 [05:07<00:23, 5225.96 examples/s]Map (num_proc=32):  82%|████████▏ | 544659/665298 [05:07<00:25, 4770.38 examples/s]Map (num_proc=32):  82%|████████▏ | 545670/665298 [05:07<00:27, 4326.84 examples/s]Map (num_proc=32):  82%|████████▏ | 546477/665298 [05:09<00:56, 2121.12 examples/s]Map (num_proc=32):  82%|████████▏ | 547063/665298 [05:09<00:54, 2175.98 examples/s]Map (num_proc=32):  82%|████████▏ | 547573/665298 [05:09<00:50, 2320.09 examples/s]Map (num_proc=32):  82%|████████▏ | 548034/665298 [05:09<00:49, 2381.24 examples/s]Map (num_proc=32):  82%|████████▏ | 548437/665298 [05:09<00:46, 2489.07 examples/s]Map (num_proc=32):  82%|████████▏ | 548808/665298 [05:09<00:44, 2627.04 examples/s]Map (num_proc=32):  83%|████████▎ | 549172/665298 [05:10<00:58, 1969.84 examples/s]Map (num_proc=32):  83%|████████▎ | 549496/665298 [05:10<00:55, 2104.88 examples/s]Map (num_proc=32):  83%|████████▎ | 549795/665298 [05:10<00:56, 2036.38 examples/s]Map (num_proc=32):  83%|████████▎ | 550101/665298 [05:11<02:36, 735.93 examples/s] Map (num_proc=32):  83%|████████▎ | 550479/665298 [05:11<01:58, 965.74 examples/s]Map (num_proc=32):  83%|████████▎ | 550734/665298 [05:12<02:01, 945.92 examples/s]Map (num_proc=32):  83%|████████▎ | 551109/665298 [05:12<01:32, 1239.07 examples/s]Map (num_proc=32):  83%|████████▎ | 551468/665298 [05:12<01:13, 1540.13 examples/s]Map (num_proc=32):  83%|████████▎ | 551918/665298 [05:12<00:58, 1944.04 examples/s]Map (num_proc=32):  83%|████████▎ | 552358/665298 [05:12<00:47, 2353.71 examples/s]Map (num_proc=32):  83%|████████▎ | 552843/665298 [05:12<00:39, 2836.35 examples/s]Map (num_proc=32):  83%|████████▎ | 553261/665298 [05:12<00:35, 3128.78 examples/s]Map (num_proc=32):  83%|████████▎ | 553713/665298 [05:13<00:33, 3377.70 examples/s]Map (num_proc=32):  83%|████████▎ | 554225/665298 [05:13<00:29, 3730.84 examples/s]Map (num_proc=32):  83%|████████▎ | 554684/665298 [05:13<00:29, 3789.27 examples/s]Map (num_proc=32):  83%|████████▎ | 555166/665298 [05:13<00:27, 4022.79 examples/s]Map (num_proc=32):  84%|████████▎ | 555781/665298 [05:13<00:24, 4495.54 examples/s]Map (num_proc=32):  84%|████████▎ | 556259/665298 [05:13<00:24, 4473.79 examples/s]Map (num_proc=32):  84%|████████▎ | 556727/665298 [05:13<00:38, 2856.23 examples/s]Map (num_proc=32):  84%|████████▍ | 557291/665298 [05:13<00:32, 3354.22 examples/s]Map (num_proc=32):  84%|████████▍ | 557875/665298 [05:14<00:27, 3871.92 examples/s]Map (num_proc=32):  84%|████████▍ | 558383/665298 [05:14<00:58, 1814.10 examples/s]Map (num_proc=32):  84%|████████▍ | 558759/665298 [05:16<02:31, 703.48 examples/s] Map (num_proc=32):  84%|████████▍ | 559137/665298 [05:16<02:00, 881.67 examples/s]Map (num_proc=32):  84%|████████▍ | 559456/665298 [05:16<01:42, 1036.65 examples/s]Map (num_proc=32):  85%|████████▍ | 562572/665298 [05:16<00:26, 3859.96 examples/s]Map (num_proc=32):  85%|████████▍ | 563533/665298 [05:17<00:30, 3377.77 examples/s]Map (num_proc=32):  85%|████████▍ | 564301/665298 [05:17<00:32, 3146.84 examples/s]Map (num_proc=32):  85%|████████▍ | 564877/665298 [05:17<00:35, 2845.40 examples/s]Map (num_proc=32):  85%|████████▍ | 565381/665298 [05:17<00:36, 2772.13 examples/s]Map (num_proc=32):  85%|████████▌ | 565838/665298 [05:18<00:35, 2780.70 examples/s]Map (num_proc=32):  85%|████████▌ | 566226/665298 [05:18<00:36, 2708.16 examples/s]Map (num_proc=32):  85%|████████▌ | 566581/665298 [05:18<00:37, 2617.18 examples/s]Map (num_proc=32):  85%|████████▌ | 566941/665298 [05:18<00:39, 2486.70 examples/s]Map (num_proc=32):  85%|████████▌ | 567266/665298 [05:18<00:39, 2474.45 examples/s]Map (num_proc=32):  85%|████████▌ | 567579/665298 [05:18<00:38, 2512.66 examples/s]Map (num_proc=32):  85%|████████▌ | 567904/665298 [05:18<00:39, 2489.25 examples/s]Map (num_proc=32):  85%|████████▌ | 568162/665298 [05:19<00:38, 2508.54 examples/s]Map (num_proc=32):  85%|████████▌ | 568452/665298 [05:19<00:39, 2482.67 examples/s]Map (num_proc=32):  85%|████████▌ | 568769/665298 [05:19<00:40, 2356.84 examples/s]Map (num_proc=32):  86%|████████▌ | 569068/665298 [05:19<00:40, 2374.95 examples/s]Map (num_proc=32):  86%|████████▌ | 569329/665298 [05:19<00:39, 2428.76 examples/s]Map (num_proc=32):  86%|████████▌ | 569589/665298 [05:19<00:38, 2468.76 examples/s]Map (num_proc=32):  86%|████████▌ | 569851/665298 [05:19<00:52, 1805.88 examples/s]Map (num_proc=32):  86%|████████▌ | 570196/665298 [05:19<00:45, 2111.42 examples/s]Map (num_proc=32):  86%|████████▌ | 570456/665298 [05:21<02:16, 692.49 examples/s] Map (num_proc=32):  86%|████████▌ | 570961/665298 [05:21<01:29, 1057.18 examples/s]Map (num_proc=32):  86%|████████▌ | 571215/665298 [05:21<01:25, 1099.50 examples/s]Map (num_proc=32):  86%|████████▌ | 571471/665298 [05:21<01:27, 1069.66 examples/s]Map (num_proc=32):  86%|████████▌ | 571660/665298 [05:21<01:25, 1096.75 examples/s]Map (num_proc=32):  86%|████████▌ | 571834/665298 [05:21<01:24, 1108.39 examples/s]Map (num_proc=32):  86%|████████▌ | 572023/665298 [05:22<01:21, 1141.21 examples/s]Map (num_proc=32):  86%|████████▌ | 572210/665298 [05:22<01:19, 1175.03 examples/s]Map (num_proc=32):  86%|████████▌ | 572348/665298 [05:22<01:25, 1092.62 examples/s]Map (num_proc=32):  86%|████████▌ | 572469/665298 [05:22<01:23, 1114.19 examples/s]Map (num_proc=32):  86%|████████▌ | 572595/665298 [05:22<01:21, 1131.63 examples/s]Map (num_proc=32):  86%|████████▌ | 572769/665298 [05:22<01:21, 1135.50 examples/s]Map (num_proc=32):  86%|████████▌ | 572959/665298 [05:22<01:17, 1185.54 examples/s]Map (num_proc=32):  86%|████████▌ | 573155/665298 [05:23<01:15, 1219.72 examples/s]Map (num_proc=32):  86%|████████▌ | 573335/665298 [05:23<01:08, 1334.16 examples/s]Map (num_proc=32):  86%|████████▌ | 573535/665298 [05:23<01:04, 1429.16 examples/s]Map (num_proc=32):  86%|████████▌ | 573791/665298 [05:23<00:53, 1706.00 examples/s]Map (num_proc=32):  86%|████████▋ | 574042/665298 [05:23<00:48, 1885.65 examples/s]Map (num_proc=32):  86%|████████▋ | 574300/665298 [05:23<00:44, 2064.13 examples/s]Map (num_proc=32):  86%|████████▋ | 574557/665298 [05:23<01:04, 1415.45 examples/s]Map (num_proc=32):  86%|████████▋ | 575003/665298 [05:23<00:45, 1978.61 examples/s]Map (num_proc=32):  87%|████████▋ | 575538/665298 [05:24<00:34, 2590.48 examples/s]Map (num_proc=32):  87%|████████▋ | 575861/665298 [05:24<00:33, 2663.82 examples/s]Map (num_proc=32):  87%|████████▋ | 576183/665298 [05:24<00:34, 2550.18 examples/s]Map (num_proc=32):  87%|████████▋ | 576494/665298 [05:24<00:33, 2615.60 examples/s]Map (num_proc=32):  87%|████████▋ | 576812/665298 [05:24<00:35, 2497.00 examples/s]Map (num_proc=32):  87%|████████▋ | 577121/665298 [05:24<00:37, 2352.80 examples/s]Map (num_proc=32):  87%|████████▋ | 577380/665298 [05:24<00:36, 2408.23 examples/s]Map (num_proc=32):  87%|████████▋ | 577665/665298 [05:25<00:37, 2310.88 examples/s]Map (num_proc=32):  87%|████████▋ | 577991/665298 [05:25<00:35, 2452.80 examples/s]Map (num_proc=32):  87%|████████▋ | 578247/665298 [05:25<00:35, 2462.89 examples/s]Map (num_proc=32):  87%|████████▋ | 578535/665298 [05:25<00:42, 2028.44 examples/s]Map (num_proc=32):  87%|████████▋ | 578789/665298 [05:25<00:40, 2144.44 examples/s]Map (num_proc=32):  87%|████████▋ | 579018/665298 [05:25<00:40, 2116.58 examples/s]Map (num_proc=32):  87%|████████▋ | 579280/665298 [05:25<00:38, 2234.62 examples/s]Map (num_proc=32):  87%|████████▋ | 579518/665298 [05:25<00:46, 1832.99 examples/s]Map (num_proc=32):  87%|████████▋ | 579778/665298 [05:26<00:54, 1567.44 examples/s]Map (num_proc=32):  87%|████████▋ | 579988/665298 [05:26<00:50, 1675.48 examples/s]Map (num_proc=32):  87%|████████▋ | 580181/665298 [05:26<00:54, 1551.93 examples/s]Map (num_proc=32):  87%|████████▋ | 580442/665298 [05:26<00:49, 1724.17 examples/s]Map (num_proc=32):  87%|████████▋ | 580698/665298 [05:26<00:44, 1919.62 examples/s]Map (num_proc=32):  87%|████████▋ | 580958/665298 [05:26<00:40, 2087.03 examples/s]Map (num_proc=32):  87%|████████▋ | 581188/665298 [05:26<00:41, 2049.00 examples/s]Map (num_proc=32):  87%|████████▋ | 581448/665298 [05:27<00:45, 1843.18 examples/s]Map (num_proc=32):  87%|████████▋ | 581853/665298 [05:27<00:36, 2266.69 examples/s]Map (num_proc=32):  87%|████████▋ | 582111/665298 [05:27<00:35, 2338.25 examples/s]Map (num_proc=32):  88%|████████▊ | 582370/665298 [05:27<00:34, 2400.73 examples/s]Map (num_proc=32):  88%|████████▊ | 582625/665298 [05:27<00:33, 2437.77 examples/s]Map (num_proc=32):  88%|████████▊ | 582880/665298 [05:27<00:33, 2464.71 examples/s]Map (num_proc=32):  88%|████████▊ | 583171/665298 [05:27<00:34, 2410.42 examples/s]Map (num_proc=32):  88%|████████▊ | 583426/665298 [05:27<00:33, 2443.71 examples/s]Map (num_proc=32):  88%|████████▊ | 583728/665298 [05:27<00:35, 2276.41 examples/s]Map (num_proc=32):  88%|████████▊ | 584015/665298 [05:28<00:35, 2277.72 examples/s]Map (num_proc=32):  88%|████████▊ | 584272/665298 [05:28<00:34, 2352.02 examples/s]Map (num_proc=32):  88%|████████▊ | 584526/665298 [05:28<00:33, 2395.29 examples/s]Map (num_proc=32):  88%|████████▊ | 584787/665298 [05:28<00:32, 2449.52 examples/s]Map (num_proc=32):  88%|████████▊ | 585092/665298 [05:28<00:35, 2287.98 examples/s]Map (num_proc=32):  88%|████████▊ | 585414/665298 [05:28<00:32, 2469.85 examples/s]Map (num_proc=32):  88%|████████▊ | 585667/665298 [05:28<00:32, 2481.06 examples/s]Map (num_proc=32):  88%|████████▊ | 585981/665298 [05:28<00:34, 2273.91 examples/s]Map (num_proc=32):  88%|████████▊ | 586239/665298 [05:28<00:33, 2348.34 examples/s]Map (num_proc=32):  88%|████████▊ | 586499/665298 [05:29<00:32, 2406.88 examples/s]Map (num_proc=32):  88%|████████▊ | 586761/665298 [05:29<00:31, 2461.35 examples/s]Map (num_proc=32):  88%|████████▊ | 587058/665298 [05:29<00:34, 2275.17 examples/s]Map (num_proc=32):  88%|████████▊ | 587318/665298 [05:29<00:33, 2343.01 examples/s]Map (num_proc=32):  88%|████████▊ | 587568/665298 [05:29<00:32, 2383.50 examples/s]Map (num_proc=32):  88%|████████▊ | 587853/665298 [05:29<00:32, 2397.14 examples/s]Map (num_proc=32):  88%|████████▊ | 588112/665298 [05:29<00:31, 2446.76 examples/s]Map (num_proc=32):  88%|████████▊ | 588375/665298 [05:29<00:30, 2488.54 examples/s]Map (num_proc=32):  88%|████████▊ | 588637/665298 [05:29<00:30, 2518.74 examples/s]Map (num_proc=32):  89%|████████▊ | 588945/665298 [05:30<00:32, 2335.82 examples/s]Map (num_proc=32):  89%|████████▊ | 589236/665298 [05:30<00:40, 1869.05 examples/s]Map (num_proc=32):  89%|████████▊ | 589694/665298 [05:30<00:32, 2342.89 examples/s]Map (num_proc=32):  89%|████████▊ | 589990/665298 [05:30<00:30, 2485.28 examples/s]Map (num_proc=32):  89%|████████▊ | 590309/665298 [05:30<00:30, 2419.66 examples/s]Map (num_proc=32):  89%|████████▉ | 590577/665298 [05:30<00:30, 2451.71 examples/s]Map (num_proc=32):  89%|████████▉ | 590836/665298 [05:30<00:37, 1972.26 examples/s]Map (num_proc=32):  89%|████████▉ | 591086/665298 [05:32<02:16, 543.02 examples/s] Map (num_proc=32):  89%|████████▉ | 591550/665298 [05:32<01:26, 855.96 examples/s]Map (num_proc=32):  89%|████████▉ | 592138/665298 [05:32<00:54, 1339.69 examples/s]Map (num_proc=32):  89%|████████▉ | 592502/665298 [05:32<00:48, 1497.85 examples/s]Map (num_proc=32):  89%|████████▉ | 592800/665298 [05:32<00:51, 1404.98 examples/s]Map (num_proc=32):  89%|████████▉ | 593045/665298 [05:33<00:48, 1485.01 examples/s]Map (num_proc=32):  89%|████████▉ | 593488/665298 [05:33<00:37, 1901.33 examples/s]Map (num_proc=32):  89%|████████▉ | 593829/665298 [05:33<00:33, 2138.98 examples/s]Map (num_proc=32):  89%|████████▉ | 594189/665298 [05:33<00:29, 2409.23 examples/s]Map (num_proc=32):  89%|████████▉ | 594595/665298 [05:33<00:26, 2710.76 examples/s]Map (num_proc=32):  89%|████████▉ | 595037/665298 [05:33<00:24, 2839.20 examples/s]Map (num_proc=32):  89%|████████▉ | 595423/665298 [05:33<00:22, 3074.17 examples/s]Map (num_proc=32):  90%|████████▉ | 595895/665298 [05:33<00:21, 3232.57 examples/s]Map (num_proc=32):  90%|████████▉ | 596261/665298 [05:34<00:20, 3334.49 examples/s]Map (num_proc=32):  90%|████████▉ | 596763/665298 [05:34<00:20, 3336.96 examples/s]Map (num_proc=32):  90%|████████▉ | 597145/665298 [05:34<00:20, 3359.63 examples/s]Map (num_proc=32):  90%|████████▉ | 597507/665298 [05:34<00:20, 3260.03 examples/s]Map (num_proc=32):  90%|████████▉ | 597891/665298 [05:34<00:19, 3405.97 examples/s]Map (num_proc=32):  90%|████████▉ | 598337/665298 [05:34<00:19, 3470.07 examples/s]Map (num_proc=32):  90%|████████▉ | 598725/665298 [05:34<00:18, 3577.26 examples/s]Map (num_proc=32):  90%|█████████ | 599207/665298 [05:34<00:19, 3405.46 examples/s]Map (num_proc=32):  90%|█████████ | 599597/665298 [05:35<00:19, 3417.29 examples/s]Map (num_proc=32):  90%|█████████ | 600067/665298 [05:35<00:19, 3384.39 examples/s]Map (num_proc=32):  90%|█████████ | 600514/665298 [05:35<00:27, 2391.80 examples/s]Map (num_proc=32):  90%|█████████ | 601468/665298 [05:35<00:17, 3689.67 examples/s]Map (num_proc=32):  90%|█████████ | 602009/665298 [05:35<00:17, 3673.30 examples/s]Map (num_proc=32):  91%|█████████ | 602462/665298 [05:35<00:16, 3768.68 examples/s]Map (num_proc=32):  91%|█████████ | 602945/665298 [05:35<00:17, 3634.68 examples/s]Map (num_proc=32):  91%|█████████ | 603385/665298 [05:36<00:18, 3413.00 examples/s]Map (num_proc=32):  91%|█████████ | 603814/665298 [05:36<00:18, 3282.85 examples/s]Map (num_proc=32):  91%|█████████ | 604240/665298 [05:36<00:19, 3114.78 examples/s]Map (num_proc=32):  91%|█████████ | 604567/665298 [05:36<00:19, 3137.08 examples/s]Map (num_proc=32):  91%|█████████ | 604937/665298 [05:36<00:18, 3238.47 examples/s]Map (num_proc=32):  91%|█████████ | 605442/665298 [05:36<00:17, 3419.98 examples/s]Map (num_proc=32):  91%|█████████ | 605840/665298 [05:36<00:16, 3553.45 examples/s]Map (num_proc=32):  91%|█████████ | 606202/665298 [05:37<00:18, 3225.05 examples/s]Map (num_proc=32):  91%|█████████ | 606655/665298 [05:37<00:17, 3297.06 examples/s]Map (num_proc=32):  91%|█████████ | 607044/665298 [05:37<00:17, 3365.51 examples/s]Map (num_proc=32):  91%|█████████▏| 607445/665298 [05:37<00:16, 3521.51 examples/s]Map (num_proc=32):  91%|█████████▏| 607902/665298 [05:37<00:15, 3620.83 examples/s]Map (num_proc=32):  91%|█████████▏| 608299/665298 [05:37<00:15, 3714.39 examples/s]Map (num_proc=32):  91%|█████████▏| 608701/665298 [05:37<00:14, 3795.95 examples/s]Map (num_proc=32):  92%|█████████▏| 609138/665298 [05:37<00:16, 3433.95 examples/s]Map (num_proc=32):  92%|█████████▏| 609518/665298 [05:37<00:18, 2989.63 examples/s]Map (num_proc=32):  92%|█████████▏| 609973/665298 [05:38<00:17, 3144.90 examples/s]Map (num_proc=32):  92%|█████████▏| 610348/665298 [05:38<00:20, 2709.66 examples/s]Map (num_proc=32):  92%|█████████▏| 610727/665298 [05:38<00:18, 2941.82 examples/s]Map (num_proc=32):  92%|█████████▏| 611093/665298 [05:38<00:17, 3014.69 examples/s]Map (num_proc=32):  92%|█████████▏| 611472/665298 [05:38<00:18, 2942.96 examples/s]Map (num_proc=32):  92%|█████████▏| 611850/665298 [05:38<00:17, 3142.37 examples/s]Map (num_proc=32):  92%|█████████▏| 612313/665298 [05:38<00:15, 3523.42 examples/s]Map (num_proc=32):  92%|█████████▏| 612711/665298 [05:38<00:15, 3503.14 examples/s]Map (num_proc=32):  92%|█████████▏| 613096/665298 [05:39<00:14, 3592.79 examples/s]Map (num_proc=32):  92%|█████████▏| 613485/665298 [05:39<00:14, 3672.54 examples/s]Map (num_proc=32):  92%|█████████▏| 613885/665298 [05:39<00:13, 3760.44 examples/s]Map (num_proc=32):  92%|█████████▏| 614353/665298 [05:39<00:15, 3387.45 examples/s]Map (num_proc=32):  92%|█████████▏| 614752/665298 [05:39<00:18, 2752.21 examples/s]Map (num_proc=32):  92%|█████████▏| 615088/665298 [05:39<00:17, 2884.82 examples/s]Map (num_proc=32):  93%|█████████▎| 615473/665298 [05:39<00:16, 3108.92 examples/s]Map (num_proc=32):  93%|█████████▎| 615886/665298 [05:39<00:16, 3049.22 examples/s]Map (num_proc=32):  93%|█████████▎| 616267/665298 [05:40<00:15, 3235.30 examples/s]Map (num_proc=32):  93%|█████████▎| 616654/665298 [05:40<00:14, 3397.24 examples/s]Map (num_proc=32):  93%|█████████▎| 617050/665298 [05:40<00:13, 3549.91 examples/s]Map (num_proc=32):  93%|█████████▎| 617464/665298 [05:40<00:14, 3304.39 examples/s]Map (num_proc=32):  93%|█████████▎| 617861/665298 [05:40<00:13, 3467.91 examples/s]Map (num_proc=32):  93%|█████████▎| 618268/665298 [05:40<00:13, 3608.18 examples/s]Map (num_proc=32):  93%|█████████▎| 618667/665298 [05:40<00:12, 3708.72 examples/s]Map (num_proc=32):  93%|█████████▎| 619120/665298 [05:40<00:12, 3686.44 examples/s]Map (num_proc=32):  93%|█████████▎| 619500/665298 [05:40<00:12, 3714.85 examples/s]Map (num_proc=32):  93%|█████████▎| 619882/665298 [05:41<00:12, 3742.68 examples/s]Map (num_proc=32):  93%|█████████▎| 620265/665298 [05:41<00:13, 3235.88 examples/s]Map (num_proc=32):  93%|█████████▎| 620666/665298 [05:41<00:22, 2003.54 examples/s]Map (num_proc=32):  93%|█████████▎| 621503/665298 [05:41<00:14, 3084.06 examples/s]Map (num_proc=32):  94%|█████████▎| 622387/665298 [05:41<00:10, 4233.36 examples/s]Map (num_proc=32):  94%|█████████▎| 623066/665298 [05:42<00:14, 2976.38 examples/s]Map (num_proc=32):  94%|█████████▎| 623513/665298 [05:42<00:26, 1586.61 examples/s]Map (num_proc=32):  94%|█████████▍| 623844/665298 [05:44<00:49, 829.35 examples/s] Map (num_proc=32):  94%|█████████▍| 624111/665298 [05:44<00:43, 946.46 examples/s]Map (num_proc=32):  94%|█████████▍| 624383/665298 [05:44<00:37, 1095.91 examples/s]Map (num_proc=32):  94%|█████████▍| 624640/665298 [05:44<00:34, 1194.06 examples/s]Map (num_proc=32):  94%|█████████▍| 624872/665298 [05:44<00:45, 897.11 examples/s] Map (num_proc=32):  94%|█████████▍| 625058/665298 [05:45<00:51, 778.53 examples/s]Map (num_proc=32):  94%|█████████▍| 625205/665298 [05:45<00:54, 733.43 examples/s]Map (num_proc=32):  94%|█████████▍| 625333/665298 [05:45<00:59, 672.83 examples/s]Map (num_proc=32):  94%|█████████▍| 625443/665298 [05:46<01:02, 636.80 examples/s]Map (num_proc=32):  94%|█████████▍| 625541/665298 [05:46<01:06, 598.75 examples/s]Map (num_proc=32):  94%|█████████▍| 625633/665298 [05:46<01:05, 603.20 examples/s]Map (num_proc=32):  94%|█████████▍| 625718/665298 [05:46<01:22, 479.65 examples/s]Map (num_proc=32):  94%|█████████▍| 625783/665298 [05:46<01:25, 464.59 examples/s]Map (num_proc=32):  94%|█████████▍| 625857/665298 [05:47<01:24, 468.04 examples/s]Map (num_proc=32):  94%|█████████▍| 625933/665298 [05:47<01:22, 476.75 examples/s]Map (num_proc=32):  94%|█████████▍| 625993/665298 [05:47<01:20, 487.32 examples/s]Map (num_proc=32):  94%|█████████▍| 626062/665298 [05:47<01:50, 355.65 examples/s]Map (num_proc=32):  94%|█████████▍| 626236/665298 [05:47<01:10, 557.80 examples/s]Map (num_proc=32):  94%|█████████▍| 626330/665298 [05:47<01:08, 571.24 examples/s]Map (num_proc=32):  94%|█████████▍| 626411/665298 [05:48<01:11, 541.10 examples/s]Map (num_proc=32):  94%|█████████▍| 626494/665298 [05:48<01:10, 548.81 examples/s]Map (num_proc=32):  94%|█████████▍| 626582/665298 [05:48<01:09, 555.30 examples/s]Map (num_proc=32):  94%|█████████▍| 626658/665298 [05:48<01:11, 544.02 examples/s]Map (num_proc=32):  94%|█████████▍| 626716/665298 [05:48<01:11, 539.87 examples/s]Map (num_proc=32):  94%|█████████▍| 626785/665298 [05:48<01:40, 381.87 examples/s]Map (num_proc=32):  94%|█████████▍| 626836/665298 [05:49<01:38, 390.16 examples/s]Map (num_proc=32):  94%|█████████▍| 626914/665298 [05:49<01:26, 441.36 examples/s]Map (num_proc=32):  94%|█████████▍| 626967/665298 [05:49<01:23, 458.51 examples/s]Map (num_proc=32):  94%|█████████▍| 627026/665298 [05:49<01:19, 484.19 examples/s]Map (num_proc=32):  94%|█████████▍| 627104/665298 [05:49<01:17, 491.19 examples/s]Map (num_proc=32):  94%|█████████▍| 627166/665298 [05:49<01:13, 519.82 examples/s]Map (num_proc=32):  94%|█████████▍| 627249/665298 [05:49<01:12, 526.81 examples/s]Map (num_proc=32):  94%|█████████▍| 627308/665298 [05:49<01:10, 540.92 examples/s]Map (num_proc=32):  94%|█████████▍| 627366/665298 [05:50<01:09, 547.10 examples/s]Map (num_proc=32):  94%|█████████▍| 627447/665298 [05:50<01:10, 534.36 examples/s]Map (num_proc=32):  94%|█████████▍| 627503/665298 [05:50<01:10, 539.15 examples/s]Map (num_proc=32):  94%|█████████▍| 627564/665298 [05:50<01:08, 552.26 examples/s]Map (num_proc=32):  94%|█████████▍| 627625/665298 [05:50<01:07, 561.78 examples/s]Map (num_proc=32):  94%|█████████▍| 627713/665298 [05:50<01:07, 555.19 examples/s]Map (num_proc=32):  94%|█████████▍| 627770/665298 [05:50<01:37, 383.27 examples/s]Map (num_proc=32):  94%|█████████▍| 627832/665298 [05:51<01:27, 427.68 examples/s]Map (num_proc=32):  94%|█████████▍| 627904/665298 [05:51<01:25, 438.31 examples/s]Map (num_proc=32):  94%|█████████▍| 627961/665298 [05:51<01:21, 458.98 examples/s]Map (num_proc=32):  94%|█████████▍| 628021/665298 [05:51<01:17, 483.47 examples/s]Map (num_proc=32):  94%|█████████▍| 628076/665298 [05:51<01:14, 499.51 examples/s]Map (num_proc=32):  94%|█████████▍| 628139/665298 [05:51<01:09, 531.02 examples/s]Map (num_proc=32):  94%|█████████▍| 628196/665298 [05:51<01:08, 540.44 examples/s]Map (num_proc=32):  94%|█████████▍| 628256/665298 [05:51<01:08, 541.85 examples/s]Map (num_proc=32):  94%|█████████▍| 628335/665298 [05:51<01:09, 531.47 examples/s]Map (num_proc=32):  94%|█████████▍| 628395/665298 [05:52<01:08, 542.31 examples/s]Map (num_proc=32):  94%|█████████▍| 628469/665298 [05:52<01:11, 517.98 examples/s]Map (num_proc=32):  94%|█████████▍| 628528/665298 [05:52<01:21, 450.69 examples/s]Map (num_proc=32):  94%|█████████▍| 628652/665298 [05:52<01:01, 592.82 examples/s]Map (num_proc=32):  95%|█████████▍| 628718/665298 [05:52<01:28, 415.15 examples/s]Map (num_proc=32):  95%|█████████▍| 628770/665298 [05:52<01:24, 432.56 examples/s]Map (num_proc=32):  95%|█████████▍| 628843/665298 [05:53<01:22, 444.45 examples/s]Map (num_proc=32):  95%|█████████▍| 628914/665298 [05:53<01:20, 449.34 examples/s]Map (num_proc=32):  95%|█████████▍| 628968/665298 [05:53<01:17, 466.63 examples/s]Map (num_proc=32):  95%|█████████▍| 629019/665298 [05:53<01:16, 474.18 examples/s]Map (num_proc=32):  95%|█████████▍| 629072/665298 [05:53<01:14, 486.98 examples/s]Map (num_proc=32):  95%|█████████▍| 629146/665298 [05:53<01:14, 484.67 examples/s]Map (num_proc=32):  95%|█████████▍| 629203/665298 [05:53<01:12, 500.52 examples/s]Map (num_proc=32):  95%|█████████▍| 629265/665298 [05:53<01:07, 530.35 examples/s]Map (num_proc=32):  95%|█████████▍| 629344/665298 [05:54<01:08, 522.96 examples/s]Map (num_proc=32):  95%|█████████▍| 629403/665298 [05:54<01:06, 535.86 examples/s]Map (num_proc=32):  95%|█████████▍| 629465/665298 [05:54<01:04, 552.77 examples/s]Map (num_proc=32):  95%|█████████▍| 629524/665298 [05:54<01:03, 562.15 examples/s]Map (num_proc=32):  95%|█████████▍| 629607/665298 [05:54<01:04, 550.64 examples/s]Map (num_proc=32):  95%|█████████▍| 629690/665298 [05:54<01:05, 547.75 examples/s]Map (num_proc=32):  95%|█████████▍| 629746/665298 [05:54<01:31, 387.88 examples/s]Map (num_proc=32):  95%|█████████▍| 629795/665298 [05:55<01:27, 405.90 examples/s]Map (num_proc=32):  95%|█████████▍| 629850/665298 [05:55<01:21, 435.98 examples/s]Map (num_proc=32):  95%|█████████▍| 629901/665298 [05:55<01:18, 448.23 examples/s]Map (num_proc=32):  95%|█████████▍| 629957/665298 [05:55<01:15, 469.85 examples/s]Map (num_proc=32):  95%|█████████▍| 630012/665298 [05:55<01:12, 487.68 examples/s]Map (num_proc=32):  95%|█████████▍| 630093/665298 [05:55<01:01, 570.45 examples/s]Map (num_proc=32):  95%|█████████▍| 630206/665298 [05:55<00:48, 723.89 examples/s]Map (num_proc=32):  95%|█████████▍| 630316/665298 [05:55<00:42, 828.41 examples/s]Map (num_proc=32):  95%|█████████▍| 630429/665298 [05:55<00:38, 910.45 examples/s]Map (num_proc=32):  95%|█████████▍| 630548/665298 [05:55<00:35, 990.53 examples/s]Map (num_proc=32):  95%|█████████▍| 630650/665298 [05:56<00:34, 991.86 examples/s]Map (num_proc=32):  95%|█████████▍| 630771/665298 [05:56<00:32, 1046.99 examples/s]Map (num_proc=32):  95%|█████████▍| 630878/665298 [05:56<00:33, 1042.79 examples/s]Map (num_proc=32):  95%|█████████▍| 631002/665298 [05:56<00:31, 1082.08 examples/s]Map (num_proc=32):  95%|█████████▍| 631123/665298 [05:56<00:30, 1113.73 examples/s]Map (num_proc=32):  95%|█████████▍| 631262/665298 [05:56<00:31, 1095.14 examples/s]Map (num_proc=32):  95%|█████████▍| 631401/665298 [05:56<00:33, 1024.03 examples/s]Map (num_proc=32):  95%|█████████▍| 631507/665298 [05:56<00:37, 899.89 examples/s] Map (num_proc=32):  95%|█████████▍| 631616/665298 [05:57<00:35, 937.70 examples/s]Map (num_proc=32):  95%|█████████▍| 631736/665298 [05:57<00:33, 992.20 examples/s]Map (num_proc=32):  95%|█████████▍| 631849/665298 [05:57<00:32, 1023.65 examples/s]Map (num_proc=32):  95%|█████████▍| 631959/665298 [05:57<00:32, 1035.38 examples/s]Map (num_proc=32):  95%|█████████▌| 632069/665298 [05:57<00:39, 851.66 examples/s] Map (num_proc=32):  95%|█████████▌| 632179/665298 [05:57<00:36, 902.36 examples/s]Map (num_proc=32):  95%|█████████▌| 632283/665298 [05:57<00:35, 928.35 examples/s]Map (num_proc=32):  95%|█████████▌| 632389/665298 [05:57<00:34, 956.70 examples/s]Map (num_proc=32):  95%|█████████▌| 632505/665298 [05:57<00:32, 1000.13 examples/s]Map (num_proc=32):  95%|█████████▌| 632615/665298 [05:58<00:32, 1014.08 examples/s]Map (num_proc=32):  95%|█████████▌| 632735/665298 [05:58<00:33, 978.04 examples/s] Map (num_proc=32):  95%|█████████▌| 632850/665298 [05:58<00:32, 1012.87 examples/s]Map (num_proc=32):  95%|█████████▌| 632956/665298 [05:58<00:31, 1021.15 examples/s]Map (num_proc=32):  95%|█████████▌| 633066/665298 [05:58<00:31, 1035.86 examples/s]Map (num_proc=32):  95%|█████████▌| 633172/665298 [05:58<00:31, 1030.92 examples/s]Map (num_proc=32):  95%|█████████▌| 633289/665298 [05:58<00:30, 1061.87 examples/s]Map (num_proc=32):  95%|█████████▌| 633401/665298 [05:58<00:36, 883.78 examples/s] Map (num_proc=32):  95%|█████████▌| 633517/665298 [05:59<00:37, 854.14 examples/s]Map (num_proc=32):  95%|█████████▌| 633633/665298 [05:59<00:34, 918.77 examples/s]Map (num_proc=32):  95%|█████████▌| 633740/665298 [05:59<00:33, 941.89 examples/s]Map (num_proc=32):  95%|█████████▌| 633846/665298 [05:59<00:32, 971.15 examples/s]Map (num_proc=32):  95%|█████████▌| 633957/665298 [05:59<00:31, 1002.68 examples/s]Map (num_proc=32):  95%|█████████▌| 634072/665298 [05:59<00:30, 1037.66 examples/s]Map (num_proc=32):  95%|█████████▌| 634178/665298 [05:59<00:36, 862.79 examples/s] Map (num_proc=32):  95%|█████████▌| 634282/665298 [05:59<00:34, 898.64 examples/s]Map (num_proc=32):  95%|█████████▌| 634394/665298 [05:59<00:32, 951.97 examples/s]Map (num_proc=32):  95%|█████████▌| 634502/665298 [06:00<00:31, 974.83 examples/s]Map (num_proc=32):  95%|█████████▌| 634623/665298 [06:00<00:29, 1035.12 examples/s]Map (num_proc=32):  95%|█████████▌| 634739/665298 [06:00<00:28, 1060.97 examples/s]Map (num_proc=32):  95%|█████████▌| 634850/665298 [06:00<00:28, 1069.33 examples/s]Map (num_proc=32):  95%|█████████▌| 634966/665298 [06:00<00:27, 1085.39 examples/s]Map (num_proc=32):  95%|█████████▌| 635098/665298 [06:00<00:28, 1051.16 examples/s]Map (num_proc=32):  95%|█████████▌| 635218/665298 [06:00<00:27, 1082.92 examples/s]Map (num_proc=32):  95%|█████████▌| 635329/665298 [06:00<00:33, 905.81 examples/s] Map (num_proc=32):  96%|█████████▌| 635445/665298 [06:00<00:33, 892.69 examples/s]Map (num_proc=32):  96%|█████████▌| 635547/665298 [06:01<00:32, 919.07 examples/s]Map (num_proc=32):  96%|█████████▌| 635649/665298 [06:01<00:31, 939.16 examples/s]Map (num_proc=32):  96%|█████████▌| 635757/665298 [06:01<00:30, 975.93 examples/s]Map (num_proc=32):  96%|█████████▌| 635857/665298 [06:01<00:30, 971.75 examples/s]Map (num_proc=32):  96%|█████████▌| 635961/665298 [06:01<00:29, 982.92 examples/s]Map (num_proc=32):  96%|█████████▌| 636069/665298 [06:01<00:29, 1007.34 examples/s]Map (num_proc=32):  96%|█████████▌| 636196/665298 [06:01<00:35, 825.57 examples/s] Map (num_proc=32):  96%|█████████▌| 636309/665298 [06:01<00:32, 894.53 examples/s]Map (num_proc=32):  96%|█████████▌| 636405/665298 [06:01<00:32, 902.13 examples/s]Map (num_proc=32):  96%|█████████▌| 636510/665298 [06:02<00:30, 934.49 examples/s]Map (num_proc=32):  96%|█████████▌| 636619/665298 [06:02<00:29, 965.08 examples/s]Map (num_proc=32):  96%|█████████▌| 636727/665298 [06:02<00:28, 986.54 examples/s]Map (num_proc=32):  96%|█████████▌| 636837/665298 [06:02<00:28, 1014.16 examples/s]Map (num_proc=32):  96%|█████████▌| 636952/665298 [06:02<00:27, 1044.50 examples/s]Map (num_proc=32):  96%|█████████▌| 637062/665298 [06:02<00:26, 1052.35 examples/s]Map (num_proc=32):  96%|█████████▌| 637191/665298 [06:02<00:27, 1003.86 examples/s]Map (num_proc=32):  96%|█████████▌| 637322/665298 [06:02<00:32, 849.96 examples/s] Map (num_proc=32):  96%|█████████▌| 637423/665298 [06:03<00:33, 821.47 examples/s]Map (num_proc=32):  96%|█████████▌| 637529/665298 [06:03<00:31, 869.64 examples/s]Map (num_proc=32):  96%|█████████▌| 637632/665298 [06:03<00:30, 905.20 examples/s]Map (num_proc=32):  96%|█████████▌| 637738/665298 [06:03<00:29, 943.44 examples/s]Map (num_proc=32):  96%|█████████▌| 637863/665298 [06:03<00:26, 1023.65 examples/s]Map (num_proc=32):  96%|█████████▌| 637981/665298 [06:03<00:25, 1056.90 examples/s]Map (num_proc=32):  96%|█████████▌| 638109/665298 [06:03<00:31, 866.90 examples/s] Map (num_proc=32):  96%|█████████▌| 638222/665298 [06:03<00:29, 917.48 examples/s]Map (num_proc=32):  96%|█████████▌| 638346/665298 [06:04<00:28, 944.39 examples/s]Map (num_proc=32):  96%|█████████▌| 638463/665298 [06:04<00:27, 977.89 examples/s]Map (num_proc=32):  96%|█████████▌| 638581/665298 [06:04<00:26, 1027.31 examples/s]Map (num_proc=32):  96%|█████████▌| 638709/665298 [06:04<00:25, 1025.53 examples/s]Map (num_proc=32):  96%|█████████▌| 638816/665298 [06:04<00:25, 1035.81 examples/s]Map (num_proc=32):  96%|█████████▌| 638927/665298 [06:04<00:25, 1053.07 examples/s]Map (num_proc=32):  96%|█████████▌| 639065/665298 [06:04<00:24, 1057.30 examples/s]Map (num_proc=32):  96%|█████████▌| 639183/665298 [06:04<00:24, 1082.02 examples/s]Map (num_proc=32):  96%|█████████▌| 639309/665298 [06:04<00:26, 979.68 examples/s] Map (num_proc=32):  96%|█████████▌| 639416/665298 [06:05<00:27, 944.32 examples/s]Map (num_proc=32):  96%|█████████▌| 639524/665298 [06:05<00:26, 970.04 examples/s]Map (num_proc=32):  96%|█████████▌| 639631/665298 [06:05<00:25, 994.19 examples/s]Map (num_proc=32):  96%|█████████▌| 639755/665298 [06:05<00:24, 1059.07 examples/s]Map (num_proc=32):  96%|█████████▌| 639867/665298 [06:05<00:23, 1071.98 examples/s]Map (num_proc=32):  96%|█████████▌| 639982/665298 [06:05<00:23, 1090.51 examples/s]Map (num_proc=32):  96%|█████████▌| 640094/665298 [06:05<00:29, 851.92 examples/s] Map (num_proc=32):  96%|█████████▌| 640199/665298 [06:05<00:27, 897.39 examples/s]Map (num_proc=32):  96%|█████████▌| 640298/665298 [06:05<00:27, 917.99 examples/s]Map (num_proc=32):  96%|█████████▋| 640410/665298 [06:06<00:25, 970.02 examples/s]Map (num_proc=32):  96%|█████████▋| 640515/665298 [06:06<00:25, 982.06 examples/s]Map (num_proc=32):  96%|█████████▋| 640617/665298 [06:06<00:24, 991.49 examples/s]Map (num_proc=32):  96%|█████████▋| 640739/665298 [06:06<00:23, 1052.01 examples/s]Map (num_proc=32):  96%|█████████▋| 640850/665298 [06:06<00:22, 1068.11 examples/s]Map (num_proc=32):  96%|█████████▋| 640984/665298 [06:06<00:21, 1129.03 examples/s]Map (num_proc=32):  96%|█████████▋| 641111/665298 [06:06<00:23, 1020.03 examples/s]Map (num_proc=32):  96%|█████████▋| 641231/665298 [06:06<00:22, 1063.75 examples/s]Map (num_proc=32):  96%|█████████▋| 641349/665298 [06:07<00:25, 953.35 examples/s] Map (num_proc=32):  96%|█████████▋| 641450/665298 [06:07<00:24, 965.11 examples/s]Map (num_proc=32):  96%|█████████▋| 641563/665298 [06:07<00:23, 1007.77 examples/s]Map (num_proc=32):  96%|█████████▋| 641682/665298 [06:07<00:25, 930.73 examples/s] Map (num_proc=32):  96%|█████████▋| 641792/665298 [06:07<00:24, 965.13 examples/s]Map (num_proc=32):  96%|█████████▋| 641909/665298 [06:07<00:23, 1013.06 examples/s]Map (num_proc=32):  97%|█████████▋| 642019/665298 [06:07<00:22, 1021.77 examples/s]Map (num_proc=32):  97%|█████████▋| 642125/665298 [06:07<00:26, 881.15 examples/s] Map (num_proc=32):  97%|█████████▋| 642219/665298 [06:07<00:25, 893.30 examples/s]Map (num_proc=32):  97%|█████████▋| 642331/665298 [06:08<00:24, 951.59 examples/s]Map (num_proc=32):  97%|█████████▋| 642443/665298 [06:08<00:23, 993.43 examples/s]Map (num_proc=32):  97%|█████████▋| 642558/665298 [06:08<00:22, 1027.86 examples/s]Map (num_proc=32):  97%|█████████▋| 642665/665298 [06:08<00:21, 1038.30 examples/s]Map (num_proc=32):  97%|█████████▋| 642774/665298 [06:08<00:21, 1028.88 examples/s]Map (num_proc=32):  97%|█████████▋| 642904/665298 [06:08<00:22, 982.20 examples/s] Map (num_proc=32):  97%|█████████▋| 643024/665298 [06:08<00:21, 1038.11 examples/s]Map (num_proc=32):  97%|█████████▋| 643137/665298 [06:08<00:20, 1058.35 examples/s]Map (num_proc=32):  97%|█████████▋| 643274/665298 [06:08<00:19, 1101.51 examples/s]Map (num_proc=32):  97%|█████████▋| 643401/665298 [06:09<00:24, 893.35 examples/s] Map (num_proc=32):  97%|█████████▋| 643511/665298 [06:09<00:24, 884.77 examples/s]Map (num_proc=32):  97%|█████████▋| 643624/665298 [06:09<00:22, 942.64 examples/s]Map (num_proc=32):  97%|█████████▋| 643733/665298 [06:09<00:22, 973.64 examples/s]Map (num_proc=32):  97%|█████████▋| 643864/665298 [06:09<00:21, 982.25 examples/s]Map (num_proc=32):  97%|█████████▋| 643975/665298 [06:09<00:22, 951.85 examples/s]Map (num_proc=32):  97%|█████████▋| 644097/665298 [06:09<00:24, 867.55 examples/s]Map (num_proc=32):  97%|█████████▋| 644207/665298 [06:09<00:23, 911.33 examples/s]Map (num_proc=32):  97%|█████████▋| 644316/665298 [06:10<00:21, 955.98 examples/s]Map (num_proc=32):  97%|█████████▋| 644421/665298 [06:10<00:21, 971.35 examples/s]Map (num_proc=32):  97%|█████████▋| 644538/665298 [06:10<00:26, 793.72 examples/s]Map (num_proc=32):  97%|█████████▋| 644654/665298 [06:10<00:23, 876.02 examples/s]Map (num_proc=32):  97%|█████████▋| 644767/665298 [06:10<00:21, 935.82 examples/s]Map (num_proc=32):  97%|█████████▋| 644880/665298 [06:10<00:20, 981.97 examples/s]Map (num_proc=32):  97%|█████████▋| 645005/665298 [06:10<00:19, 1047.97 examples/s]Map (num_proc=32):  97%|█████████▋| 645120/665298 [06:10<00:18, 1066.29 examples/s]Map (num_proc=32):  97%|█████████▋| 645233/665298 [06:11<00:18, 1079.23 examples/s]Map (num_proc=32):  97%|█████████▋| 645351/665298 [06:11<00:18, 1104.95 examples/s]Map (num_proc=32):  97%|█████████▋| 645483/665298 [06:11<00:18, 1056.33 examples/s]Map (num_proc=32):  97%|█████████▋| 645597/665298 [06:11<00:21, 909.16 examples/s] Map (num_proc=32):  97%|█████████▋| 645701/665298 [06:11<00:20, 936.25 examples/s]Map (num_proc=32):  97%|█████████▋| 645801/665298 [06:11<00:20, 945.51 examples/s]Map (num_proc=32):  97%|█████████▋| 645913/665298 [06:11<00:24, 790.34 examples/s]Map (num_proc=32):  97%|█████████▋| 646026/665298 [06:11<00:22, 864.42 examples/s]Map (num_proc=32):  97%|█████████▋| 646136/665298 [06:12<00:20, 913.68 examples/s]Map (num_proc=32):  97%|█████████▋| 646253/665298 [06:12<00:19, 979.42 examples/s]Map (num_proc=32):  97%|█████████▋| 646374/665298 [06:12<00:20, 906.38 examples/s]Map (num_proc=32):  97%|█████████▋| 646476/665298 [06:12<00:20, 929.83 examples/s]Map (num_proc=32):  97%|█████████▋| 646584/665298 [06:12<00:19, 968.22 examples/s]Map (num_proc=32):  97%|█████████▋| 646707/665298 [06:12<00:17, 1036.62 examples/s]Map (num_proc=32):  97%|█████████▋| 646818/665298 [06:12<00:17, 1051.43 examples/s]Map (num_proc=32):  97%|█████████▋| 646930/665298 [06:12<00:17, 1053.96 examples/s]Map (num_proc=32):  97%|█████████▋| 647067/665298 [06:12<00:16, 1106.04 examples/s]Map (num_proc=32):  97%|█████████▋| 647200/665298 [06:13<00:17, 1040.37 examples/s]Map (num_proc=32):  97%|█████████▋| 647314/665298 [06:13<00:16, 1061.64 examples/s]Map (num_proc=32):  97%|█████████▋| 647422/665298 [06:13<00:16, 1061.17 examples/s]Map (num_proc=32):  97%|█████████▋| 647549/665298 [06:13<00:20, 870.47 examples/s] Map (num_proc=32):  97%|█████████▋| 647655/665298 [06:13<00:19, 912.08 examples/s]Map (num_proc=32):  97%|█████████▋| 647772/665298 [06:13<00:17, 974.14 examples/s]Map (num_proc=32):  97%|█████████▋| 647883/665298 [06:13<00:20, 843.01 examples/s]Map (num_proc=32):  97%|█████████▋| 647984/665298 [06:13<00:21, 790.52 examples/s]Map (num_proc=32):  97%|█████████▋| 648092/665298 [06:14<00:20, 850.93 examples/s]Map (num_proc=32):  97%|█████████▋| 648201/665298 [06:14<00:19, 897.57 examples/s]Map (num_proc=32):  97%|█████████▋| 648318/665298 [06:14<00:17, 965.37 examples/s]Map (num_proc=32):  97%|█████████▋| 648424/665298 [06:14<00:17, 978.97 examples/s]Map (num_proc=32):  97%|█████████▋| 648537/665298 [06:14<00:16, 1009.50 examples/s]Map (num_proc=32):  97%|█████████▋| 648643/665298 [06:14<00:16, 1017.06 examples/s]Map (num_proc=32):  98%|█████████▊| 648761/665298 [06:14<00:15, 1057.09 examples/s]Map (num_proc=32):  98%|█████████▊| 648875/665298 [06:14<00:15, 1078.48 examples/s]Map (num_proc=32):  98%|█████████▊| 649001/665298 [06:14<00:14, 1094.20 examples/s]Map (num_proc=32):  98%|█████████▊| 649116/665298 [06:15<00:16, 995.17 examples/s] Map (num_proc=32):  98%|█████████▊| 649236/665298 [06:15<00:15, 1045.20 examples/s]Map (num_proc=32):  98%|█████████▊| 649353/665298 [06:15<00:14, 1074.25 examples/s]Map (num_proc=32):  98%|█████████▊| 649485/665298 [06:15<00:16, 980.18 examples/s] Map (num_proc=32):  98%|█████████▊| 649607/665298 [06:15<00:17, 919.17 examples/s]Map (num_proc=32):  98%|█████████▊| 649717/665298 [06:15<00:16, 959.18 examples/s]Map (num_proc=32):  98%|█████████▊| 649842/665298 [06:15<00:16, 927.80 examples/s]Map (num_proc=32):  98%|█████████▊| 649954/665298 [06:15<00:17, 860.40 examples/s]Map (num_proc=32):  98%|█████████▊| 650060/665298 [06:16<00:16, 901.45 examples/s]Map (num_proc=32):  98%|█████████▊| 650170/665298 [06:16<00:16, 941.21 examples/s]Map (num_proc=32):  98%|█████████▊| 650274/665298 [06:16<00:15, 959.19 examples/s]Map (num_proc=32):  98%|█████████▊| 650391/665298 [06:16<00:16, 918.43 examples/s]Map (num_proc=32):  98%|█████████▊| 650498/665298 [06:16<00:15, 955.21 examples/s]Map (num_proc=32):  98%|█████████▊| 650600/665298 [06:16<00:15, 969.82 examples/s]Map (num_proc=32):  98%|█████████▊| 650709/665298 [06:16<00:14, 998.78 examples/s]Map (num_proc=32):  98%|█████████▊| 650827/665298 [06:16<00:13, 1040.93 examples/s]Map (num_proc=32):  98%|█████████▊| 650943/665298 [06:16<00:13, 1068.53 examples/s]Map (num_proc=32):  98%|█████████▊| 651068/665298 [06:17<00:12, 1112.60 examples/s]Map (num_proc=32):  98%|█████████▊| 651181/665298 [06:17<00:12, 1108.10 examples/s]Map (num_proc=32):  98%|█████████▊| 651316/665298 [06:17<00:12, 1120.33 examples/s]Map (num_proc=32):  98%|█████████▊| 651458/665298 [06:17<00:12, 1085.42 examples/s]Map (num_proc=32):  98%|█████████▊| 651575/665298 [06:17<00:15, 884.74 examples/s] Map (num_proc=32):  98%|█████████▊| 651684/665298 [06:17<00:14, 927.63 examples/s]Map (num_proc=32):  98%|█████████▊| 651790/665298 [06:17<00:14, 951.64 examples/s]Map (num_proc=32):  98%|█████████▊| 651911/665298 [06:17<00:16, 803.25 examples/s]Map (num_proc=32):  98%|█████████▊| 652015/665298 [06:18<00:15, 845.22 examples/s]Map (num_proc=32):  98%|█████████▊| 652120/665298 [06:18<00:14, 888.78 examples/s]Map (num_proc=32):  98%|█████████▊| 652228/665298 [06:18<00:13, 934.24 examples/s]Map (num_proc=32):  98%|█████████▊| 652333/665298 [06:18<00:13, 958.93 examples/s]Map (num_proc=32):  98%|█████████▊| 652450/665298 [06:18<00:12, 1006.55 examples/s]Map (num_proc=32):  98%|█████████▊| 652568/665298 [06:18<00:12, 1047.30 examples/s]Map (num_proc=32):  98%|█████████▊| 652697/665298 [06:18<00:11, 1084.42 examples/s]Map (num_proc=32):  98%|█████████▊| 652830/665298 [06:18<00:12, 1020.92 examples/s]Map (num_proc=32):  98%|█████████▊| 652943/665298 [06:18<00:11, 1048.64 examples/s]Map (num_proc=32):  98%|█████████▊| 653075/665298 [06:19<00:11, 1081.16 examples/s]Map (num_proc=32):  98%|█████████▊| 653194/665298 [06:19<00:10, 1105.54 examples/s]Map (num_proc=32):  98%|█████████▊| 653308/665298 [06:19<00:10, 1100.92 examples/s]Map (num_proc=32):  98%|█████████▊| 653419/665298 [06:19<00:10, 1100.88 examples/s]Map (num_proc=32):  98%|█████████▊| 653532/665298 [06:19<00:12, 961.70 examples/s] Map (num_proc=32):  98%|█████████▊| 653657/665298 [06:19<00:12, 908.50 examples/s]Map (num_proc=32):  98%|█████████▊| 653767/665298 [06:19<00:12, 948.28 examples/s]Map (num_proc=32):  98%|█████████▊| 653875/665298 [06:19<00:13, 860.49 examples/s]Map (num_proc=32):  98%|█████████▊| 653968/665298 [06:20<00:13, 841.78 examples/s]Map (num_proc=32):  98%|█████████▊| 654080/665298 [06:20<00:12, 905.47 examples/s]Map (num_proc=32):  98%|█████████▊| 654196/665298 [06:20<00:11, 970.16 examples/s]Map (num_proc=32):  98%|█████████▊| 654302/665298 [06:20<00:11, 990.12 examples/s]Map (num_proc=32):  98%|█████████▊| 654405/665298 [06:20<00:10, 994.09 examples/s]Map (num_proc=32):  98%|█████████▊| 654519/665298 [06:20<00:10, 1031.16 examples/s]Map (num_proc=32):  98%|█████████▊| 654637/665298 [06:20<00:09, 1068.62 examples/s]Map (num_proc=32):  98%|█████████▊| 654772/665298 [06:20<00:10, 1010.50 examples/s]Map (num_proc=32):  98%|█████████▊| 654893/665298 [06:20<00:09, 1062.47 examples/s]Map (num_proc=32):  98%|█████████▊| 655004/665298 [06:21<00:09, 1072.95 examples/s]Map (num_proc=32):  98%|█████████▊| 655133/665298 [06:21<00:09, 1086.40 examples/s]Map (num_proc=32):  98%|█████████▊| 655262/665298 [06:21<00:09, 1035.78 examples/s]Map (num_proc=32):  99%|█████████▊| 655394/665298 [06:21<00:12, 806.43 examples/s] Map (num_proc=32):  99%|█████████▊| 655490/665298 [06:21<00:12, 756.51 examples/s]Map (num_proc=32):  99%|█████████▊| 655598/665298 [06:21<00:11, 817.10 examples/s]Map (num_proc=32):  99%|█████████▊| 655706/665298 [06:21<00:10, 876.62 examples/s]Map (num_proc=32):  99%|█████████▊| 655813/665298 [06:21<00:10, 914.79 examples/s]Map (num_proc=32):  99%|█████████▊| 655925/665298 [06:22<00:10, 883.41 examples/s]Map (num_proc=32):  99%|█████████▊| 656037/665298 [06:22<00:11, 823.82 examples/s]Map (num_proc=32):  99%|█████████▊| 656141/665298 [06:22<00:10, 865.91 examples/s]Map (num_proc=32):  99%|█████████▊| 656249/665298 [06:22<00:09, 909.54 examples/s]Map (num_proc=32):  99%|█████████▊| 656364/665298 [06:22<00:09, 961.87 examples/s]Map (num_proc=32):  99%|█████████▊| 656472/665298 [06:22<00:08, 991.15 examples/s]Map (num_proc=32):  99%|█████████▊| 656599/665298 [06:22<00:08, 1020.16 examples/s]Map (num_proc=32):  99%|█████████▊| 656707/665298 [06:22<00:08, 1019.97 examples/s]Map (num_proc=32):  99%|█████████▊| 656830/665298 [06:23<00:08, 980.35 examples/s] Map (num_proc=32):  99%|█████████▊| 656940/665298 [06:23<00:08, 1002.31 examples/s]Map (num_proc=32):  99%|█████████▉| 657050/665298 [06:23<00:08, 1019.49 examples/s]Map (num_proc=32):  99%|█████████▉| 657160/665298 [06:23<00:07, 1036.78 examples/s]Map (num_proc=32):  99%|█████████▉| 657276/665298 [06:23<00:07, 1065.09 examples/s]Map (num_proc=32):  99%|█████████▉| 657392/665298 [06:23<00:07, 1045.10 examples/s]Map (num_proc=32):  99%|█████████▉| 657498/665298 [06:23<00:08, 896.69 examples/s] Map (num_proc=32):  99%|█████████▉| 657620/665298 [06:23<00:07, 969.55 examples/s]Map (num_proc=32):  99%|█████████▉| 657728/665298 [06:23<00:07, 994.51 examples/s]Map (num_proc=32):  99%|█████████▉| 657836/665298 [06:24<00:07, 1012.23 examples/s]Map (num_proc=32):  99%|█████████▉| 657940/665298 [06:24<00:07, 1014.43 examples/s]Map (num_proc=32):  99%|█████████▉| 658066/665298 [06:24<00:08, 832.63 examples/s] Map (num_proc=32):  99%|█████████▉| 658162/665298 [06:24<00:08, 835.28 examples/s]Map (num_proc=32):  99%|█████████▉| 658291/665298 [06:24<00:07, 938.18 examples/s]Map (num_proc=32):  99%|█████████▉| 658410/665298 [06:24<00:06, 989.88 examples/s]Map (num_proc=32):  99%|█████████▉| 658523/665298 [06:24<00:06, 1021.53 examples/s]Map (num_proc=32):  99%|█████████▉| 658635/665298 [06:24<00:06, 1040.12 examples/s]Map (num_proc=32):  99%|█████████▉| 658765/665298 [06:25<00:06, 1010.71 examples/s]Map (num_proc=32):  99%|█████████▉| 658878/665298 [06:25<00:06, 1036.33 examples/s]Map (num_proc=32):  99%|█████████▉| 658992/665298 [06:25<00:07, 886.30 examples/s] Map (num_proc=32):  99%|█████████▉| 659106/665298 [06:25<00:08, 749.85 examples/s]Map (num_proc=32):  99%|█████████▉| 659210/665298 [06:25<00:09, 662.41 examples/s]Map (num_proc=32):  99%|█████████▉| 659299/665298 [06:25<00:09, 639.89 examples/s]Map (num_proc=32):  99%|█████████▉| 659384/665298 [06:26<00:09, 618.27 examples/s]Map (num_proc=32):  99%|█████████▉| 659462/665298 [06:26<00:09, 584.81 examples/s]Map (num_proc=32):  99%|█████████▉| 659524/665298 [06:28<01:05, 87.63 examples/s] Map (num_proc=32):  99%|█████████▉| 660508/665298 [06:29<00:09, 483.82 examples/s]Map (num_proc=32):  99%|█████████▉| 660694/665298 [06:29<00:09, 490.20 examples/s]Map (num_proc=32):  99%|█████████▉| 660848/665298 [06:29<00:09, 491.59 examples/s]Map (num_proc=32):  99%|█████████▉| 660970/665298 [06:30<00:08, 488.70 examples/s]Map (num_proc=32):  99%|█████████▉| 661081/665298 [06:30<00:08, 496.16 examples/s]Map (num_proc=32):  99%|█████████▉| 661185/665298 [06:30<00:08, 498.55 examples/s]Map (num_proc=32):  99%|█████████▉| 661276/665298 [06:30<00:07, 515.31 examples/s]Map (num_proc=32):  99%|█████████▉| 661363/665298 [06:30<00:07, 522.77 examples/s]Map (num_proc=32):  99%|█████████▉| 661446/665298 [06:30<00:07, 527.05 examples/s]Map (num_proc=32):  99%|█████████▉| 661508/665298 [06:31<00:09, 410.11 examples/s]Map (num_proc=32):  99%|█████████▉| 661569/665298 [06:31<00:09, 405.49 examples/s]Map (num_proc=32):  99%|█████████▉| 661626/665298 [06:31<00:08, 431.59 examples/s]Map (num_proc=32):  99%|█████████▉| 661676/665298 [06:31<00:08, 443.89 examples/s]Map (num_proc=32):  99%|█████████▉| 661741/665298 [06:31<00:07, 485.19 examples/s]Map (num_proc=32):  99%|█████████▉| 661798/665298 [06:31<00:06, 501.45 examples/s]Map (num_proc=32):  99%|█████████▉| 661863/665298 [06:31<00:06, 537.61 examples/s]Map (num_proc=32):  99%|█████████▉| 661940/665298 [06:32<00:06, 515.57 examples/s]Map (num_proc=32): 100%|█████████▉| 661997/665298 [06:32<00:06, 527.49 examples/s]Map (num_proc=32): 100%|█████████▉| 662058/665298 [06:32<00:05, 543.56 examples/s]Map (num_proc=32): 100%|█████████▉| 662129/665298 [06:32<00:06, 517.25 examples/s]Map (num_proc=32): 100%|█████████▉| 662189/665298 [06:32<00:05, 536.84 examples/s]Map (num_proc=32): 100%|█████████▉| 662261/665298 [06:32<00:05, 507.88 examples/s]Map (num_proc=32): 100%|█████████▉| 662321/665298 [06:32<00:05, 527.10 examples/s]Map (num_proc=32): 100%|█████████▉| 662401/665298 [06:32<00:05, 521.64 examples/s]Map (num_proc=32): 100%|█████████▉| 662461/665298 [06:33<00:05, 537.44 examples/s]Map (num_proc=32): 100%|█████████▉| 662536/665298 [06:33<00:07, 394.41 examples/s]Map (num_proc=32): 100%|█████████▉| 662593/665298 [06:33<00:06, 426.92 examples/s]Map (num_proc=32): 100%|█████████▉| 662642/665298 [06:33<00:06, 439.30 examples/s]Map (num_proc=32): 100%|█████████▉| 662696/665298 [06:33<00:05, 460.28 examples/s]Map (num_proc=32): 100%|█████████▉| 662751/665298 [06:33<00:05, 480.18 examples/s]Map (num_proc=32): 100%|█████████▉| 662809/665298 [06:33<00:05, 489.07 examples/s]Map (num_proc=32): 100%|█████████▉| 662867/665298 [06:33<00:04, 510.76 examples/s]Map (num_proc=32): 100%|█████████▉| 662949/665298 [06:34<00:04, 517.12 examples/s]Map (num_proc=32): 100%|█████████▉| 663006/665298 [06:34<00:04, 524.78 examples/s]Map (num_proc=32): 100%|█████████▉| 663062/665298 [06:34<00:04, 531.23 examples/s]Map (num_proc=32): 100%|█████████▉| 663143/665298 [06:34<00:04, 523.79 examples/s]Map (num_proc=32): 100%|█████████▉| 663220/665298 [06:34<00:04, 514.36 examples/s]Map (num_proc=32): 100%|█████████▉| 663279/665298 [06:34<00:03, 530.59 examples/s]Map (num_proc=32): 100%|█████████▉| 663339/665298 [06:34<00:03, 544.88 examples/s]Map (num_proc=32): 100%|█████████▉| 663413/665298 [06:35<00:03, 522.17 examples/s]Map (num_proc=32): 100%|█████████▉| 663496/665298 [06:35<00:03, 525.59 examples/s]Map (num_proc=32): 100%|█████████▉| 663555/665298 [06:35<00:04, 380.06 examples/s]Map (num_proc=32): 100%|█████████▉| 663609/665298 [06:35<00:04, 410.10 examples/s]Map (num_proc=32): 100%|█████████▉| 663665/665298 [06:35<00:03, 436.70 examples/s]Map (num_proc=32): 100%|█████████▉| 663720/665298 [06:35<00:03, 460.04 examples/s]Map (num_proc=32): 100%|█████████▉| 663780/665298 [06:35<00:03, 493.83 examples/s]Map (num_proc=32): 100%|█████████▉| 663834/665298 [06:35<00:02, 500.83 examples/s]Map (num_proc=32): 100%|█████████▉| 663910/665298 [06:36<00:02, 498.09 examples/s]Map (num_proc=32): 100%|█████████▉| 663967/665298 [06:36<00:02, 512.28 examples/s]Map (num_proc=32): 100%|█████████▉| 664027/665298 [06:36<00:02, 531.99 examples/s]Map (num_proc=32): 100%|█████████▉| 664109/665298 [06:36<00:02, 530.36 examples/s]Map (num_proc=32): 100%|█████████▉| 664177/665298 [06:36<00:02, 502.89 examples/s]Map (num_proc=32): 100%|█████████▉| 664241/665298 [06:36<00:01, 531.62 examples/s]Map (num_proc=32): 100%|█████████▉| 664301/665298 [06:36<00:01, 546.77 examples/s]Map (num_proc=32): 100%|█████████▉| 664381/665298 [06:37<00:01, 541.02 examples/s]Map (num_proc=32): 100%|█████████▉| 664440/665298 [06:37<00:01, 549.00 examples/s]Map (num_proc=32): 100%|█████████▉| 664508/665298 [06:37<00:02, 385.89 examples/s]Map (num_proc=32): 100%|█████████▉| 664581/665298 [06:37<00:01, 410.92 examples/s]Map (num_proc=32): 100%|█████████▉| 664628/665298 [06:37<00:01, 422.13 examples/s]Map (num_proc=32): 100%|█████████▉| 664679/665298 [06:37<00:01, 441.25 examples/s]Map (num_proc=32): 100%|█████████▉| 664738/665298 [06:37<00:01, 474.48 examples/s]Map (num_proc=32): 100%|█████████▉| 664794/665298 [06:37<00:01, 490.98 examples/s]Map (num_proc=32): 100%|█████████▉| 664868/665298 [06:38<00:00, 489.96 examples/s]Map (num_proc=32): 100%|█████████▉| 664919/665298 [06:38<00:00, 491.90 examples/s]Map (num_proc=32): 100%|█████████▉| 664974/665298 [06:38<00:00, 501.12 examples/s]Map (num_proc=32): 100%|█████████▉| 665031/665298 [06:38<00:00, 517.27 examples/s]Map (num_proc=32): 100%|█████████▉| 665090/665298 [06:38<00:00, 520.68 examples/s]Map (num_proc=32): 100%|█████████▉| 665168/665298 [06:38<00:00, 529.05 examples/s]Map (num_proc=32): 100%|█████████▉| 665243/665298 [06:38<00:00, 515.59 examples/s]Map (num_proc=32): 100%|██████████| 665298/665298 [06:39<00:00, 401.89 examples/s]Map (num_proc=32): 100%|██████████| 665298/665298 [06:43<00:00, 1648.02 examples/s]
Filter (num_proc=32):   0%|          | 0/665298 [00:00<?, ? examples/s]Filter (num_proc=32):   0%|          | 1000/665298 [00:01<21:28, 515.75 examples/s]Filter (num_proc=32):   0%|          | 2000/665298 [00:02<11:33, 956.71 examples/s]Filter (num_proc=32):   0%|          | 3000/665298 [00:02<07:12, 1531.39 examples/s]Filter (num_proc=32):   1%|          | 4000/665298 [00:02<05:32, 1986.02 examples/s]Filter (num_proc=32):   1%|          | 5000/665298 [00:02<04:10, 2631.81 examples/s]Filter (num_proc=32):   1%|          | 7000/665298 [00:03<02:22, 4610.68 examples/s]Filter (num_proc=32):   1%|▏         | 9000/665298 [00:03<01:41, 6470.99 examples/s]Filter (num_proc=32):   2%|▏         | 11000/665298 [00:03<01:26, 7606.99 examples/s]Filter (num_proc=32):   2%|▏         | 14000/665298 [00:03<00:57, 11354.39 examples/s]Filter (num_proc=32):   2%|▏         | 16000/665298 [00:03<00:54, 11821.66 examples/s]Filter (num_proc=32):   3%|▎         | 18000/665298 [00:03<00:50, 12845.97 examples/s]Filter (num_proc=32):   3%|▎         | 21000/665298 [00:03<00:39, 16238.82 examples/s]Filter (num_proc=32):   4%|▎         | 24000/665298 [00:04<00:35, 18312.65 examples/s]Filter (num_proc=32):   4%|▍         | 28000/665298 [00:04<00:32, 19873.94 examples/s]Filter (num_proc=32):   5%|▍         | 33000/665298 [00:04<00:25, 25130.35 examples/s]Filter (num_proc=32):   5%|▌         | 36000/665298 [00:04<00:25, 24231.21 examples/s]Filter (num_proc=32):   6%|▌         | 39000/665298 [00:04<00:27, 23164.36 examples/s]Filter (num_proc=32):   7%|▋         | 44000/665298 [00:04<00:23, 26902.60 examples/s]Filter (num_proc=32):   8%|▊         | 50000/665298 [00:04<00:19, 31215.14 examples/s]Filter (num_proc=32):   8%|▊         | 55000/665298 [00:05<00:18, 33120.09 examples/s]Filter (num_proc=32):   9%|▉         | 59000/665298 [00:05<00:19, 31738.38 examples/s]Filter (num_proc=32):  10%|▉         | 64000/665298 [00:05<00:19, 30477.52 examples/s]Filter (num_proc=32):  11%|█         | 70000/665298 [00:05<00:17, 34101.58 examples/s]Filter (num_proc=32):  11%|█▏        | 76000/665298 [00:05<00:15, 38371.46 examples/s]Filter (num_proc=32):  12%|█▏        | 80000/665298 [00:05<00:15, 37159.71 examples/s]Filter (num_proc=32):  13%|█▎        | 85000/665298 [00:05<00:16, 35172.01 examples/s]Filter (num_proc=32):  14%|█▍        | 93000/665298 [00:06<00:13, 42488.42 examples/s]Filter (num_proc=32):  15%|█▍        | 98000/665298 [00:06<00:13, 43612.90 examples/s]Filter (num_proc=32):  15%|█▌        | 102791/665298 [00:06<00:13, 43051.27 examples/s]Filter (num_proc=32):  16%|█▌        | 107791/665298 [00:06<00:12, 43954.61 examples/s]Filter (num_proc=32):  17%|█▋        | 112791/665298 [00:06<00:12, 44535.04 examples/s]Filter (num_proc=32):  18%|█▊        | 117791/665298 [00:06<00:12, 43934.50 examples/s]Filter (num_proc=32):  19%|█▉        | 124791/665298 [00:06<00:11, 45383.00 examples/s]Filter (num_proc=32):  20%|█▉        | 132791/665298 [00:06<00:09, 54137.90 examples/s]Filter (num_proc=32):  21%|██        | 138791/665298 [00:06<00:10, 49500.56 examples/s]Filter (num_proc=32):  22%|██▏       | 145791/665298 [00:07<00:09, 54256.27 examples/s]Filter (num_proc=32):  23%|██▎       | 151582/665298 [00:07<00:15, 33528.97 examples/s]Filter (num_proc=32):  27%|██▋       | 176373/665298 [00:07<00:06, 73262.41 examples/s]Filter (num_proc=32):  28%|██▊       | 187373/665298 [00:07<00:07, 63734.99 examples/s]Filter (num_proc=32):  30%|██▉       | 196373/665298 [00:07<00:07, 60686.94 examples/s]Filter (num_proc=32):  31%|███▏      | 209164/665298 [00:08<00:06, 72276.69 examples/s]Filter (num_proc=32):  33%|███▎      | 219164/665298 [00:08<00:05, 77525.59 examples/s]Filter (num_proc=32):  34%|███▍      | 229164/665298 [00:08<00:07, 61105.76 examples/s]Filter (num_proc=32):  36%|███▌      | 237164/665298 [00:08<00:06, 64290.93 examples/s]Filter (num_proc=32):  37%|███▋      | 244955/665298 [00:08<00:07, 55808.85 examples/s]Filter (num_proc=32):  38%|███▊      | 251955/665298 [00:08<00:07, 57438.78 examples/s]Filter (num_proc=32):  40%|████      | 267537/665298 [00:08<00:05, 75201.06 examples/s]Filter (num_proc=32):  42%|████▏     | 276537/665298 [00:09<00:05, 65255.74 examples/s]Filter (num_proc=32):  43%|████▎     | 284537/665298 [00:09<00:06, 54426.20 examples/s]Filter (num_proc=32):  44%|████▍     | 291537/665298 [00:09<00:07, 51882.27 examples/s]Filter (num_proc=32):  45%|████▍     | 298119/665298 [00:09<00:07, 48055.30 examples/s]Filter (num_proc=32):  46%|████▌     | 305910/665298 [00:09<00:07, 47722.50 examples/s]Filter (num_proc=32):  47%|████▋     | 311910/665298 [00:10<00:09, 37461.51 examples/s]Filter (num_proc=32):  48%|████▊     | 316492/665298 [00:10<00:10, 31770.33 examples/s]Filter (num_proc=32):  48%|████▊     | 320074/665298 [00:10<00:13, 25557.91 examples/s]Filter (num_proc=32):  49%|████▊     | 323865/665298 [00:10<00:16, 20199.81 examples/s]Filter (num_proc=32):  49%|████▉     | 327865/665298 [00:11<00:20, 16646.48 examples/s]Filter (num_proc=32):  50%|████▉     | 329865/665298 [00:11<00:20, 16073.78 examples/s]Filter (num_proc=32):  50%|████▉     | 331865/665298 [00:11<00:27, 12255.50 examples/s]Filter (num_proc=32):  50%|█████     | 333865/665298 [00:11<00:26, 12584.83 examples/s]Filter (num_proc=32):  50%|█████     | 335865/665298 [00:12<00:29, 11016.58 examples/s]Filter (num_proc=32):  51%|█████     | 338865/665298 [00:12<00:27, 11831.81 examples/s]Filter (num_proc=32):  51%|█████     | 340865/665298 [00:12<00:28, 11566.80 examples/s]Filter (num_proc=32):  52%|█████▏    | 342865/665298 [00:12<00:27, 11696.55 examples/s]Filter (num_proc=32):  52%|█████▏    | 344865/665298 [00:12<00:26, 12125.00 examples/s]Filter (num_proc=32):  52%|█████▏    | 347865/665298 [00:13<00:25, 12589.93 examples/s]Filter (num_proc=32):  53%|█████▎    | 349865/665298 [00:13<00:29, 10832.89 examples/s]Filter (num_proc=32):  53%|█████▎    | 351865/665298 [00:13<00:27, 11551.44 examples/s]Filter (num_proc=32):  53%|█████▎    | 353865/665298 [00:13<00:31, 9934.82 examples/s] Filter (num_proc=32):  53%|█████▎    | 355865/665298 [00:13<00:28, 10730.78 examples/s]Filter (num_proc=32):  54%|█████▍    | 358865/665298 [00:14<00:25, 11993.29 examples/s]Filter (num_proc=32):  54%|█████▍    | 360865/665298 [00:14<00:27, 11249.36 examples/s]Filter (num_proc=32):  55%|█████▍    | 362865/665298 [00:14<00:28, 10508.28 examples/s]Filter (num_proc=32):  55%|█████▍    | 365865/665298 [00:14<00:24, 12320.48 examples/s]Filter (num_proc=32):  55%|█████▌    | 368865/665298 [00:14<00:23, 12714.78 examples/s]Filter (num_proc=32):  56%|█████▌    | 370865/665298 [00:15<00:24, 11936.32 examples/s]Filter (num_proc=32):  56%|█████▌    | 373865/665298 [00:15<00:21, 13344.49 examples/s]Filter (num_proc=32):  57%|█████▋    | 376865/665298 [00:15<00:21, 13586.61 examples/s]Filter (num_proc=32):  57%|█████▋    | 378865/665298 [00:15<00:23, 12355.21 examples/s]Filter (num_proc=32):  57%|█████▋    | 381656/665298 [00:15<00:19, 14609.71 examples/s]Filter (num_proc=32):  58%|█████▊    | 383656/665298 [00:15<00:18, 15089.13 examples/s]Filter (num_proc=32):  58%|█████▊    | 385656/665298 [00:16<00:18, 14792.72 examples/s]Filter (num_proc=32):  58%|█████▊    | 387656/665298 [00:16<00:22, 12612.98 examples/s]Filter (num_proc=32):  59%|█████▊    | 390656/665298 [00:16<00:19, 14022.04 examples/s]Filter (num_proc=32):  59%|█████▉    | 394656/665298 [00:16<00:16, 16483.20 examples/s]Filter (num_proc=32):  60%|█████▉    | 397656/665298 [00:16<00:15, 17603.79 examples/s]Filter (num_proc=32):  61%|██████    | 402656/665298 [00:16<00:11, 23107.54 examples/s]Filter (num_proc=32):  61%|██████    | 405656/665298 [00:17<00:13, 19636.92 examples/s]Filter (num_proc=32):  62%|██████▏   | 409656/665298 [00:17<00:12, 20556.09 examples/s]Filter (num_proc=32):  62%|██████▏   | 412656/665298 [00:17<00:12, 19648.79 examples/s]Filter (num_proc=32):  63%|██████▎   | 417447/665298 [00:17<00:10, 24412.98 examples/s]Filter (num_proc=32):  63%|██████▎   | 420447/665298 [00:17<00:10, 24084.73 examples/s]Filter (num_proc=32):  64%|██████▍   | 425447/665298 [00:18<00:12, 18565.33 examples/s]Filter (num_proc=32):  65%|██████▌   | 434447/665298 [00:18<00:07, 30266.31 examples/s]Filter (num_proc=32):  66%|██████▌   | 439447/665298 [00:18<00:07, 30776.22 examples/s]Filter (num_proc=32):  67%|██████▋   | 444447/665298 [00:18<00:06, 34142.80 examples/s]Filter (num_proc=32):  68%|██████▊   | 449447/665298 [00:18<00:06, 33879.66 examples/s]Filter (num_proc=32):  68%|██████▊   | 454447/665298 [00:18<00:06, 34746.74 examples/s]Filter (num_proc=32):  69%|██████▉   | 459238/665298 [00:18<00:05, 37565.34 examples/s]Filter (num_proc=32):  70%|██████▉   | 464238/665298 [00:19<00:05, 35782.74 examples/s]Filter (num_proc=32):  71%|███████   | 469238/665298 [00:19<00:05, 37963.88 examples/s]Filter (num_proc=32):  72%|███████▏  | 476238/665298 [00:19<00:04, 45445.45 examples/s]Filter (num_proc=32):  72%|███████▏  | 481238/665298 [00:19<00:04, 38107.58 examples/s]Filter (num_proc=32):  73%|███████▎  | 486238/665298 [00:19<00:04, 38778.02 examples/s]Filter (num_proc=32):  74%|███████▍  | 492238/665298 [00:19<00:04, 42648.19 examples/s]Filter (num_proc=32):  75%|███████▍  | 497238/665298 [00:19<00:03, 43207.55 examples/s]Filter (num_proc=32):  75%|███████▌  | 502238/665298 [00:19<00:03, 44132.62 examples/s]Filter (num_proc=32):  76%|███████▌  | 507238/665298 [00:20<00:04, 36889.96 examples/s]Filter (num_proc=32):  77%|███████▋  | 513238/665298 [00:20<00:03, 40709.48 examples/s]Filter (num_proc=32):  78%|███████▊  | 518238/665298 [00:20<00:03, 40479.13 examples/s]Filter (num_proc=32):  79%|███████▊  | 523238/665298 [00:20<00:03, 41860.67 examples/s]Filter (num_proc=32):  79%|███████▉  | 528238/665298 [00:20<00:03, 42329.54 examples/s]Filter (num_proc=32):  80%|████████  | 533028/665298 [00:20<00:03, 38795.48 examples/s]Filter (num_proc=32):  81%|████████  | 539818/665298 [00:20<00:02, 41861.81 examples/s]Filter (num_proc=32):  82%|████████▏ | 544818/665298 [00:21<00:03, 34489.03 examples/s]Filter (num_proc=32):  83%|████████▎ | 550818/665298 [00:21<00:03, 35051.62 examples/s]Filter (num_proc=32):  83%|████████▎ | 554818/665298 [00:21<00:03, 34344.12 examples/s]Filter (num_proc=32):  84%|████████▍ | 558818/665298 [00:21<00:03, 32700.78 examples/s]Filter (num_proc=32):  85%|████████▍ | 562818/665298 [00:21<00:03, 28698.04 examples/s]Filter (num_proc=32):  86%|████████▌ | 570608/665298 [00:21<00:02, 32421.07 examples/s]Filter (num_proc=32):  86%|████████▋ | 574608/665298 [00:21<00:02, 33484.38 examples/s]Filter (num_proc=32):  87%|████████▋ | 578398/665298 [00:22<00:03, 27840.02 examples/s]Filter (num_proc=32):  88%|████████▊ | 583188/665298 [00:22<00:02, 29473.19 examples/s]Filter (num_proc=32):  88%|████████▊ | 586978/665298 [00:22<00:03, 24582.02 examples/s]Filter (num_proc=32):  89%|████████▊ | 589978/665298 [00:22<00:03, 22453.34 examples/s]Filter (num_proc=32):  89%|████████▉ | 592978/665298 [00:23<00:04, 17712.76 examples/s]Filter (num_proc=32):  89%|████████▉ | 594978/665298 [00:23<00:04, 17058.02 examples/s]Filter (num_proc=32):  90%|████████▉ | 597978/665298 [00:23<00:04, 15744.16 examples/s]Filter (num_proc=32):  90%|█████████ | 600978/665298 [00:23<00:04, 14002.34 examples/s]Filter (num_proc=32):  91%|█████████ | 602978/665298 [00:23<00:04, 14899.58 examples/s]Filter (num_proc=32):  91%|█████████ | 604978/665298 [00:23<00:03, 15481.43 examples/s]Filter (num_proc=32):  91%|█████████ | 606768/665298 [00:24<00:04, 14165.73 examples/s]Filter (num_proc=32):  92%|█████████▏| 608768/665298 [00:24<00:04, 13857.75 examples/s]Filter (num_proc=32):  92%|█████████▏| 610768/665298 [00:24<00:04, 13070.59 examples/s]Filter (num_proc=32):  92%|█████████▏| 612768/665298 [00:24<00:04, 13079.65 examples/s]Filter (num_proc=32):  92%|█████████▏| 614768/665298 [00:24<00:03, 14042.53 examples/s]Filter (num_proc=32):  93%|█████████▎| 616558/665298 [00:24<00:04, 9964.83 examples/s] Filter (num_proc=32):  93%|█████████▎| 619558/665298 [00:25<00:03, 12536.54 examples/s]Filter (num_proc=32):  93%|█████████▎| 621558/665298 [00:25<00:03, 13240.74 examples/s]Filter (num_proc=32):  94%|█████████▎| 623558/665298 [00:25<00:04, 8726.19 examples/s] Filter (num_proc=32):  94%|█████████▍| 626348/665298 [00:25<00:03, 11323.21 examples/s]Filter (num_proc=32):  94%|█████████▍| 628348/665298 [00:25<00:03, 11704.24 examples/s]Filter (num_proc=32):  95%|█████████▍| 630138/665298 [00:26<00:04, 8736.68 examples/s] Filter (num_proc=32):  95%|█████████▌| 632138/665298 [00:26<00:03, 9838.89 examples/s]Filter (num_proc=32):  95%|█████████▌| 634138/665298 [00:26<00:04, 6726.53 examples/s]Filter (num_proc=32):  96%|█████████▌| 636138/665298 [00:27<00:03, 8104.47 examples/s]Filter (num_proc=32):  96%|█████████▌| 638138/665298 [00:27<00:04, 6117.34 examples/s]Filter (num_proc=32):  96%|█████████▌| 640138/665298 [00:27<00:03, 7320.90 examples/s]Filter (num_proc=32):  96%|█████████▋| 641928/665298 [00:28<00:04, 5699.62 examples/s]Filter (num_proc=32):  97%|█████████▋| 643928/665298 [00:28<00:03, 6846.51 examples/s]Filter (num_proc=32):  97%|█████████▋| 644928/665298 [00:28<00:04, 4925.05 examples/s]Filter (num_proc=32):  97%|█████████▋| 646928/665298 [00:29<00:03, 5967.05 examples/s]Filter (num_proc=32):  98%|█████████▊| 648718/665298 [00:29<00:03, 5031.45 examples/s]Filter (num_proc=32):  98%|█████████▊| 649718/665298 [00:29<00:02, 5212.14 examples/s]Filter (num_proc=32):  98%|█████████▊| 650718/665298 [00:30<00:03, 3875.37 examples/s]Filter (num_proc=32):  98%|█████████▊| 651718/665298 [00:30<00:03, 4270.09 examples/s]Filter (num_proc=32):  98%|█████████▊| 652718/665298 [00:30<00:03, 3346.99 examples/s]Filter (num_proc=32):  98%|█████████▊| 653718/665298 [00:31<00:03, 3617.59 examples/s]Filter (num_proc=32):  98%|█████████▊| 654718/665298 [00:31<00:03, 3117.80 examples/s]Filter (num_proc=32):  99%|█████████▊| 655718/665298 [00:31<00:02, 3298.43 examples/s]Filter (num_proc=32):  99%|█████████▊| 656718/665298 [00:32<00:02, 2996.78 examples/s]Filter (num_proc=32):  99%|█████████▉| 657718/665298 [00:32<00:02, 3299.64 examples/s]Filter (num_proc=32):  99%|█████████▉| 658718/665298 [00:32<00:02, 2986.25 examples/s]Filter (num_proc=32):  99%|█████████▉| 659718/665298 [00:33<00:01, 3233.08 examples/s]Filter (num_proc=32):  99%|█████████▉| 660718/665298 [00:33<00:01, 2961.38 examples/s]Filter (num_proc=32):  99%|█████████▉| 661718/665298 [00:33<00:01, 3291.61 examples/s]Filter (num_proc=32): 100%|█████████▉| 662508/665298 [00:33<00:00, 3151.38 examples/s]Filter (num_proc=32): 100%|█████████▉| 663508/665298 [00:34<00:00, 3040.75 examples/s]Filter (num_proc=32): 100%|█████████▉| 664508/665298 [00:34<00:00, 2339.20 examples/s]Filter (num_proc=32): 100%|██████████| 665298/665298 [00:35<00:00, 2060.48 examples/s]Filter (num_proc=32): 100%|██████████| 665298/665298 [00:35<00:00, 18631.65 examples/s]
Map (num_proc=32):   0%|          | 0/665252 [00:00<?, ? examples/s]Map (num_proc=32):   0%|          | 220/665252 [00:28<23:43:43,  7.79 examples/s]Map (num_proc=32):   0%|          | 855/665252 [00:28<4:39:39, 39.60 examples/s] Map (num_proc=32):   0%|          | 1413/665252 [00:28<2:21:06, 78.41 examples/s]Map (num_proc=32):   0%|          | 2000/665252 [00:28<1:21:57, 134.87 examples/s]Map (num_proc=32):   0%|          | 2412/665252 [00:28<58:04, 190.20 examples/s]  Map (num_proc=32):   0%|          | 2845/665252 [00:29<40:45, 270.85 examples/s]Map (num_proc=32):   1%|          | 3426/665252 [00:29<26:54, 409.96 examples/s]Map (num_proc=32):   1%|          | 3842/665252 [00:29<20:11, 545.93 examples/s]Map (num_proc=32):   1%|          | 4408/665252 [00:29<14:36, 754.36 examples/s]Map (num_proc=32):   1%|          | 4825/665252 [00:29<11:22, 967.46 examples/s]Map (num_proc=32):   1%|          | 5212/665252 [00:29<09:33, 1151.25 examples/s]Map (num_proc=32):   1%|          | 5642/665252 [00:29<07:30, 1465.47 examples/s]Map (num_proc=32):   1%|          | 6000/665252 [00:30<06:42, 1639.04 examples/s]Map (num_proc=32):   1%|          | 6428/665252 [00:30<05:26, 2018.82 examples/s]Map (num_proc=32):   1%|          | 7000/665252 [00:30<04:55, 2230.48 examples/s]Map (num_proc=32):   1%|          | 7651/665252 [00:30<03:57, 2765.83 examples/s]Map (num_proc=32):   1%|          | 8206/665252 [00:30<03:53, 2808.02 examples/s]Map (num_proc=32):   1%|▏         | 8813/665252 [00:30<03:28, 3154.93 examples/s]Map (num_proc=32):   1%|▏         | 9209/665252 [00:30<03:37, 3018.71 examples/s]Map (num_proc=32):   1%|▏         | 9647/665252 [00:31<03:19, 3292.94 examples/s]Map (num_proc=32):   2%|▏         | 10211/665252 [00:31<03:24, 3204.64 examples/s]Map (num_proc=32):   2%|▏         | 10612/665252 [00:31<03:13, 3375.00 examples/s]Map (num_proc=32):   2%|▏         | 11000/665252 [00:31<03:30, 3103.95 examples/s]Map (num_proc=32):   2%|▏         | 11420/665252 [00:31<03:14, 3353.17 examples/s]Map (num_proc=32):   2%|▏         | 11856/665252 [00:31<03:01, 3599.68 examples/s]Map (num_proc=32):   2%|▏         | 12421/665252 [00:31<03:20, 3251.04 examples/s]Map (num_proc=32):   2%|▏         | 12843/665252 [00:32<03:08, 3465.81 examples/s]Map (num_proc=32):   2%|▏         | 13426/665252 [00:32<03:19, 3263.39 examples/s]Map (num_proc=32):   2%|▏         | 13836/665252 [00:32<04:18, 2516.92 examples/s]Map (num_proc=32):   2%|▏         | 14209/665252 [00:32<04:19, 2506.06 examples/s]Map (num_proc=32):   2%|▏         | 14639/665252 [00:32<03:47, 2855.86 examples/s]Map (num_proc=32):   2%|▏         | 15000/665252 [00:32<03:57, 2736.99 examples/s]Map (num_proc=32):   2%|▏         | 15438/665252 [00:32<03:29, 3096.05 examples/s]Map (num_proc=32):   2%|▏         | 15862/665252 [00:33<03:12, 3366.99 examples/s]Map (num_proc=32):   2%|▏         | 16423/665252 [00:33<03:51, 2798.59 examples/s]Map (num_proc=32):   3%|▎         | 16831/665252 [00:33<03:31, 3060.44 examples/s]Map (num_proc=32):   3%|▎         | 17222/665252 [00:33<03:42, 2916.93 examples/s]Map (num_proc=32):   3%|▎         | 17675/665252 [00:33<03:17, 3278.64 examples/s]Map (num_proc=32):   3%|▎         | 18218/665252 [00:33<03:24, 3160.49 examples/s]Map (num_proc=32):   3%|▎         | 18863/665252 [00:34<03:04, 3504.65 examples/s]Map (num_proc=32):   3%|▎         | 19428/665252 [00:34<03:13, 3331.72 examples/s]Map (num_proc=32):   3%|▎         | 19856/665252 [00:34<03:02, 3530.21 examples/s]Map (num_proc=32):   3%|▎         | 20440/665252 [00:34<03:10, 3380.78 examples/s]Map (num_proc=32):   3%|▎         | 21000/665252 [00:45<1:08:47, 156.09 examples/s]Map (num_proc=32):   3%|▎         | 21621/665252 [00:45<46:47, 229.29 examples/s]  Map (num_proc=32):   3%|▎         | 22008/665252 [00:45<37:04, 289.13 examples/s]Map (num_proc=32):   3%|▎         | 22436/665252 [00:46<27:54, 383.84 examples/s]Map (num_proc=32):   3%|▎         | 22999/665252 [00:46<19:52, 538.73 examples/s]Map (num_proc=32):   4%|▎         | 23396/665252 [00:46<15:33, 687.70 examples/s]Map (num_proc=32):   4%|▎         | 23790/665252 [00:46<12:31, 853.41 examples/s]Map (num_proc=32):   4%|▎         | 24441/665252 [00:46<08:40, 1231.57 examples/s]Map (num_proc=32):   4%|▍         | 24991/665252 [00:46<07:12, 1481.95 examples/s]Map (num_proc=32):   4%|▍         | 25397/665252 [00:46<06:03, 1760.88 examples/s]Map (num_proc=32):   4%|▍         | 25790/665252 [00:47<05:32, 1924.65 examples/s]Map (num_proc=32):   4%|▍         | 26419/665252 [00:47<04:25, 2405.91 examples/s]Map (num_proc=32):   4%|▍         | 26790/665252 [00:47<04:21, 2442.23 examples/s]Map (num_proc=32):   4%|▍         | 27417/665252 [00:47<03:38, 2912.64 examples/s]Map (num_proc=32):   4%|▍         | 27790/665252 [00:47<03:48, 2785.62 examples/s]Map (num_proc=32):   4%|▍         | 28425/665252 [00:47<03:20, 3182.78 examples/s]Map (num_proc=32):   4%|▍         | 28790/665252 [00:48<04:32, 2339.05 examples/s]Map (num_proc=32):   4%|▍         | 29790/665252 [00:48<03:05, 3420.13 examples/s]Map (num_proc=32):   5%|▍         | 30428/665252 [00:48<02:52, 3671.12 examples/s]Map (num_proc=32):   5%|▍         | 31003/665252 [00:48<03:03, 3452.89 examples/s]Map (num_proc=32):   5%|▍         | 31662/665252 [00:48<02:49, 3734.43 examples/s]Map (num_proc=32):   5%|▍         | 32215/665252 [00:48<03:02, 3463.90 examples/s]Map (num_proc=32):   5%|▍         | 32657/665252 [00:49<02:53, 3651.66 examples/s]Map (num_proc=32):   5%|▍         | 33216/665252 [00:49<03:05, 3415.33 examples/s]Map (num_proc=32):   5%|▌         | 33790/665252 [00:49<03:08, 3347.04 examples/s]Map (num_proc=32):   5%|▌         | 34215/665252 [00:49<03:02, 3459.90 examples/s]Map (num_proc=32):   5%|▌         | 34784/665252 [00:49<03:45, 2795.30 examples/s]Map (num_proc=32):   5%|▌         | 35222/665252 [00:49<03:48, 2756.75 examples/s]Map (num_proc=32):   5%|▌         | 35790/665252 [00:50<03:42, 2826.62 examples/s]Map (num_proc=32):   5%|▌         | 36439/665252 [00:50<03:15, 3210.17 examples/s]Map (num_proc=32):   6%|▌         | 36790/665252 [00:50<03:54, 2682.06 examples/s]Map (num_proc=32):   6%|▌         | 37443/665252 [00:50<03:22, 3094.08 examples/s]Map (num_proc=32):   6%|▌         | 37790/665252 [00:50<03:53, 2687.87 examples/s]Map (num_proc=32):   6%|▌         | 38595/665252 [00:50<02:59, 3482.55 examples/s]Map (num_proc=32):   6%|▌         | 39001/665252 [00:51<03:14, 3221.44 examples/s]Map (num_proc=32):   6%|▌         | 39440/665252 [00:51<03:00, 3462.87 examples/s]Map (num_proc=32):   6%|▌         | 39995/665252 [00:51<03:09, 3295.27 examples/s]Map (num_proc=32):   6%|▌         | 40446/665252 [00:51<02:55, 3553.28 examples/s]Map (num_proc=32):   6%|▌         | 41003/665252 [00:51<03:05, 3367.68 examples/s]Map (num_proc=32):   6%|▌         | 41441/665252 [00:51<02:53, 3587.13 examples/s]Map (num_proc=32):   6%|▋         | 41995/665252 [01:06<1:29:11, 116.47 examples/s]Map (num_proc=32):   6%|▋         | 42580/665252 [01:06<1:00:57, 170.26 examples/s]Map (num_proc=32):   6%|▋         | 43195/665252 [01:06<41:36, 249.20 examples/s]  Map (num_proc=32):   7%|▋         | 43580/665252 [01:06<33:01, 313.75 examples/s]Map (num_proc=32):   7%|▋         | 44226/665252 [01:06<22:10, 466.82 examples/s]Map (num_proc=32):   7%|▋         | 44789/665252 [01:06<16:28, 627.62 examples/s]Map (num_proc=32):   7%|▋         | 45227/665252 [01:07<12:53, 801.74 examples/s]Map (num_proc=32):   7%|▋         | 45784/665252 [01:07<09:58, 1034.77 examples/s]Map (num_proc=32):   7%|▋         | 46198/665252 [01:07<08:05, 1274.32 examples/s]Map (num_proc=32):   7%|▋         | 46580/665252 [01:07<07:03, 1461.29 examples/s]Map (num_proc=32):   7%|▋         | 47012/665252 [01:07<05:45, 1791.89 examples/s]Map (num_proc=32):   7%|▋         | 47443/665252 [01:07<04:46, 2156.27 examples/s]Map (num_proc=32):   7%|▋         | 48000/665252 [01:07<04:17, 2393.66 examples/s]Map (num_proc=32):   7%|▋         | 48580/665252 [01:08<04:00, 2559.94 examples/s]Map (num_proc=32):   7%|▋         | 49017/665252 [01:08<03:34, 2879.23 examples/s]Map (num_proc=32):   7%|▋         | 49445/665252 [01:08<03:14, 3160.71 examples/s]Map (num_proc=32):   8%|▊         | 49997/665252 [01:08<03:18, 3092.82 examples/s]Map (num_proc=32):   8%|▊         | 50442/665252 [01:08<03:01, 3378.76 examples/s]Map (num_proc=32):   8%|▊         | 51019/665252 [01:08<03:08, 3251.32 examples/s]Map (num_proc=32):   8%|▊         | 51580/665252 [01:08<03:12, 3192.73 examples/s]Map (num_proc=32):   8%|▊         | 52009/665252 [01:09<02:59, 3419.26 examples/s]Map (num_proc=32):   8%|▊         | 52455/665252 [01:09<02:47, 3652.89 examples/s]Map (num_proc=32):   8%|▊         | 53007/665252 [01:09<02:59, 3403.77 examples/s]Map (num_proc=32):   8%|▊         | 53398/665252 [01:09<05:13, 1950.85 examples/s]Map (num_proc=32):   8%|▊         | 55003/665252 [01:09<02:37, 3886.75 examples/s]Map (num_proc=32):   8%|▊         | 55580/665252 [01:10<03:26, 2945.89 examples/s]Map (num_proc=32):   8%|▊         | 56228/665252 [01:10<03:08, 3222.36 examples/s]Map (num_proc=32):   9%|▊         | 56790/665252 [01:10<03:12, 3165.82 examples/s]Map (num_proc=32):   9%|▊         | 57438/665252 [01:10<03:08, 3223.09 examples/s]Map (num_proc=32):   9%|▊         | 57865/665252 [01:10<03:22, 3000.75 examples/s]Map (num_proc=32):   9%|▉         | 58580/665252 [01:11<02:50, 3562.60 examples/s]Map (num_proc=32):   9%|▉         | 59580/665252 [01:11<02:22, 4262.33 examples/s]Map (num_proc=32):   9%|▉         | 60337/665252 [01:11<02:14, 4489.33 examples/s]Map (num_proc=32):   9%|▉         | 61322/665252 [01:11<01:52, 5392.03 examples/s]Map (num_proc=32):   9%|▉         | 61945/665252 [01:11<01:52, 5373.17 examples/s]Map (num_proc=32):   9%|▉         | 62722/665252 [01:27<1:03:22, 158.46 examples/s]Map (num_proc=32):  10%|▉         | 63370/665252 [01:27<46:57, 213.59 examples/s]  Map (num_proc=32):  10%|▉         | 64046/665252 [01:27<33:59, 294.73 examples/s]Map (num_proc=32):  10%|▉         | 64727/665252 [01:27<24:40, 405.54 examples/s]Map (num_proc=32):  10%|▉         | 65370/665252 [01:27<18:17, 546.62 examples/s]Map (num_proc=32):  10%|▉         | 66370/665252 [01:28<11:52, 840.27 examples/s]Map (num_proc=32):  10%|█         | 67370/665252 [01:28<08:16, 1203.63 examples/s]Map (num_proc=32):  10%|█         | 68370/665252 [01:28<06:05, 1633.61 examples/s]Map (num_proc=32):  10%|█         | 69370/665252 [01:28<04:38, 2143.22 examples/s]Map (num_proc=32):  11%|█         | 70104/665252 [01:29<05:00, 1983.47 examples/s]Map (num_proc=32):  11%|█         | 72723/665252 [01:29<02:27, 4005.95 examples/s]Map (num_proc=32):  11%|█         | 73711/665252 [01:29<02:16, 4333.25 examples/s]Map (num_proc=32):  11%|█         | 74729/665252 [01:29<02:07, 4632.70 examples/s]Map (num_proc=32):  11%|█▏        | 75730/665252 [01:29<01:59, 4918.13 examples/s]Map (num_proc=32):  11%|█▏        | 76430/665252 [01:29<02:26, 4022.72 examples/s]Map (num_proc=32):  12%|█▏        | 77370/665252 [01:30<02:13, 4415.79 examples/s]Map (num_proc=32):  12%|█▏        | 78370/665252 [01:30<02:02, 4777.00 examples/s]Map (num_proc=32):  12%|█▏        | 79370/665252 [01:30<01:56, 5018.32 examples/s]Map (num_proc=32):  12%|█▏        | 80370/665252 [01:30<01:50, 5275.05 examples/s]Map (num_proc=32):  12%|█▏        | 81105/665252 [01:30<01:43, 5658.04 examples/s]Map (num_proc=32):  12%|█▏        | 82062/665252 [01:30<01:44, 5585.08 examples/s]Map (num_proc=32):  12%|█▏        | 82711/665252 [01:31<01:48, 5344.45 examples/s]Map (num_proc=32):  13%|█▎        | 83160/665252 [01:47<01:48, 5344.45 examples/s]Map (num_proc=32):  13%|█▎        | 83483/665252 [01:49<1:06:58, 144.76 examples/s]Map (num_proc=32):  13%|█▎        | 84134/665252 [01:49<50:21, 192.33 examples/s]  Map (num_proc=32):  13%|█▎        | 84841/665252 [01:49<36:41, 263.63 examples/s]Map (num_proc=32):  13%|█▎        | 85473/665252 [01:50<27:29, 351.42 examples/s]Map (num_proc=32):  13%|█▎        | 86224/665252 [01:50<19:32, 493.78 examples/s]Map (num_proc=32):  13%|█▎        | 87160/665252 [01:50<13:12, 729.79 examples/s]Map (num_proc=32):  13%|█▎        | 87829/665252 [01:50<10:04, 955.01 examples/s]Map (num_proc=32):  13%|█▎        | 88488/665252 [01:50<07:53, 1217.37 examples/s]Map (num_proc=32):  13%|█▎        | 89160/665252 [01:50<06:11, 1549.26 examples/s]Map (num_proc=32):  14%|█▎        | 89817/665252 [01:50<04:50, 1978.42 examples/s]Map (num_proc=32):  14%|█▎        | 90490/665252 [01:51<03:59, 2397.83 examples/s]Map (num_proc=32):  14%|█▎        | 91160/665252 [01:51<03:23, 2821.94 examples/s]Map (num_proc=32):  14%|█▍        | 92160/665252 [01:51<02:48, 3403.93 examples/s]Map (num_proc=32):  14%|█▍        | 92830/665252 [01:51<02:27, 3887.68 examples/s]Map (num_proc=32):  14%|█▍        | 93485/665252 [01:51<02:19, 4101.75 examples/s]Map (num_proc=32):  14%|█▍        | 94160/665252 [01:51<02:16, 4185.11 examples/s]Map (num_proc=32):  14%|█▍        | 95160/665252 [01:51<02:02, 4659.63 examples/s]Map (num_proc=32):  14%|█▍        | 95840/665252 [01:52<01:51, 5087.22 examples/s]Map (num_proc=32):  15%|█▍        | 96492/665252 [01:52<01:54, 4946.01 examples/s]Map (num_proc=32):  15%|█▍        | 97123/665252 [01:52<02:43, 3474.21 examples/s]Map (num_proc=32):  15%|█▍        | 97866/665252 [01:52<02:28, 3830.29 examples/s]Map (num_proc=32):  15%|█▍        | 98479/665252 [01:52<02:20, 4021.05 examples/s]Map (num_proc=32):  15%|█▍        | 99160/665252 [01:52<02:13, 4255.63 examples/s]Map (num_proc=32):  15%|█▌        | 99843/665252 [01:52<01:57, 4795.55 examples/s]Map (num_proc=32):  15%|█▌        | 100494/665252 [01:53<02:00, 4668.67 examples/s]Map (num_proc=32):  15%|█▌        | 101160/665252 [01:53<01:57, 4800.26 examples/s]Map (num_proc=32):  15%|█▌        | 102160/665252 [01:53<01:52, 5022.58 examples/s]Map (num_proc=32):  15%|█▌        | 102831/665252 [01:53<01:44, 5390.93 examples/s]Map (num_proc=32):  16%|█▌        | 103498/665252 [01:53<01:47, 5213.33 examples/s]Map (num_proc=32):  16%|█▌        | 103949/665252 [02:07<01:47, 5213.33 examples/s]Map (num_proc=32):  16%|█▌        | 104270/665252 [02:11<1:08:05, 137.32 examples/s]Map (num_proc=32):  16%|█▌        | 104968/665252 [02:11<48:53, 191.00 examples/s]  Map (num_proc=32):  16%|█▌        | 105621/665252 [02:11<35:36, 261.92 examples/s]Map (num_proc=32):  16%|█▌        | 106281/665252 [02:11<25:53, 359.78 examples/s]Map (num_proc=32):  16%|█▌        | 106949/665252 [02:11<18:49, 494.47 examples/s]Map (num_proc=32):  16%|█▌        | 107949/665252 [02:11<12:11, 762.20 examples/s]Map (num_proc=32):  16%|█▋        | 108615/665252 [02:11<09:16, 999.72 examples/s]Map (num_proc=32):  16%|█▋        | 109280/665252 [02:12<07:14, 1278.50 examples/s]Map (num_proc=32):  17%|█▋        | 109949/665252 [02:12<05:42, 1623.44 examples/s]Map (num_proc=32):  17%|█▋        | 110627/665252 [02:12<04:25, 2086.28 examples/s]Map (num_proc=32):  17%|█▋        | 111284/665252 [02:12<03:40, 2510.71 examples/s]Map (num_proc=32):  17%|█▋        | 111949/665252 [02:12<03:07, 2943.46 examples/s]Map (num_proc=32):  17%|█▋        | 112635/665252 [02:12<02:35, 3555.86 examples/s]Map (num_proc=32):  17%|█▋        | 113278/665252 [02:12<02:27, 3752.45 examples/s]Map (num_proc=32):  17%|█▋        | 113949/665252 [02:13<02:14, 4089.72 examples/s]Map (num_proc=32):  17%|█▋        | 114621/665252 [02:13<01:58, 4630.76 examples/s]Map (num_proc=32):  17%|█▋        | 115280/665252 [02:13<01:56, 4730.84 examples/s]Map (num_proc=32):  17%|█▋        | 115949/665252 [02:13<01:54, 4812.95 examples/s]Map (num_proc=32):  18%|█▊        | 116640/665252 [02:13<01:43, 5303.02 examples/s]Map (num_proc=32):  18%|█▊        | 117281/665252 [02:13<01:45, 5201.50 examples/s]Map (num_proc=32):  18%|█▊        | 117910/665252 [02:13<02:15, 4041.65 examples/s]Map (num_proc=32):  18%|█▊        | 118679/665252 [02:13<02:05, 4346.93 examples/s]Map (num_proc=32):  18%|█▊        | 119288/665252 [02:14<02:02, 4441.12 examples/s]Map (num_proc=32):  18%|█▊        | 119949/665252 [02:14<01:58, 4590.47 examples/s]Map (num_proc=32):  18%|█▊        | 120949/665252 [02:14<01:51, 4875.68 examples/s]Map (num_proc=32):  18%|█▊        | 121638/665252 [02:14<01:42, 5289.18 examples/s]Map (num_proc=32):  18%|█▊        | 122287/665252 [02:14<01:43, 5234.11 examples/s]Map (num_proc=32):  18%|█▊        | 122949/665252 [02:14<01:46, 5110.84 examples/s]Map (num_proc=32):  19%|█▊        | 123638/665252 [02:15<03:00, 2994.26 examples/s]Map (num_proc=32):  19%|█▉        | 124738/665252 [02:27<03:00, 2994.26 examples/s]Map (num_proc=32):  19%|█▉        | 125067/665252 [02:35<1:00:17, 149.33 examples/s]Map (num_proc=32):  19%|█▉        | 125753/665252 [02:35<46:02, 195.26 examples/s]  Map (num_proc=32):  19%|█▉        | 126415/665252 [02:35<34:47, 258.14 examples/s]Map (num_proc=32):  19%|█▉        | 127060/665252 [02:35<27:03, 331.57 examples/s]Map (num_proc=32):  19%|█▉        | 127738/665252 [02:35<19:56, 449.38 examples/s]Map (num_proc=32):  19%|█▉        | 128400/665252 [02:36<14:43, 607.90 examples/s]Map (num_proc=32):  19%|█▉        | 129058/665252 [02:36<11:01, 810.00 examples/s]Map (num_proc=32):  20%|█▉        | 129769/665252 [02:36<08:12, 1087.67 examples/s]Map (num_proc=32):  20%|█▉        | 130434/665252 [02:36<06:13, 1431.31 examples/s]Map (num_proc=32):  20%|█▉        | 131063/665252 [02:36<04:59, 1785.45 examples/s]Map (num_proc=32):  20%|█▉        | 131796/665252 [02:36<03:57, 2250.39 examples/s]Map (num_proc=32):  20%|█▉        | 132482/665252 [02:36<03:10, 2798.89 examples/s]Map (num_proc=32):  20%|██        | 133064/665252 [02:36<02:49, 3137.27 examples/s]Map (num_proc=32):  20%|██        | 133738/665252 [02:37<02:33, 3458.12 examples/s]Map (num_proc=32):  20%|██        | 134388/665252 [02:37<02:12, 4011.04 examples/s]Map (num_proc=32):  20%|██        | 135122/665252 [02:37<02:04, 4250.37 examples/s]Map (num_proc=32):  20%|██        | 135738/665252 [02:37<01:59, 4428.74 examples/s]Map (num_proc=32):  21%|██        | 136416/665252 [02:37<01:46, 4952.19 examples/s]Map (num_proc=32):  21%|██        | 137058/665252 [02:37<01:47, 4923.42 examples/s]Map (num_proc=32):  21%|██        | 137738/665252 [02:37<01:46, 4959.37 examples/s]Map (num_proc=32):  21%|██        | 138353/665252 [02:38<02:19, 3770.79 examples/s]Map (num_proc=32):  21%|██        | 139091/665252 [02:38<02:04, 4228.27 examples/s]Map (num_proc=32):  21%|██        | 139738/665252 [02:38<02:00, 4347.06 examples/s]Map (num_proc=32):  21%|██        | 140420/665252 [02:38<01:53, 4606.01 examples/s]Map (num_proc=32):  21%|██        | 141068/665252 [02:38<01:57, 4468.78 examples/s]Map (num_proc=32):  21%|██▏       | 142073/665252 [02:38<01:33, 5586.13 examples/s]Map (num_proc=32):  21%|██▏       | 142738/665252 [02:38<01:36, 5429.93 examples/s]Map (num_proc=32):  22%|██▏       | 143738/665252 [02:39<01:35, 5434.71 examples/s]Map (num_proc=32):  22%|██▏       | 144424/665252 [02:39<01:30, 5749.84 examples/s]Map (num_proc=32):  22%|██▏       | 145079/665252 [02:39<01:34, 5507.99 examples/s]Map (num_proc=32):  22%|██▏       | 145854/665252 [02:55<55:54, 154.82 examples/s] Map (num_proc=32):  22%|██▏       | 146556/665252 [02:55<40:22, 214.10 examples/s]Map (num_proc=32):  22%|██▏       | 147231/665252 [02:55<29:18, 294.58 examples/s]Map (num_proc=32):  22%|██▏       | 147859/665252 [02:55<21:46, 395.99 examples/s]Map (num_proc=32):  22%|██▏       | 148527/665252 [02:55<15:51, 543.29 examples/s]Map (num_proc=32):  22%|██▏       | 149198/665252 [02:55<11:32, 744.80 examples/s]Map (num_proc=32):  23%|██▎       | 149848/665252 [02:55<08:41, 988.72 examples/s]Map (num_proc=32):  23%|██▎       | 150527/665252 [02:55<06:35, 1300.40 examples/s]Map (num_proc=32):  23%|██▎       | 151527/665252 [02:56<04:37, 1847.97 examples/s]Map (num_proc=32):  23%|██▎       | 152205/665252 [02:56<03:42, 2306.30 examples/s]Map (num_proc=32):  23%|██▎       | 152859/665252 [02:56<03:09, 2697.69 examples/s]Map (num_proc=32):  23%|██▎       | 153527/665252 [02:56<02:44, 3110.68 examples/s]Map (num_proc=32):  23%|██▎       | 154527/665252 [02:56<02:20, 3646.82 examples/s]Map (num_proc=32):  23%|██▎       | 155190/665252 [02:56<02:03, 4136.00 examples/s]Map (num_proc=32):  23%|██▎       | 155859/665252 [02:56<01:56, 4364.73 examples/s]Map (num_proc=32):  24%|██▎       | 156527/665252 [02:57<01:52, 4541.46 examples/s]Map (num_proc=32):  24%|██▎       | 157204/665252 [02:57<01:41, 5003.98 examples/s]Map (num_proc=32):  24%|██▎       | 157890/665252 [02:57<01:39, 5074.44 examples/s]Map (num_proc=32):  24%|██▍       | 158527/665252 [02:57<01:36, 5246.52 examples/s]Map (num_proc=32):  24%|██▍       | 159142/665252 [02:57<01:58, 4269.78 examples/s]Map (num_proc=32):  24%|██▍       | 159923/665252 [02:57<01:50, 4590.68 examples/s]Map (num_proc=32):  24%|██▍       | 160527/665252 [02:57<01:44, 4846.94 examples/s]Map (num_proc=32):  24%|██▍       | 161527/665252 [02:58<01:32, 5441.13 examples/s]Map (num_proc=32):  24%|██▍       | 162527/665252 [02:58<01:27, 5753.59 examples/s]Map (num_proc=32):  25%|██▍       | 163527/665252 [02:58<01:23, 5990.62 examples/s]Map (num_proc=32):  25%|██▍       | 164303/665252 [02:58<01:18, 6383.45 examples/s]Map (num_proc=32):  25%|██▍       | 165341/665252 [02:58<01:18, 6385.47 examples/s]Map (num_proc=32):  25%|██▌       | 166316/665252 [02:58<01:20, 6173.24 examples/s]Map (num_proc=32):  25%|██▌       | 167092/665252 [03:16<49:32, 167.59 examples/s] Map (num_proc=32):  25%|██▌       | 167701/665252 [03:16<38:32, 215.15 examples/s]Map (num_proc=32):  25%|██▌       | 168316/665252 [03:16<29:19, 282.39 examples/s]Map (num_proc=32):  25%|██▌       | 169316/665252 [03:16<19:02, 434.19 examples/s]Map (num_proc=32):  26%|██▌       | 170316/665252 [03:16<12:55, 637.81 examples/s]Map (num_proc=32):  26%|██▌       | 171316/665252 [03:16<09:04, 907.11 examples/s]Map (num_proc=32):  26%|██▌       | 172116/665252 [03:16<06:52, 1196.86 examples/s]Map (num_proc=32):  26%|██▌       | 173076/665252 [03:16<05:05, 1609.98 examples/s]Map (num_proc=32):  26%|██▌       | 174106/665252 [03:17<03:51, 2124.37 examples/s]Map (num_proc=32):  26%|██▋       | 175119/665252 [03:17<03:01, 2700.19 examples/s]Map (num_proc=32):  26%|██▋       | 176130/665252 [03:17<02:27, 3306.59 examples/s]Map (num_proc=32):  27%|██▋       | 177125/665252 [03:17<02:04, 3919.58 examples/s]Map (num_proc=32):  27%|██▋       | 178132/665252 [03:17<01:49, 4447.47 examples/s]Map (num_proc=32):  27%|██▋       | 179130/665252 [03:17<01:39, 4900.15 examples/s]Map (num_proc=32):  27%|██▋       | 179931/665252 [03:18<02:01, 3998.65 examples/s]Map (num_proc=32):  27%|██▋       | 180716/665252 [03:18<01:48, 4450.33 examples/s]Map (num_proc=32):  27%|██▋       | 181316/665252 [03:18<01:43, 4694.53 examples/s]Map (num_proc=32):  27%|██▋       | 182316/665252 [03:18<01:33, 5141.07 examples/s]Map (num_proc=32):  28%|██▊       | 183318/665252 [03:18<01:25, 5662.05 examples/s]Map (num_proc=32):  28%|██▊       | 184105/665252 [03:18<01:19, 6048.39 examples/s]Map (num_proc=32):  28%|██▊       | 185135/665252 [03:18<01:16, 6262.89 examples/s]Map (num_proc=32):  28%|██▊       | 186121/665252 [03:19<01:15, 6305.99 examples/s]Map (num_proc=32):  28%|██▊       | 187105/665252 [03:19<01:20, 5939.48 examples/s]Map (num_proc=32):  28%|██▊       | 187105/665252 [03:37<01:20, 5939.48 examples/s]Map (num_proc=32):  28%|██▊       | 187489/665252 [03:38<59:10, 134.56 examples/s] Map (num_proc=32):  28%|██▊       | 188105/665252 [03:38<44:34, 178.39 examples/s]Map (num_proc=32):  28%|██▊       | 189105/665252 [03:38<28:28, 278.67 examples/s]Map (num_proc=32):  29%|██▊       | 189903/665252 [03:38<20:18, 390.26 examples/s]Map (num_proc=32):  29%|██▊       | 190923/665252 [03:38<13:34, 582.70 examples/s]Map (num_proc=32):  29%|██▉       | 191903/665252 [03:38<09:32, 827.08 examples/s]Map (num_proc=32):  29%|██▉       | 192881/665252 [03:39<06:53, 1141.83 examples/s]Map (num_proc=32):  29%|██▉       | 193905/665252 [03:39<05:03, 1554.32 examples/s]Map (num_proc=32):  29%|██▉       | 194918/665252 [03:39<03:51, 2030.49 examples/s]Map (num_proc=32):  29%|██▉       | 195880/665252 [03:39<03:04, 2543.37 examples/s]Map (num_proc=32):  30%|██▉       | 196908/665252 [03:39<02:28, 3161.70 examples/s]Map (num_proc=32):  30%|██▉       | 197903/665252 [03:39<02:05, 3714.84 examples/s]Map (num_proc=32):  30%|██▉       | 198849/665252 [03:40<01:51, 4182.05 examples/s]Map (num_proc=32):  30%|██▉       | 199511/665252 [03:40<01:43, 4490.77 examples/s]Map (num_proc=32):  30%|███       | 200500/665252 [03:40<01:35, 4856.71 examples/s]Map (num_proc=32):  30%|███       | 201502/665252 [03:40<01:28, 5241.03 examples/s]Map (num_proc=32):  30%|███       | 202492/665252 [03:40<01:46, 4359.46 examples/s]Map (num_proc=32):  31%|███       | 203515/665252 [03:40<01:34, 4884.55 examples/s]Map (num_proc=32):  31%|███       | 204105/665252 [03:41<01:31, 5048.97 examples/s]Map (num_proc=32):  31%|███       | 205105/665252 [03:41<01:24, 5441.75 examples/s]Map (num_proc=32):  31%|███       | 205889/665252 [03:41<01:17, 5933.78 examples/s]Map (num_proc=32):  31%|███       | 206919/665252 [03:41<01:14, 6164.25 examples/s]Map (num_proc=32):  31%|███▏      | 207894/665252 [03:41<01:16, 5940.01 examples/s]Map (num_proc=32):  31%|███▏      | 207894/665252 [03:57<01:16, 5940.01 examples/s]Map (num_proc=32):  31%|███▏      | 208285/665252 [04:00<55:39, 136.84 examples/s] Map (num_proc=32):  31%|███▏      | 208894/665252 [04:00<41:52, 181.61 examples/s]Map (num_proc=32):  32%|███▏      | 209689/665252 [04:00<28:42, 264.43 examples/s]Map (num_proc=32):  32%|███▏      | 210711/665252 [04:00<18:28, 410.01 examples/s]Map (num_proc=32):  32%|███▏      | 211702/665252 [04:00<12:34, 601.16 examples/s]Map (num_proc=32):  32%|███▏      | 212670/665252 [04:00<08:53, 847.54 examples/s]Map (num_proc=32):  32%|███▏      | 213698/665252 [04:00<06:20, 1185.71 examples/s]Map (num_proc=32):  32%|███▏      | 214637/665252 [04:01<04:48, 1563.77 examples/s]Map (num_proc=32):  32%|███▏      | 215286/665252 [04:01<04:00, 1867.90 examples/s]Map (num_proc=32):  33%|███▎      | 216283/665252 [04:01<03:03, 2449.22 examples/s]Map (num_proc=32):  33%|███▎      | 217294/665252 [04:01<02:25, 3070.46 examples/s]Map (num_proc=32):  33%|███▎      | 218282/665252 [04:01<02:01, 3680.03 examples/s]Map (num_proc=32):  33%|███▎      | 219286/665252 [04:01<01:44, 4262.59 examples/s]Map (num_proc=32):  33%|███▎      | 220295/665252 [04:01<01:32, 4788.76 examples/s]Map (num_proc=32):  33%|███▎      | 221289/665252 [04:02<01:26, 5112.49 examples/s]Map (num_proc=32):  33%|███▎      | 222300/665252 [04:02<01:41, 4359.67 examples/s]Map (num_proc=32):  34%|███▎      | 222894/665252 [04:02<01:36, 4592.84 examples/s]Map (num_proc=32):  34%|███▎      | 223715/665252 [04:02<01:24, 5250.04 examples/s]Map (num_proc=32):  34%|███▍      | 224707/665252 [04:02<01:18, 5599.26 examples/s]Map (num_proc=32):  34%|███▍      | 225703/665252 [04:02<01:14, 5863.94 examples/s]Map (num_proc=32):  34%|███▍      | 226713/665252 [04:03<01:11, 6110.13 examples/s]Map (num_proc=32):  34%|███▍      | 227711/665252 [04:03<01:10, 6205.23 examples/s]Map (num_proc=32):  34%|███▍      | 228683/665252 [04:03<01:14, 5832.14 examples/s]Map (num_proc=32):  34%|███▍      | 228683/665252 [04:17<01:14, 5832.14 examples/s]Map (num_proc=32):  34%|███▍      | 229066/665252 [04:20<48:27, 150.02 examples/s] Map (num_proc=32):  35%|███▍      | 229683/665252 [04:20<36:37, 198.19 examples/s]Map (num_proc=32):  35%|███▍      | 230683/665252 [04:20<23:31, 307.79 examples/s]Map (num_proc=32):  35%|███▍      | 231448/665252 [04:21<17:02, 424.28 examples/s]Map (num_proc=32):  35%|███▍      | 232474/665252 [04:21<11:21, 634.65 examples/s]Map (num_proc=32):  35%|███▌      | 233457/665252 [04:21<08:02, 895.74 examples/s]Map (num_proc=32):  35%|███▌      | 234492/665252 [04:21<06:09, 1165.68 examples/s]Map (num_proc=32):  36%|███▌      | 236483/665252 [04:21<03:25, 2085.49 examples/s]Map (num_proc=32):  36%|███▌      | 237471/665252 [04:21<02:50, 2505.77 examples/s]Map (num_proc=32):  36%|███▌      | 238500/665252 [04:22<02:21, 3016.94 examples/s]Map (num_proc=32):  36%|███▌      | 239496/665252 [04:22<02:00, 3544.92 examples/s]Map (num_proc=32):  36%|███▌      | 240471/665252 [04:22<01:45, 4040.72 examples/s]Map (num_proc=32):  36%|███▋      | 241683/665252 [04:22<01:28, 4770.00 examples/s]Map (num_proc=32):  36%|███▋      | 242683/665252 [04:22<01:16, 5508.04 examples/s]Map (num_proc=32):  37%|███▋      | 243683/665252 [04:23<01:39, 4222.88 examples/s]Map (num_proc=32):  37%|███▋      | 244683/665252 [04:23<01:23, 5024.29 examples/s]Map (num_proc=32):  37%|███▋      | 245683/665252 [04:23<01:13, 5697.74 examples/s]Map (num_proc=32):  37%|███▋      | 246683/665252 [04:23<01:05, 6408.35 examples/s]Map (num_proc=32):  37%|███▋      | 247683/665252 [04:23<00:58, 7119.93 examples/s]Map (num_proc=32):  37%|███▋      | 248683/665252 [04:23<00:56, 7318.83 examples/s]Map (num_proc=32):  38%|███▊      | 249472/665252 [04:37<00:56, 7318.83 examples/s]Map (num_proc=32):  38%|███▊      | 249959/665252 [04:41<35:04, 197.38 examples/s] Map (num_proc=32):  38%|███▊      | 251149/665252 [04:41<24:05, 286.55 examples/s]Map (num_proc=32):  38%|███▊      | 252458/665252 [04:41<16:16, 422.90 examples/s]Map (num_proc=32):  38%|███▊      | 253480/665252 [04:42<12:06, 566.51 examples/s]Map (num_proc=32):  38%|███▊      | 254709/665252 [04:42<08:55, 766.55 examples/s]Map (num_proc=32):  39%|███▊      | 257472/665252 [04:42<04:30, 1505.79 examples/s]Map (num_proc=32):  39%|███▉      | 259236/665252 [04:42<03:18, 2042.75 examples/s]Map (num_proc=32):  39%|███▉      | 260472/665252 [04:42<02:43, 2473.86 examples/s]Map (num_proc=32):  39%|███▉      | 261970/665252 [04:43<02:09, 3125.00 examples/s]Map (num_proc=32):  40%|███▉      | 262972/665252 [04:43<01:51, 3612.10 examples/s]Map (num_proc=32):  40%|███▉      | 263964/665252 [04:43<01:37, 4122.28 examples/s]Map (num_proc=32):  40%|███▉      | 264954/665252 [04:43<01:40, 3980.13 examples/s]Map (num_proc=32):  40%|███▉      | 265980/665252 [04:43<01:26, 4631.58 examples/s]Map (num_proc=32):  40%|████      | 266982/665252 [04:43<01:15, 5294.80 examples/s]Map (num_proc=32):  40%|████      | 267982/665252 [04:44<01:08, 5838.86 examples/s]Map (num_proc=32):  40%|████      | 268981/665252 [04:44<01:02, 6356.03 examples/s]Map (num_proc=32):  41%|████      | 269980/665252 [04:44<00:58, 6812.61 examples/s]Map (num_proc=32):  41%|████      | 270261/665252 [04:57<00:57, 6812.61 examples/s]Map (num_proc=32):  41%|████      | 270749/665252 [05:03<41:21, 158.97 examples/s] Map (num_proc=32):  41%|████      | 271941/665252 [05:04<27:06, 241.78 examples/s]Map (num_proc=32):  41%|████      | 273254/665252 [05:04<17:43, 368.77 examples/s]Map (num_proc=32):  41%|████      | 274275/665252 [05:04<12:56, 503.61 examples/s]Map (num_proc=32):  41%|████▏     | 275261/665252 [05:04<09:31, 682.78 examples/s]Map (num_proc=32):  42%|████▏     | 276505/665252 [05:04<06:33, 986.83 examples/s]Map (num_proc=32):  42%|████▏     | 277748/665252 [05:04<04:40, 1381.62 examples/s]Map (num_proc=32):  42%|████▏     | 278752/665252 [05:04<03:36, 1781.79 examples/s]Map (num_proc=32):  42%|████▏     | 279731/665252 [05:05<02:50, 2266.79 examples/s]Map (num_proc=32):  42%|████▏     | 281035/665252 [05:05<02:06, 3037.90 examples/s]Map (num_proc=32):  42%|████▏     | 282261/665252 [05:05<01:42, 3723.27 examples/s]Map (num_proc=32):  43%|████▎     | 283261/665252 [05:05<01:27, 4363.08 examples/s]Map (num_proc=32):  43%|████▎     | 284261/665252 [05:05<01:16, 4949.78 examples/s]Map (num_proc=32):  43%|████▎     | 285261/665252 [05:05<01:26, 4416.92 examples/s]Map (num_proc=32):  43%|████▎     | 286261/665252 [05:06<01:14, 5116.85 examples/s]Map (num_proc=32):  43%|████▎     | 287261/665252 [05:06<01:06, 5717.74 examples/s]Map (num_proc=32):  43%|████▎     | 288261/665252 [05:06<01:00, 6277.23 examples/s]Map (num_proc=32):  43%|████▎     | 289261/665252 [05:06<00:55, 6809.35 examples/s]Map (num_proc=32):  44%|████▎     | 290261/665252 [05:06<00:52, 7181.77 examples/s]Map (num_proc=32):  44%|████▍     | 291050/665252 [05:17<00:52, 7181.77 examples/s]Map (num_proc=32):  44%|████▍     | 291532/665252 [05:24<31:22, 198.49 examples/s] Map (num_proc=32):  44%|████▍     | 292722/665252 [05:24<21:32, 288.31 examples/s]Map (num_proc=32):  44%|████▍     | 293545/665252 [05:24<16:32, 374.58 examples/s]Map (num_proc=32):  44%|████▍     | 294800/665252 [05:24<11:04, 557.19 examples/s]Map (num_proc=32):  45%|████▍     | 296050/665252 [05:25<07:42, 798.63 examples/s]Map (num_proc=32):  45%|████▍     | 297050/665252 [05:25<05:47, 1059.21 examples/s]Map (num_proc=32):  45%|████▍     | 298050/665252 [05:25<04:22, 1396.80 examples/s]Map (num_proc=32):  45%|████▍     | 299050/665252 [05:25<03:19, 1836.91 examples/s]Map (num_proc=32):  45%|████▌     | 300044/665252 [05:25<02:32, 2402.37 examples/s]Map (num_proc=32):  45%|████▌     | 301094/665252 [05:25<02:01, 2998.40 examples/s]Map (num_proc=32):  45%|████▌     | 302050/665252 [05:25<01:39, 3640.35 examples/s]Map (num_proc=32):  46%|████▌     | 303050/665252 [05:25<01:22, 4403.65 examples/s]Map (num_proc=32):  46%|████▌     | 304048/665252 [05:26<01:09, 5234.43 examples/s]Map (num_proc=32):  46%|████▌     | 305050/665252 [05:26<01:04, 5606.68 examples/s]Map (num_proc=32):  46%|████▌     | 306044/665252 [05:26<00:56, 6304.44 examples/s]Map (num_proc=32):  46%|████▌     | 307018/665252 [05:26<01:13, 4846.84 examples/s]Map (num_proc=32):  46%|████▋     | 308050/665252 [05:26<01:06, 5366.34 examples/s]Map (num_proc=32):  46%|████▋     | 309050/665252 [05:26<00:59, 5939.85 examples/s]Map (num_proc=32):  47%|████▋     | 310050/665252 [05:26<00:54, 6514.05 examples/s]Map (num_proc=32):  47%|████▋     | 311039/665252 [05:27<00:49, 7115.41 examples/s]Map (num_proc=32):  47%|████▋     | 311839/665252 [05:37<00:49, 7115.41 examples/s]Map (num_proc=32):  47%|████▋     | 312323/665252 [05:45<31:11, 188.55 examples/s] Map (num_proc=32):  47%|████▋     | 313511/665252 [05:46<21:23, 274.12 examples/s]Map (num_proc=32):  47%|████▋     | 314330/665252 [05:46<16:24, 356.39 examples/s]Map (num_proc=32):  47%|████▋     | 315330/665252 [05:46<11:45, 495.95 examples/s]Map (num_proc=32):  48%|████▊     | 316298/665252 [05:46<08:40, 669.86 examples/s]Map (num_proc=32):  48%|████▊     | 317032/665252 [05:46<07:00, 827.21 examples/s]Map (num_proc=32):  48%|████▊     | 317734/665252 [05:46<05:35, 1037.17 examples/s]Map (num_proc=32):  48%|████▊     | 318308/665252 [05:47<04:44, 1219.75 examples/s]Map (num_proc=32):  48%|████▊     | 318839/665252 [05:47<04:04, 1414.49 examples/s]Map (num_proc=32):  48%|████▊     | 319536/665252 [05:47<03:13, 1785.93 examples/s]Map (num_proc=32):  48%|████▊     | 320041/665252 [05:47<02:53, 1984.77 examples/s]Map (num_proc=32):  48%|████▊     | 320514/665252 [05:47<02:29, 2305.30 examples/s]Map (num_proc=32):  48%|████▊     | 321066/665252 [05:47<02:20, 2453.83 examples/s]Map (num_proc=32):  48%|████▊     | 321772/665252 [05:47<01:56, 2941.33 examples/s]Map (num_proc=32):  48%|████▊     | 322292/665252 [05:48<01:54, 3001.53 examples/s]Map (num_proc=32):  49%|████▊     | 322839/665252 [05:48<01:50, 3084.94 examples/s]Map (num_proc=32):  49%|████▊     | 323540/665252 [05:48<01:37, 3522.32 examples/s]Map (num_proc=32):  49%|████▊     | 324058/665252 [05:48<01:39, 3413.56 examples/s]Map (num_proc=32):  49%|████▉     | 324534/665252 [05:48<01:33, 3637.36 examples/s]Map (num_proc=32):  49%|████▉     | 325066/665252 [05:48<01:35, 3544.88 examples/s]Map (num_proc=32):  49%|████▉     | 325531/665252 [05:48<01:30, 3737.43 examples/s]Map (num_proc=32):  49%|████▉     | 326061/665252 [05:49<01:35, 3559.73 examples/s]Map (num_proc=32):  49%|████▉     | 326550/665252 [05:49<01:28, 3848.28 examples/s]Map (num_proc=32):  49%|████▉     | 327068/665252 [05:49<02:05, 2704.57 examples/s]Map (num_proc=32):  49%|████▉     | 327556/665252 [05:49<01:48, 3099.32 examples/s]Map (num_proc=32):  49%|████▉     | 328068/665252 [05:49<01:46, 3169.12 examples/s]Map (num_proc=32):  49%|████▉     | 328557/665252 [05:49<01:35, 3524.58 examples/s]Map (num_proc=32):  49%|████▉     | 329074/665252 [05:50<01:41, 3317.53 examples/s]Map (num_proc=32):  50%|████▉     | 329797/665252 [05:50<01:29, 3759.30 examples/s]Map (num_proc=32):  50%|████▉     | 330307/665252 [05:50<01:33, 3587.72 examples/s]Map (num_proc=32):  50%|████▉     | 330839/665252 [05:50<01:36, 3469.56 examples/s]Map (num_proc=32):  50%|████▉     | 331529/665252 [05:50<01:26, 3844.37 examples/s]Map (num_proc=32):  50%|████▉     | 332076/665252 [05:50<01:32, 3619.09 examples/s]Map (num_proc=32):  50%|█████     | 332628/665252 [05:51<01:32, 3595.91 examples/s]Map (num_proc=32):  50%|█████     | 333095/665252 [06:07<51:30, 107.47 examples/s] Map (num_proc=32):  50%|█████     | 333628/665252 [06:07<36:38, 150.86 examples/s]Map (num_proc=32):  50%|█████     | 334100/665252 [06:07<26:54, 205.17 examples/s]Map (num_proc=32):  50%|█████     | 334628/665252 [06:07<19:11, 287.01 examples/s]Map (num_proc=32):  50%|█████     | 335106/665252 [06:07<14:06, 390.16 examples/s]Map (num_proc=32):  50%|█████     | 335628/665252 [06:08<10:17, 533.53 examples/s]Map (num_proc=32):  51%|█████     | 336364/665252 [06:08<06:46, 809.38 examples/s]Map (num_proc=32):  51%|█████     | 336867/665252 [06:08<05:22, 1019.10 examples/s]Map (num_proc=32):  51%|█████     | 337347/665252 [06:08<04:13, 1291.91 examples/s]Map (num_proc=32):  51%|█████     | 337870/665252 [06:08<03:26, 1586.15 examples/s]Map (num_proc=32):  51%|█████     | 338350/665252 [06:08<02:48, 1939.62 examples/s]Map (num_proc=32):  51%|█████     | 338870/665252 [06:08<02:30, 2175.50 examples/s]Map (num_proc=32):  51%|█████     | 339603/665252 [06:09<01:58, 2752.90 examples/s]Map (num_proc=32):  51%|█████     | 340103/665252 [06:09<01:53, 2863.49 examples/s]Map (num_proc=32):  51%|█████     | 340628/665252 [06:09<01:48, 3001.89 examples/s]Map (num_proc=32):  51%|█████▏    | 341108/665252 [06:09<01:36, 3343.46 examples/s]Map (num_proc=32):  51%|█████▏    | 341599/665252 [06:09<01:28, 3675.21 examples/s]Map (num_proc=32):  51%|█████▏    | 342104/665252 [06:09<01:31, 3537.16 examples/s]Map (num_proc=32):  52%|█████▏    | 342613/665252 [06:09<01:22, 3889.85 examples/s]Map (num_proc=32):  52%|█████▏    | 343111/665252 [06:09<01:25, 3755.93 examples/s]Map (num_proc=32):  52%|█████▏    | 343599/665252 [06:10<01:21, 3953.96 examples/s]Map (num_proc=32):  52%|█████▏    | 344114/665252 [06:10<01:26, 3715.86 examples/s]Map (num_proc=32):  52%|█████▏    | 344619/665252 [06:10<01:19, 4020.09 examples/s]Map (num_proc=32):  52%|█████▏    | 345118/665252 [06:10<01:24, 3776.35 examples/s]Map (num_proc=32):  52%|█████▏    | 345628/665252 [06:10<01:26, 3680.90 examples/s]Map (num_proc=32):  52%|█████▏    | 346122/665252 [06:10<01:21, 3934.82 examples/s]Map (num_proc=32):  52%|█████▏    | 346628/665252 [06:10<01:25, 3736.53 examples/s]Map (num_proc=32):  52%|█████▏    | 347380/665252 [06:11<01:15, 4193.73 examples/s]Map (num_proc=32):  52%|█████▏    | 347868/665252 [06:11<01:49, 2886.86 examples/s]Map (num_proc=32):  52%|█████▏    | 348370/665252 [06:11<01:36, 3274.72 examples/s]Map (num_proc=32):  52%|█████▏    | 348875/665252 [06:11<01:36, 3283.98 examples/s]Map (num_proc=32):  53%|█████▎    | 349628/665252 [06:11<01:30, 3496.06 examples/s]Map (num_proc=32):  53%|█████▎    | 350112/665252 [06:11<01:23, 3758.90 examples/s]Map (num_proc=32):  53%|█████▎    | 350633/665252 [06:12<01:25, 3672.93 examples/s]Map (num_proc=32):  53%|█████▎    | 351426/665252 [06:12<02:11, 2393.29 examples/s]Map (num_proc=32):  53%|█████▎    | 353417/665252 [06:12<01:08, 4567.47 examples/s]Map (num_proc=32):  53%|█████▎    | 353417/665252 [06:27<01:08, 4567.47 examples/s]Map (num_proc=32):  53%|█████▎    | 353662/665252 [06:30<36:39, 141.69 examples/s] Map (num_proc=32):  53%|█████▎    | 354168/665252 [06:31<29:42, 174.48 examples/s]Map (num_proc=32):  54%|█████▎    | 356224/665252 [06:31<13:21, 385.52 examples/s]Map (num_proc=32):  54%|█████▎    | 357270/665252 [06:31<09:49, 522.40 examples/s]Map (num_proc=32):  54%|█████▍    | 357994/665252 [06:31<07:54, 648.03 examples/s]Map (num_proc=32):  54%|█████▍    | 358717/665252 [06:32<06:16, 814.53 examples/s]Map (num_proc=32):  54%|█████▍    | 359429/665252 [06:32<04:56, 1031.48 examples/s]Map (num_proc=32):  54%|█████▍    | 360107/665252 [06:32<03:52, 1313.36 examples/s]Map (num_proc=32):  54%|█████▍    | 360763/665252 [06:32<03:08, 1615.00 examples/s]Map (num_proc=32):  54%|█████▍    | 361417/665252 [06:32<02:33, 1985.76 examples/s]Map (num_proc=32):  54%|█████▍    | 362417/665252 [06:32<01:52, 2683.02 examples/s]Map (num_proc=32):  55%|█████▍    | 363228/665252 [06:32<01:29, 3359.48 examples/s]Map (num_proc=32):  55%|█████▍    | 364176/665252 [06:32<01:15, 3980.86 examples/s]Map (num_proc=32):  55%|█████▍    | 364992/665252 [06:33<01:21, 3679.94 examples/s]Map (num_proc=32):  55%|█████▍    | 365602/665252 [06:33<01:28, 3374.73 examples/s]Map (num_proc=32):  55%|█████▌    | 366165/665252 [06:33<01:26, 3458.04 examples/s]Map (num_proc=32):  55%|█████▌    | 366603/665252 [06:33<01:34, 3172.71 examples/s]Map (num_proc=32):  55%|█████▌    | 367183/665252 [06:33<01:29, 3338.72 examples/s]Map (num_proc=32):  55%|█████▌    | 367599/665252 [06:34<01:36, 3079.66 examples/s]Map (num_proc=32):  55%|█████▌    | 367989/665252 [06:34<01:31, 3234.61 examples/s]Map (num_proc=32):  55%|█████▌    | 368417/665252 [06:34<02:10, 2280.32 examples/s]Map (num_proc=32):  55%|█████▌    | 368979/665252 [06:34<01:52, 2640.37 examples/s]Map (num_proc=32):  56%|█████▌    | 369347/665252 [06:34<01:44, 2825.95 examples/s]Map (num_proc=32):  56%|█████▌    | 369798/665252 [06:35<01:49, 2705.08 examples/s]Map (num_proc=32):  56%|█████▌    | 370190/665252 [06:35<01:40, 2945.33 examples/s]Map (num_proc=32):  56%|█████▌    | 370610/665252 [06:35<01:45, 2788.74 examples/s]Map (num_proc=32):  56%|█████▌    | 371158/665252 [06:35<01:35, 3066.71 examples/s]Map (num_proc=32):  56%|█████▌    | 371605/665252 [06:35<01:42, 2858.23 examples/s]Map (num_proc=32):  56%|█████▌    | 371999/665252 [06:35<01:35, 3084.28 examples/s]Map (num_proc=32):  56%|█████▌    | 372417/665252 [06:35<01:42, 2870.38 examples/s]Map (num_proc=32):  56%|█████▌    | 372801/665252 [06:35<01:35, 3077.94 examples/s]Map (num_proc=32):  56%|█████▌    | 373384/665252 [06:36<01:27, 3329.55 examples/s]Map (num_proc=32):  56%|█████▌    | 373801/665252 [06:36<01:37, 2993.14 examples/s]Map (num_proc=32):  56%|█████▋    | 374206/665252 [06:36<01:56, 2496.14 examples/s]Map (num_proc=32):  56%|█████▋    | 374206/665252 [06:47<01:56, 2496.14 examples/s]Map (num_proc=32):  56%|█████▋    | 374383/665252 [06:54<1:14:32, 65.04 examples/s]Map (num_proc=32):  56%|█████▋    | 374907/665252 [06:54<46:18, 104.51 examples/s] Map (num_proc=32):  56%|█████▋    | 375385/665252 [06:54<31:33, 153.08 examples/s]Map (num_proc=32):  56%|█████▋    | 375745/665252 [06:55<23:33, 204.88 examples/s]Map (num_proc=32):  57%|█████▋    | 376105/665252 [06:55<17:26, 276.23 examples/s]Map (num_proc=32):  57%|█████▋    | 376563/665252 [06:55<12:21, 389.56 examples/s]Map (num_proc=32):  57%|█████▋    | 376922/665252 [06:55<09:21, 513.77 examples/s]Map (num_proc=32):  57%|█████▋    | 377378/665252 [06:55<06:56, 691.46 examples/s]Map (num_proc=32):  57%|█████▋    | 377729/665252 [06:55<05:26, 879.28 examples/s]Map (num_proc=32):  57%|█████▋    | 378095/665252 [06:55<04:16, 1120.89 examples/s]Map (num_proc=32):  57%|█████▋    | 378546/665252 [06:56<03:38, 1311.86 examples/s]Map (num_proc=32):  57%|█████▋    | 378914/665252 [06:56<02:59, 1599.52 examples/s]Map (num_proc=32):  57%|█████▋    | 379375/665252 [06:56<02:40, 1777.05 examples/s]Map (num_proc=32):  57%|█████▋    | 379742/665252 [06:56<02:18, 2068.20 examples/s]Map (num_proc=32):  57%|█████▋    | 380206/665252 [06:56<02:12, 2148.10 examples/s]Map (num_proc=32):  57%|█████▋    | 380742/665252 [06:56<01:52, 2518.05 examples/s]Map (num_proc=32):  57%|█████▋    | 381097/665252 [06:56<01:45, 2690.86 examples/s]Map (num_proc=32):  57%|█████▋    | 381557/665252 [06:57<01:52, 2519.75 examples/s]Map (num_proc=32):  57%|█████▋    | 382099/665252 [06:57<01:40, 2810.89 examples/s]Map (num_proc=32):  58%|█████▊    | 382564/665252 [06:57<01:46, 2646.49 examples/s]Map (num_proc=32):  58%|█████▊    | 382920/665252 [06:57<01:40, 2814.98 examples/s]Map (num_proc=32):  58%|█████▊    | 383377/665252 [06:57<01:46, 2642.97 examples/s]Map (num_proc=32):  58%|█████▊    | 383921/665252 [06:57<01:35, 2950.50 examples/s]Map (num_proc=32):  58%|█████▊    | 384381/665252 [06:58<01:43, 2713.16 examples/s]Map (num_proc=32):  58%|█████▊    | 384930/665252 [06:58<01:34, 2963.62 examples/s]Map (num_proc=32):  58%|█████▊    | 385384/665252 [06:58<01:48, 2568.80 examples/s]Map (num_proc=32):  58%|█████▊    | 385933/665252 [06:58<01:30, 3077.25 examples/s]Map (num_proc=32):  58%|█████▊    | 386387/665252 [06:58<01:40, 2785.62 examples/s]Map (num_proc=32):  58%|█████▊    | 386746/665252 [06:58<01:34, 2942.04 examples/s]Map (num_proc=32):  58%|█████▊    | 387104/665252 [06:59<01:30, 3081.26 examples/s]Map (num_proc=32):  58%|█████▊    | 387561/665252 [06:59<01:41, 2743.53 examples/s]Map (num_proc=32):  58%|█████▊    | 387923/665252 [06:59<01:34, 2929.05 examples/s]Map (num_proc=32):  58%|█████▊    | 388378/665252 [06:59<01:55, 2391.30 examples/s]Map (num_proc=32):  58%|█████▊    | 388738/665252 [06:59<01:45, 2625.01 examples/s]Map (num_proc=32):  58%|█████▊    | 389103/665252 [06:59<01:37, 2846.41 examples/s]Map (num_proc=32):  59%|█████▊    | 389561/665252 [07:00<02:17, 2005.15 examples/s]Map (num_proc=32):  59%|█████▊    | 389931/665252 [07:00<01:59, 2296.49 examples/s]Map (num_proc=32):  59%|█████▊    | 390378/665252 [07:00<02:02, 2248.00 examples/s]Map (num_proc=32):  59%|█████▊    | 390740/665252 [07:00<01:49, 2505.90 examples/s]Map (num_proc=32):  59%|█████▉    | 391085/665252 [07:00<01:41, 2703.53 examples/s]Map (num_proc=32):  59%|█████▉    | 391559/665252 [07:00<01:48, 2530.20 examples/s]Map (num_proc=32):  59%|█████▉    | 391907/665252 [07:00<01:40, 2727.18 examples/s]Map (num_proc=32):  59%|█████▉    | 392371/665252 [07:01<01:48, 2523.72 examples/s]Map (num_proc=32):  59%|█████▉    | 392729/665252 [07:01<01:39, 2741.65 examples/s]Map (num_proc=32):  59%|█████▉    | 393087/665252 [07:01<01:32, 2928.44 examples/s]Map (num_proc=32):  59%|█████▉    | 393558/665252 [07:01<01:43, 2614.82 examples/s]Map (num_proc=32):  59%|█████▉    | 393913/665252 [07:01<01:36, 2814.38 examples/s]Map (num_proc=32):  59%|█████▉    | 394375/665252 [07:01<01:47, 2523.04 examples/s]Map (num_proc=32):  59%|█████▉    | 394732/665252 [07:02<01:38, 2738.83 examples/s]Map (num_proc=32):  59%|█████▉    | 395172/665252 [07:14<42:33, 105.79 examples/s] Map (num_proc=32):  59%|█████▉    | 395351/665252 [07:14<37:14, 120.81 examples/s]Map (num_proc=32):  60%|█████▉    | 396541/665252 [07:15<15:05, 296.88 examples/s]Map (num_proc=32):  60%|█████▉    | 397174/665252 [07:15<10:54, 409.81 examples/s]Map (num_proc=32):  60%|█████▉    | 397708/665252 [07:15<08:14, 541.09 examples/s]Map (num_proc=32):  60%|█████▉    | 398171/665252 [07:15<06:36, 673.08 examples/s]Map (num_proc=32):  60%|█████▉    | 398541/665252 [07:15<05:23, 824.16 examples/s]Map (num_proc=32):  60%|█████▉    | 398995/665252 [07:15<04:26, 999.64 examples/s]Map (num_proc=32):  60%|██████    | 399340/665252 [07:16<03:41, 1199.22 examples/s]Map (num_proc=32):  60%|██████    | 399887/665252 [07:16<02:48, 1578.21 examples/s]Map (num_proc=32):  60%|██████    | 400356/665252 [07:16<02:32, 1734.19 examples/s]Map (num_proc=32):  60%|██████    | 400711/665252 [07:16<02:15, 1956.20 examples/s]Map (num_proc=32):  60%|██████    | 401173/665252 [07:16<02:07, 2075.61 examples/s]Map (num_proc=32):  60%|██████    | 401540/665252 [07:16<01:52, 2340.56 examples/s]Map (num_proc=32):  60%|██████    | 401911/665252 [07:16<01:41, 2602.11 examples/s]Map (num_proc=32):  60%|██████    | 402357/665252 [07:17<01:44, 2525.07 examples/s]Map (num_proc=32):  61%|██████    | 402864/665252 [07:17<01:35, 2760.44 examples/s]Map (num_proc=32):  61%|██████    | 403355/665252 [07:17<01:38, 2669.39 examples/s]Map (num_proc=32):  61%|██████    | 403722/665252 [07:17<01:32, 2838.89 examples/s]Map (num_proc=32):  61%|██████    | 404138/665252 [07:17<01:38, 2655.84 examples/s]Map (num_proc=32):  61%|██████    | 404502/665252 [07:17<01:31, 2834.50 examples/s]Map (num_proc=32):  61%|██████    | 404871/665252 [07:17<01:25, 3028.69 examples/s]Map (num_proc=32):  61%|██████    | 405352/665252 [07:18<01:32, 2821.97 examples/s]Map (num_proc=32):  61%|██████    | 405865/665252 [07:18<01:27, 2968.40 examples/s]Map (num_proc=32):  61%|██████    | 406356/665252 [07:18<01:33, 2770.34 examples/s]Map (num_proc=32):  61%|██████    | 406727/665252 [07:18<01:27, 2959.53 examples/s]Map (num_proc=32):  61%|██████    | 407171/665252 [07:18<01:34, 2722.37 examples/s]Map (num_proc=32):  61%|██████▏   | 407543/665252 [07:18<01:27, 2931.45 examples/s]Map (num_proc=32):  61%|██████▏   | 407909/665252 [07:19<01:24, 3048.07 examples/s]Map (num_proc=32):  61%|██████▏   | 408354/665252 [07:19<01:31, 2802.98 examples/s]Map (num_proc=32):  61%|██████▏   | 408910/665252 [07:19<01:23, 3072.13 examples/s]Map (num_proc=32):  62%|██████▏   | 409358/665252 [07:19<01:41, 2525.67 examples/s]Map (num_proc=32):  62%|██████▏   | 409716/665252 [07:19<01:33, 2725.25 examples/s]Map (num_proc=32):  62%|██████▏   | 410174/665252 [07:20<02:06, 2008.90 examples/s]Map (num_proc=32):  62%|██████▏   | 410721/665252 [07:20<01:46, 2385.82 examples/s]Map (num_proc=32):  62%|██████▏   | 411173/665252 [07:20<01:45, 2406.57 examples/s]Map (num_proc=32):  62%|██████▏   | 411546/665252 [07:20<01:37, 2604.74 examples/s]Map (num_proc=32):  62%|██████▏   | 411923/665252 [07:20<01:29, 2840.85 examples/s]Map (num_proc=32):  62%|██████▏   | 412361/665252 [07:20<01:40, 2507.14 examples/s]Map (num_proc=32):  62%|██████▏   | 412995/665252 [07:21<01:30, 2797.05 examples/s]Map (num_proc=32):  62%|██████▏   | 413344/665252 [07:21<01:25, 2932.85 examples/s]Map (num_proc=32):  62%|██████▏   | 413715/665252 [07:21<01:21, 3103.19 examples/s]Map (num_proc=32):  62%|██████▏   | 414174/665252 [07:21<01:28, 2836.48 examples/s]Map (num_proc=32):  62%|██████▏   | 414547/665252 [07:21<01:22, 3028.85 examples/s]Map (num_proc=32):  62%|██████▏   | 414916/665252 [07:21<01:18, 3184.48 examples/s]Map (num_proc=32):  62%|██████▏   | 415362/665252 [07:21<01:29, 2782.36 examples/s]Map (num_proc=32):  62%|██████▏   | 415739/665252 [07:21<01:23, 3000.26 examples/s]Map (num_proc=32):  63%|██████▎   | 416133/665252 [07:35<43:07, 96.27 examples/s]  Map (num_proc=32):  63%|██████▎   | 416651/665252 [07:35<28:07, 147.35 examples/s]Map (num_proc=32):  63%|██████▎   | 417139/665252 [07:36<20:15, 204.07 examples/s]Map (num_proc=32):  63%|██████▎   | 417784/665252 [07:36<12:58, 318.07 examples/s]Map (num_proc=32):  63%|██████▎   | 418324/665252 [07:36<09:15, 444.66 examples/s]Map (num_proc=32):  63%|██████▎   | 418682/665252 [07:36<07:23, 555.73 examples/s]Map (num_proc=32):  63%|██████▎   | 419141/665252 [07:36<05:43, 716.66 examples/s]Map (num_proc=32):  63%|██████▎   | 419500/665252 [07:36<04:35, 890.97 examples/s]Map (num_proc=32):  63%|██████▎   | 419955/665252 [07:37<03:46, 1084.10 examples/s]Map (num_proc=32):  63%|██████▎   | 420311/665252 [07:37<03:06, 1313.58 examples/s]Map (num_proc=32):  63%|██████▎   | 420670/665252 [07:37<02:34, 1583.52 examples/s]Map (num_proc=32):  63%|██████▎   | 421138/665252 [07:37<02:18, 1768.00 examples/s]Map (num_proc=32):  63%|██████▎   | 421693/665252 [07:37<01:51, 2193.55 examples/s]Map (num_proc=32):  63%|██████▎   | 422019/665252 [07:38<02:39, 1520.55 examples/s]Map (num_proc=32):  63%|██████▎   | 422315/665252 [07:38<02:21, 1712.10 examples/s]Map (num_proc=32):  64%|██████▎   | 422784/665252 [07:38<02:07, 1895.79 examples/s]Map (num_proc=32):  64%|██████▎   | 423141/665252 [07:38<01:52, 2155.10 examples/s]Map (num_proc=32):  64%|██████▎   | 423503/665252 [07:38<01:39, 2433.10 examples/s]Map (num_proc=32):  64%|██████▎   | 423951/665252 [07:38<01:42, 2348.92 examples/s]Map (num_proc=32):  64%|██████▍   | 424493/665252 [07:38<01:29, 2692.99 examples/s]Map (num_proc=32):  64%|██████▍   | 424958/665252 [07:39<01:35, 2504.56 examples/s]Map (num_proc=32):  64%|██████▍   | 425311/665252 [07:39<02:40, 1491.57 examples/s]Map (num_proc=32):  64%|██████▍   | 426498/665252 [07:39<01:24, 2831.94 examples/s]Map (num_proc=32):  64%|██████▍   | 427140/665252 [07:39<01:12, 3262.08 examples/s]Map (num_proc=32):  64%|██████▍   | 427700/665252 [07:39<01:10, 3373.66 examples/s]Map (num_proc=32):  64%|██████▍   | 428330/665252 [07:40<01:16, 3103.01 examples/s]Map (num_proc=32):  64%|██████▍   | 428784/665252 [07:40<01:21, 2887.35 examples/s]Map (num_proc=32):  65%|██████▍   | 429331/665252 [07:40<01:16, 3081.40 examples/s]Map (num_proc=32):  65%|██████▍   | 429707/665252 [07:40<01:13, 3208.34 examples/s]Map (num_proc=32):  65%|██████▍   | 430150/665252 [07:40<01:31, 2558.98 examples/s]Map (num_proc=32):  65%|██████▍   | 430523/665252 [07:41<01:24, 2771.45 examples/s]Map (num_proc=32):  65%|██████▍   | 430962/665252 [07:41<01:55, 2024.47 examples/s]Map (num_proc=32):  65%|██████▍   | 431524/665252 [07:41<01:37, 2408.05 examples/s]Map (num_proc=32):  65%|██████▍   | 431960/665252 [07:41<01:37, 2382.19 examples/s]Map (num_proc=32):  65%|██████▍   | 432341/665252 [07:41<01:28, 2625.83 examples/s]Map (num_proc=32):  65%|██████▌   | 432714/665252 [07:41<01:21, 2844.76 examples/s]Map (num_proc=32):  65%|██████▌   | 433158/665252 [07:42<01:30, 2552.55 examples/s]Map (num_proc=32):  65%|██████▌   | 433715/665252 [07:42<01:14, 3114.17 examples/s]Map (num_proc=32):  65%|██████▌   | 434137/665252 [07:42<01:21, 2834.63 examples/s]Map (num_proc=32):  65%|██████▌   | 434511/665252 [07:42<01:16, 3023.71 examples/s]Map (num_proc=32):  65%|██████▌   | 434961/665252 [07:42<01:23, 2770.80 examples/s]Map (num_proc=32):  65%|██████▌   | 435334/665252 [07:42<01:17, 2974.98 examples/s]Map (num_proc=32):  65%|██████▌   | 435684/665252 [07:42<01:14, 3095.13 examples/s]Map (num_proc=32):  66%|██████▌   | 436149/665252 [07:43<01:22, 2792.51 examples/s]Map (num_proc=32):  66%|██████▌   | 436525/665252 [07:43<01:16, 3006.89 examples/s]Map (num_proc=32):  66%|██████▌   | 436573/665252 [07:57<01:16, 3006.89 examples/s]Map (num_proc=32):  66%|██████▌   | 436752/665252 [07:58<52:13, 72.92 examples/s]  Map (num_proc=32):  66%|██████▌   | 437117/665252 [07:59<36:24, 104.41 examples/s]Map (num_proc=32):  66%|██████▌   | 437469/665252 [07:59<25:51, 146.86 examples/s]Map (num_proc=32):  66%|██████▌   | 437930/665252 [07:59<17:11, 220.37 examples/s]Map (num_proc=32):  66%|██████▌   | 438266/665252 [07:59<12:48, 295.23 examples/s]Map (num_proc=32):  66%|██████▌   | 438754/665252 [07:59<08:47, 428.99 examples/s]Map (num_proc=32):  66%|██████▌   | 439115/665252 [07:59<06:42, 561.64 examples/s]Map (num_proc=32):  66%|██████▌   | 439573/665252 [08:00<04:57, 758.20 examples/s]Map (num_proc=32):  66%|██████▌   | 439944/665252 [08:00<03:51, 973.50 examples/s]Map (num_proc=32):  66%|██████▌   | 440498/665252 [08:00<02:47, 1341.16 examples/s]Map (num_proc=32):  66%|██████▋   | 440918/665252 [08:00<02:32, 1473.64 examples/s]Map (num_proc=32):  66%|██████▋   | 441286/665252 [08:00<02:08, 1743.09 examples/s]Map (num_proc=32):  66%|██████▋   | 441744/665252 [08:00<01:58, 1878.97 examples/s]Map (num_proc=32):  66%|██████▋   | 442273/665252 [08:00<01:39, 2240.79 examples/s]Map (num_proc=32):  67%|██████▋   | 442749/665252 [08:01<01:38, 2253.07 examples/s]Map (num_proc=32):  67%|██████▋   | 443106/665252 [08:01<01:29, 2477.58 examples/s]Map (num_proc=32):  67%|██████▋   | 443573/665252 [08:01<01:31, 2426.02 examples/s]Map (num_proc=32):  67%|██████▋   | 443935/665252 [08:01<01:23, 2647.45 examples/s]Map (num_proc=32):  67%|██████▋   | 444468/665252 [08:01<01:15, 2908.33 examples/s]Map (num_proc=32):  67%|██████▋   | 444936/665252 [08:01<01:23, 2634.37 examples/s]Map (num_proc=32):  67%|██████▋   | 445295/665252 [08:02<01:18, 2818.78 examples/s]Map (num_proc=32):  67%|██████▋   | 445748/665252 [08:02<01:22, 2651.82 examples/s]Map (num_proc=32):  67%|██████▋   | 446109/665252 [08:02<01:16, 2850.15 examples/s]Map (num_proc=32):  67%|██████▋   | 446485/665252 [08:02<01:11, 3056.26 examples/s]Map (num_proc=32):  67%|██████▋   | 446922/665252 [08:02<01:18, 2775.92 examples/s]Map (num_proc=32):  67%|██████▋   | 447288/665252 [08:02<01:14, 2943.85 examples/s]Map (num_proc=32):  67%|██████▋   | 447745/665252 [08:02<01:21, 2671.44 examples/s]Map (num_proc=32):  67%|██████▋   | 448295/665252 [08:03<01:13, 2963.12 examples/s]Map (num_proc=32):  67%|██████▋   | 448751/665252 [08:03<01:18, 2747.10 examples/s]Map (num_proc=32):  68%|██████▊   | 449129/665252 [08:03<01:16, 2837.42 examples/s]Map (num_proc=32):  68%|██████▊   | 449573/665252 [08:03<01:16, 2832.40 examples/s]Map (num_proc=32):  68%|██████▊   | 449933/665252 [08:03<01:11, 2999.28 examples/s]Map (num_proc=32):  68%|██████▊   | 450298/665252 [08:03<01:08, 3151.80 examples/s]Map (num_proc=32):  68%|██████▊   | 450752/665252 [08:03<01:15, 2851.91 examples/s]Map (num_proc=32):  68%|██████▊   | 451131/665252 [08:04<01:09, 3063.03 examples/s]Map (num_proc=32):  68%|██████▊   | 451498/665252 [08:04<01:06, 3209.57 examples/s]Map (num_proc=32):  68%|██████▊   | 451949/665252 [08:04<01:55, 1851.52 examples/s]Map (num_proc=32):  68%|██████▊   | 452324/665252 [08:04<01:38, 2157.99 examples/s]Map (num_proc=32):  68%|██████▊   | 452746/665252 [08:04<01:37, 2186.36 examples/s]Map (num_proc=32):  68%|██████▊   | 453299/665252 [08:05<01:22, 2569.89 examples/s]Map (num_proc=32):  68%|██████▊   | 453751/665252 [08:05<01:25, 2478.47 examples/s]Map (num_proc=32):  68%|██████▊   | 454110/665252 [08:05<01:18, 2689.79 examples/s]Map (num_proc=32):  68%|██████▊   | 454489/665252 [08:05<01:12, 2922.41 examples/s]Map (num_proc=32):  68%|██████▊   | 454932/665252 [08:05<01:17, 2700.63 examples/s]Map (num_proc=32):  68%|██████▊   | 455322/665252 [08:05<01:11, 2955.62 examples/s]Map (num_proc=32):  69%|██████▊   | 455737/665252 [08:05<01:17, 2689.16 examples/s]Map (num_proc=32):  69%|██████▊   | 456120/665252 [08:06<01:11, 2937.69 examples/s]Map (num_proc=32):  69%|██████▊   | 456506/665252 [08:06<01:06, 3151.65 examples/s]Map (num_proc=32):  69%|██████▊   | 456955/665252 [08:06<01:13, 2837.51 examples/s]Map (num_proc=32):  69%|██████▊   | 457358/665252 [08:06<01:06, 3105.14 examples/s]Map (num_proc=32):  69%|██████▉   | 457362/665252 [08:17<01:06, 3105.14 examples/s]Map (num_proc=32):  69%|██████▉   | 457547/665252 [08:19<41:21, 83.69 examples/s]  Map (num_proc=32):  69%|██████▉   | 458081/665252 [08:19<25:09, 137.26 examples/s]Map (num_proc=32):  69%|██████▉   | 458566/665252 [08:20<17:02, 202.18 examples/s]Map (num_proc=32):  69%|██████▉   | 459160/665252 [08:20<10:56, 313.69 examples/s]Map (num_proc=32):  69%|██████▉   | 459553/665252 [08:20<08:29, 403.77 examples/s]Map (num_proc=32):  69%|██████▉   | 460121/665252 [08:20<05:49, 587.59 examples/s]Map (num_proc=32):  69%|██████▉   | 460531/665252 [08:20<04:40, 730.37 examples/s]Map (num_proc=32):  69%|██████▉   | 461019/665252 [08:20<03:32, 959.34 examples/s]Map (num_proc=32):  69%|██████▉   | 461362/665252 [08:21<03:05, 1098.97 examples/s]Map (num_proc=32):  69%|██████▉   | 461736/665252 [08:21<02:29, 1357.83 examples/s]Map (num_proc=32):  69%|██████▉   | 462106/665252 [08:21<02:03, 1643.66 examples/s]Map (num_proc=32):  70%|██████▉   | 462551/665252 [08:21<01:50, 1842.16 examples/s]Map (num_proc=32):  70%|██████▉   | 462941/665252 [08:21<01:33, 2170.38 examples/s]Map (num_proc=32):  70%|██████▉   | 463337/665252 [08:21<01:20, 2502.84 examples/s]Map (num_proc=32):  70%|██████▉   | 463755/665252 [08:21<01:21, 2483.89 examples/s]Map (num_proc=32):  70%|██████▉   | 464145/665252 [08:21<01:13, 2745.38 examples/s]Map (num_proc=32):  70%|██████▉   | 464556/665252 [08:22<01:14, 2692.63 examples/s]Map (num_proc=32):  70%|██████▉   | 464949/665252 [08:22<01:07, 2963.01 examples/s]Map (num_proc=32):  70%|██████▉   | 465362/665252 [08:22<01:12, 2759.47 examples/s]Map (num_proc=32):  70%|███████   | 465965/665252 [08:22<01:02, 3206.23 examples/s]Map (num_proc=32):  70%|███████   | 466362/665252 [08:22<01:06, 2973.38 examples/s]Map (num_proc=32):  70%|███████   | 466786/665252 [08:22<01:01, 3250.82 examples/s]Map (num_proc=32):  70%|███████   | 467218/665252 [08:22<00:56, 3502.03 examples/s]Map (num_proc=32):  70%|███████   | 467754/665252 [08:23<01:00, 3239.82 examples/s]Map (num_proc=32):  70%|███████   | 468180/665252 [08:23<00:56, 3464.54 examples/s]Map (num_proc=32):  70%|███████   | 468746/665252 [08:23<01:01, 3199.72 examples/s]Map (num_proc=32):  71%|███████   | 469362/665252 [08:23<01:03, 3097.56 examples/s]Map (num_proc=32):  71%|███████   | 469771/665252 [08:23<00:59, 3296.26 examples/s]Map (num_proc=32):  71%|███████   | 470195/665252 [08:23<00:55, 3491.60 examples/s]Map (num_proc=32):  71%|███████   | 470567/665252 [08:23<01:00, 3196.97 examples/s]Map (num_proc=32):  71%|███████   | 470993/665252 [08:24<00:57, 3403.83 examples/s]Map (num_proc=32):  71%|███████   | 471362/665252 [08:24<01:01, 3149.86 examples/s]Map (num_proc=32):  71%|███████   | 471802/665252 [08:24<00:56, 3451.70 examples/s]Map (num_proc=32):  71%|███████   | 472243/665252 [08:24<00:52, 3668.93 examples/s]Map (num_proc=32):  71%|███████   | 472793/665252 [08:24<01:30, 2125.60 examples/s]Map (num_proc=32):  71%|███████   | 473217/665252 [08:24<01:17, 2469.77 examples/s]Map (num_proc=32):  71%|███████   | 473574/665252 [08:25<01:17, 2470.45 examples/s]Map (num_proc=32):  71%|███████▏  | 474006/665252 [08:25<01:07, 2838.58 examples/s]Map (num_proc=32):  71%|███████▏  | 474362/665252 [08:25<01:08, 2779.71 examples/s]Map (num_proc=32):  71%|███████▏  | 474788/665252 [08:25<01:01, 3114.88 examples/s]Map (num_proc=32):  71%|███████▏  | 475222/665252 [08:25<00:55, 3412.27 examples/s]Map (num_proc=32):  72%|███████▏  | 475779/665252 [08:25<00:59, 3205.68 examples/s]Map (num_proc=32):  72%|███████▏  | 476213/665252 [08:25<00:54, 3463.15 examples/s]Map (num_proc=32):  72%|███████▏  | 476790/665252 [08:26<00:57, 3283.86 examples/s]Map (num_proc=32):  72%|███████▏  | 477240/665252 [08:26<00:52, 3552.85 examples/s]Map (num_proc=32):  72%|███████▏  | 477822/665252 [08:26<00:55, 3405.59 examples/s]Map (num_proc=32):  72%|███████▏  | 478151/665252 [08:38<00:54, 3405.59 examples/s]Map (num_proc=32):  72%|███████▏  | 478371/665252 [08:40<26:40, 116.77 examples/s] Map (num_proc=32):  72%|███████▏  | 479040/665252 [08:41<17:23, 178.47 examples/s]Map (num_proc=32):  72%|███████▏  | 479586/665252 [08:41<12:34, 246.17 examples/s]Map (num_proc=32):  72%|███████▏  | 480052/665252 [08:41<09:25, 327.51 examples/s]Map (num_proc=32):  72%|███████▏  | 480576/665252 [08:41<06:55, 444.88 examples/s]Map (num_proc=32):  72%|███████▏  | 481048/665252 [08:41<05:10, 593.45 examples/s]Map (num_proc=32):  72%|███████▏  | 481605/665252 [08:41<03:50, 795.44 examples/s]Map (num_proc=32):  72%|███████▏  | 482075/665252 [08:41<02:57, 1032.60 examples/s]Map (num_proc=32):  73%|███████▎  | 482620/665252 [08:42<02:20, 1303.98 examples/s]Map (num_proc=32):  73%|███████▎  | 483151/665252 [08:42<01:55, 1582.34 examples/s]Map (num_proc=32):  73%|███████▎  | 483617/665252 [08:42<01:33, 1935.27 examples/s]Map (num_proc=32):  73%|███████▎  | 484151/665252 [08:42<01:21, 2208.72 examples/s]Map (num_proc=32):  73%|███████▎  | 484627/665252 [08:42<01:37, 1845.73 examples/s]Map (num_proc=32):  73%|███████▎  | 486151/665252 [08:42<00:51, 3454.71 examples/s]Map (num_proc=32):  73%|███████▎  | 486884/665252 [08:43<00:47, 3755.24 examples/s]Map (num_proc=32):  73%|███████▎  | 487651/665252 [08:43<00:47, 3775.48 examples/s]Map (num_proc=32):  73%|███████▎  | 488145/665252 [08:43<00:44, 3975.04 examples/s]Map (num_proc=32):  73%|███████▎  | 488832/665252 [08:43<01:00, 2914.69 examples/s]Map (num_proc=32):  74%|███████▎  | 489394/665252 [08:43<00:58, 3028.23 examples/s]Map (num_proc=32):  74%|███████▎  | 489903/665252 [08:44<00:53, 3306.77 examples/s]Map (num_proc=32):  74%|███████▎  | 490384/665252 [08:44<00:52, 3309.25 examples/s]Map (num_proc=32):  74%|███████▍  | 490880/665252 [08:44<00:48, 3601.90 examples/s]Map (num_proc=32):  74%|███████▍  | 491394/665252 [08:44<00:50, 3444.97 examples/s]Map (num_proc=32):  74%|███████▍  | 492151/665252 [08:44<00:48, 3603.42 examples/s]Map (num_proc=32):  74%|███████▍  | 492950/665252 [08:44<00:42, 4098.89 examples/s]Map (num_proc=32):  74%|███████▍  | 493415/665252 [08:45<01:01, 2816.93 examples/s]Map (num_proc=32):  74%|███████▍  | 494151/665252 [08:45<00:54, 3143.52 examples/s]Map (num_proc=32):  74%|███████▍  | 494690/665252 [08:45<00:48, 3485.50 examples/s]Map (num_proc=32):  74%|███████▍  | 495151/665252 [08:45<00:55, 3074.99 examples/s]Map (num_proc=32):  75%|███████▍  | 496151/665252 [08:45<00:42, 3980.20 examples/s]Map (num_proc=32):  75%|███████▍  | 496684/665252 [08:45<00:39, 4244.82 examples/s]Map (num_proc=32):  75%|███████▍  | 497416/665252 [08:46<00:40, 4114.98 examples/s]Map (num_proc=32):  75%|███████▍  | 497982/665252 [08:46<00:37, 4433.61 examples/s]Map (num_proc=32):  75%|███████▍  | 498712/665252 [08:46<00:38, 4275.98 examples/s]Map (num_proc=32):  75%|███████▌  | 498940/665252 [08:58<00:38, 4275.98 examples/s]Map (num_proc=32):  75%|███████▌  | 499213/665252 [09:00<19:59, 138.43 examples/s] Map (num_proc=32):  75%|███████▌  | 499940/665252 [09:01<13:27, 204.77 examples/s]Map (num_proc=32):  75%|███████▌  | 500801/665252 [09:01<08:42, 314.57 examples/s]Map (num_proc=32):  75%|███████▌  | 501521/665252 [09:01<06:14, 436.97 examples/s]Map (num_proc=32):  75%|███████▌  | 502237/665252 [09:01<04:32, 598.58 examples/s]Map (num_proc=32):  76%|███████▌  | 502940/665252 [09:01<03:22, 801.60 examples/s]Map (num_proc=32):  76%|███████▌  | 503873/665252 [09:01<02:18, 1168.44 examples/s]Map (num_proc=32):  76%|███████▌  | 504567/665252 [09:02<01:48, 1477.09 examples/s]Map (num_proc=32):  76%|███████▌  | 505251/665252 [09:02<01:28, 1814.82 examples/s]Map (num_proc=32):  76%|███████▌  | 505897/665252 [09:02<01:10, 2260.48 examples/s]Map (num_proc=32):  76%|███████▌  | 506552/665252 [09:02<01:01, 2583.54 examples/s]Map (num_proc=32):  76%|███████▋  | 507284/665252 [09:02<00:52, 3026.77 examples/s]Map (num_proc=32):  76%|███████▋  | 507940/665252 [09:02<00:46, 3394.33 examples/s]Map (num_proc=32):  76%|███████▋  | 508592/665252 [09:02<00:39, 3936.45 examples/s]Map (num_proc=32):  77%|███████▋  | 509330/665252 [09:03<00:37, 4203.88 examples/s]Map (num_proc=32):  77%|███████▋  | 509940/665252 [09:03<00:35, 4344.77 examples/s]Map (num_proc=32):  77%|███████▋  | 510624/665252 [09:03<00:31, 4886.44 examples/s]Map (num_proc=32):  77%|███████▋  | 511288/665252 [09:03<00:31, 4922.49 examples/s]Map (num_proc=32):  77%|███████▋  | 511940/665252 [09:03<00:30, 5034.58 examples/s]Map (num_proc=32):  77%|███████▋  | 512669/665252 [09:03<00:27, 5564.51 examples/s]Map (num_proc=32):  77%|███████▋  | 513300/665252 [09:03<00:27, 5467.68 examples/s]Map (num_proc=32):  77%|███████▋  | 513940/665252 [09:04<00:44, 3396.26 examples/s]Map (num_proc=32):  77%|███████▋  | 514940/665252 [09:04<00:36, 4090.32 examples/s]Map (num_proc=32):  78%|███████▊  | 515940/665252 [09:04<00:31, 4735.68 examples/s]Map (num_proc=32):  78%|███████▊  | 516940/665252 [09:04<00:28, 5236.80 examples/s]Map (num_proc=32):  78%|███████▊  | 517951/665252 [09:04<00:26, 5652.55 examples/s]Map (num_proc=32):  78%|███████▊  | 518861/665252 [09:04<00:23, 6360.88 examples/s]Map (num_proc=32):  78%|███████▊  | 519729/665252 [09:04<00:23, 6287.33 examples/s]Map (num_proc=32):  78%|███████▊  | 519729/665252 [09:18<00:23, 6287.33 examples/s]Map (num_proc=32):  78%|███████▊  | 520196/665252 [09:23<17:17, 139.86 examples/s] Map (num_proc=32):  78%|███████▊  | 521372/665252 [09:23<10:29, 228.49 examples/s]Map (num_proc=32):  79%|███████▊  | 522247/665252 [09:23<07:25, 320.69 examples/s]Map (num_proc=32):  79%|███████▊  | 523115/665252 [09:23<05:18, 446.78 examples/s]Map (num_proc=32):  79%|███████▉  | 524114/665252 [09:23<03:39, 642.39 examples/s]Map (num_proc=32):  79%|███████▉  | 525115/665252 [09:23<02:36, 897.25 examples/s]Map (num_proc=32):  79%|███████▉  | 526113/665252 [09:23<01:53, 1226.50 examples/s]Map (num_proc=32):  79%|███████▉  | 527093/665252 [09:24<01:24, 1627.07 examples/s]Map (num_proc=32):  79%|███████▉  | 528114/665252 [09:24<01:05, 2102.27 examples/s]Map (num_proc=32):  80%|███████▉  | 529115/665252 [09:24<00:51, 2640.51 examples/s]Map (num_proc=32):  80%|███████▉  | 530105/665252 [09:24<00:41, 3218.68 examples/s]Map (num_proc=32):  80%|███████▉  | 531116/665252 [09:24<00:35, 3773.59 examples/s]Map (num_proc=32):  80%|███████▉  | 532118/665252 [09:24<00:30, 4308.66 examples/s]Map (num_proc=32):  80%|████████  | 533119/665252 [09:24<00:27, 4748.44 examples/s]Map (num_proc=32):  80%|████████  | 534121/665252 [09:25<00:26, 5022.04 examples/s]Map (num_proc=32):  80%|████████  | 535120/665252 [09:25<00:30, 4265.74 examples/s]Map (num_proc=32):  81%|████████  | 535729/665252 [09:25<00:28, 4479.99 examples/s]Map (num_proc=32):  81%|████████  | 536729/665252 [09:25<00:25, 5034.37 examples/s]Map (num_proc=32):  81%|████████  | 537527/665252 [09:25<00:22, 5594.69 examples/s]Map (num_proc=32):  81%|████████  | 538511/665252 [09:25<00:21, 5814.84 examples/s]Map (num_proc=32):  81%|████████  | 539525/665252 [09:26<00:20, 5997.80 examples/s]Map (num_proc=32):  81%|████████▏ | 540518/665252 [09:26<00:21, 5920.66 examples/s]Map (num_proc=32):  81%|████████▏ | 540518/665252 [09:38<00:21, 5920.66 examples/s]Map (num_proc=32):  81%|████████▏ | 540896/665252 [09:44<15:01, 137.89 examples/s] Map (num_proc=32):  81%|████████▏ | 541268/665252 [09:45<12:35, 164.05 examples/s]Map (num_proc=32):  82%|████████▏ | 542518/665252 [09:45<06:58, 293.33 examples/s]Map (num_proc=32):  82%|████████▏ | 543518/665252 [09:45<04:41, 433.01 examples/s]Map (num_proc=32):  82%|████████▏ | 544518/665252 [09:45<03:14, 621.93 examples/s]Map (num_proc=32):  82%|████████▏ | 545291/665252 [09:45<02:25, 821.86 examples/s]Map (num_proc=32):  82%|████████▏ | 546296/665252 [09:45<01:43, 1150.79 examples/s]Map (num_proc=32):  82%|████████▏ | 547298/665252 [09:45<01:15, 1555.95 examples/s]Map (num_proc=32):  82%|████████▏ | 548303/665252 [09:46<00:57, 2038.51 examples/s]Map (num_proc=32):  83%|████████▎ | 549299/665252 [09:46<00:45, 2575.23 examples/s]Map (num_proc=32):  83%|████████▎ | 550306/665252 [09:46<00:36, 3152.97 examples/s]Map (num_proc=32):  83%|████████▎ | 551310/665252 [09:46<00:30, 3735.93 examples/s]Map (num_proc=32):  83%|████████▎ | 552293/665252 [09:46<00:26, 4254.27 examples/s]Map (num_proc=32):  83%|████████▎ | 553306/665252 [09:46<00:24, 4629.92 examples/s]Map (num_proc=32):  83%|████████▎ | 554311/665252 [09:47<00:21, 5062.39 examples/s]Map (num_proc=32):  83%|████████▎ | 555303/665252 [09:47<00:20, 5394.78 examples/s]Map (num_proc=32):  84%|████████▎ | 556316/665252 [09:47<00:24, 4500.92 examples/s]Map (num_proc=32):  84%|████████▎ | 556902/665252 [09:47<00:23, 4677.36 examples/s]Map (num_proc=32):  84%|████████▍ | 557518/665252 [09:47<00:21, 4897.47 examples/s]Map (num_proc=32):  84%|████████▍ | 558518/665252 [09:47<00:19, 5358.00 examples/s]Map (num_proc=32):  84%|████████▍ | 559321/665252 [09:47<00:17, 5917.12 examples/s]Map (num_proc=32):  84%|████████▍ | 560321/665252 [09:48<00:17, 6072.50 examples/s]Map (num_proc=32):  84%|████████▍ | 561307/665252 [09:48<00:17, 5881.21 examples/s]Map (num_proc=32):  84%|████████▍ | 562044/665252 [10:06<11:00, 156.29 examples/s] Map (num_proc=32):  85%|████████▍ | 562698/665252 [10:06<08:19, 205.21 examples/s]Map (num_proc=32):  85%|████████▍ | 563307/665252 [10:06<06:19, 268.87 examples/s]Map (num_proc=32):  85%|████████▍ | 564307/665252 [10:06<04:03, 413.78 examples/s]Map (num_proc=32):  85%|████████▍ | 565087/665252 [10:06<02:55, 569.63 examples/s]Map (num_proc=32):  85%|████████▌ | 566083/665252 [10:06<01:59, 831.40 examples/s]Map (num_proc=32):  85%|████████▌ | 567082/665252 [10:07<01:23, 1170.25 examples/s]Map (num_proc=32):  85%|████████▌ | 568089/665252 [10:07<01:01, 1578.43 examples/s]Map (num_proc=32):  86%|████████▌ | 569077/665252 [10:07<00:46, 2057.53 examples/s]Map (num_proc=32):  86%|████████▌ | 570091/665252 [10:07<00:36, 2613.56 examples/s]Map (num_proc=32):  86%|████████▌ | 571079/665252 [10:07<00:29, 3178.74 examples/s]Map (num_proc=32):  86%|████████▌ | 572094/665252 [10:07<00:24, 3769.00 examples/s]Map (num_proc=32):  86%|████████▌ | 573083/665252 [10:08<00:21, 4318.74 examples/s]Map (num_proc=32):  86%|████████▋ | 574092/665252 [10:08<00:19, 4634.21 examples/s]Map (num_proc=32):  86%|████████▋ | 575097/665252 [10:08<00:17, 5047.67 examples/s]Map (num_proc=32):  87%|████████▋ | 576103/665252 [10:08<00:16, 5443.12 examples/s]Map (num_proc=32):  87%|████████▋ | 577095/665252 [10:08<00:20, 4320.95 examples/s]Map (num_proc=32):  87%|████████▋ | 577702/665252 [10:09<00:19, 4578.97 examples/s]Map (num_proc=32):  87%|████████▋ | 578307/665252 [10:09<00:18, 4760.39 examples/s]Map (num_proc=32):  87%|████████▋ | 579092/665252 [10:09<00:15, 5388.02 examples/s]Map (num_proc=32):  87%|████████▋ | 580110/665252 [10:09<00:14, 5737.92 examples/s]Map (num_proc=32):  87%|████████▋ | 581099/665252 [10:09<00:14, 5882.39 examples/s]Map (num_proc=32):  88%|████████▊ | 582096/665252 [10:09<00:14, 5813.99 examples/s]Map (num_proc=32):  88%|████████▊ | 582843/665252 [10:28<08:50, 155.42 examples/s] Map (num_proc=32):  88%|████████▊ | 583476/665252 [10:28<06:44, 201.96 examples/s]Map (num_proc=32):  88%|████████▊ | 584096/665252 [10:28<05:05, 265.58 examples/s]Map (num_proc=32):  88%|████████▊ | 584850/665252 [10:28<03:34, 374.70 examples/s]Map (num_proc=32):  88%|████████▊ | 585874/665252 [10:28<02:18, 574.34 examples/s]Map (num_proc=32):  88%|████████▊ | 586868/665252 [10:28<01:34, 830.95 examples/s]Map (num_proc=32):  88%|████████▊ | 587871/665252 [10:28<01:06, 1160.37 examples/s]Map (num_proc=32):  89%|████████▊ | 588877/665252 [10:29<00:48, 1565.49 examples/s]Map (num_proc=32):  89%|████████▊ | 589876/665252 [10:29<00:36, 2046.94 examples/s]Map (num_proc=32):  89%|████████▉ | 590875/665252 [10:29<00:28, 2583.68 examples/s]Map (num_proc=32):  89%|████████▉ | 591881/665252 [10:29<00:23, 3164.31 examples/s]Map (num_proc=32):  89%|████████▉ | 592883/665252 [10:29<00:19, 3732.56 examples/s]Map (num_proc=32):  89%|████████▉ | 593879/665252 [10:29<00:16, 4267.73 examples/s]Map (num_proc=32):  89%|████████▉ | 594871/665252 [10:30<00:15, 4556.57 examples/s]Map (num_proc=32):  90%|████████▉ | 595482/665252 [10:30<00:14, 4748.42 examples/s]Map (num_proc=32):  90%|████████▉ | 596096/665252 [10:30<00:13, 4999.47 examples/s]Map (num_proc=32):  90%|████████▉ | 596888/665252 [10:30<00:12, 5555.56 examples/s]Map (num_proc=32):  90%|████████▉ | 597888/665252 [10:30<00:15, 4353.86 examples/s]Map (num_proc=32):  90%|████████▉ | 598487/665252 [10:30<00:14, 4581.57 examples/s]Map (num_proc=32):  90%|█████████ | 599096/665252 [10:30<00:13, 4829.20 examples/s]Map (num_proc=32):  90%|█████████ | 600096/665252 [10:31<00:12, 5218.93 examples/s]Map (num_proc=32):  90%|█████████ | 601096/665252 [10:31<00:11, 5521.04 examples/s]Map (num_proc=32):  91%|█████████ | 602096/665252 [10:31<00:10, 5858.72 examples/s]Map (num_proc=32):  91%|█████████ | 602885/665252 [10:31<00:10, 5915.05 examples/s]Map (num_proc=32):  91%|█████████ | 602885/665252 [10:48<00:10, 5915.05 examples/s]Map (num_proc=32):  91%|█████████ | 603300/665252 [10:50<08:02, 128.42 examples/s] Map (num_proc=32):  91%|█████████ | 603329/665252 [10:50<07:58, 129.49 examples/s]Map (num_proc=32):  91%|█████████ | 603885/665252 [10:50<05:34, 183.47 examples/s]Map (num_proc=32):  91%|█████████ | 604721/665252 [10:50<03:20, 302.38 examples/s]Map (num_proc=32):  91%|█████████ | 605779/665252 [10:50<01:57, 506.92 examples/s]Map (num_proc=32):  91%|█████████ | 606426/665252 [10:50<01:27, 673.88 examples/s]Map (num_proc=32):  91%|█████████▏| 607065/665252 [10:51<01:05, 892.01 examples/s]Map (num_proc=32):  91%|█████████▏| 607885/665252 [10:51<00:45, 1255.24 examples/s]Map (num_proc=32):  92%|█████████▏| 608980/665252 [10:51<00:30, 1854.10 examples/s]Map (num_proc=32):  92%|█████████▏| 609870/665252 [10:51<00:22, 2461.19 examples/s]Map (num_proc=32):  92%|█████████▏| 610885/665252 [10:51<00:17, 3065.56 examples/s]Map (num_proc=32):  92%|█████████▏| 611887/665252 [10:51<00:14, 3710.08 examples/s]Map (num_proc=32):  92%|█████████▏| 612904/665252 [10:51<00:11, 4366.26 examples/s]Map (num_proc=32):  92%|█████████▏| 613808/665252 [10:51<00:10, 5072.57 examples/s]Map (num_proc=32):  92%|█████████▏| 614885/665252 [10:52<00:09, 5448.03 examples/s]Map (num_proc=32):  93%|█████████▎| 615770/665252 [10:52<00:08, 6045.29 examples/s]Map (num_proc=32):  93%|█████████▎| 616885/665252 [10:52<00:07, 6198.08 examples/s]Map (num_proc=32):  93%|█████████▎| 617885/665252 [10:52<00:10, 4400.97 examples/s]Map (num_proc=32):  93%|█████████▎| 618913/665252 [10:52<00:09, 4940.84 examples/s]Map (num_proc=32):  93%|█████████▎| 619837/665252 [10:53<00:08, 5634.49 examples/s]Map (num_proc=32):  93%|█████████▎| 620885/665252 [10:53<00:07, 5623.85 examples/s]Map (num_proc=32):  93%|█████████▎| 621845/665252 [10:53<00:06, 6259.17 examples/s]Map (num_proc=32):  94%|█████████▎| 622885/665252 [10:53<00:06, 6428.82 examples/s]Map (num_proc=32):  94%|█████████▍| 623674/665252 [10:53<00:06, 6578.18 examples/s]Map (num_proc=32):  94%|█████████▍| 623674/665252 [11:08<00:06, 6578.18 examples/s]Map (num_proc=32):  94%|█████████▍| 624095/665252 [11:09<04:14, 161.91 examples/s] Map (num_proc=32):  94%|█████████▍| 624674/665252 [11:09<03:12, 210.77 examples/s]Map (num_proc=32):  94%|█████████▍| 625360/665252 [11:09<02:18, 287.62 examples/s]Map (num_proc=32):  94%|█████████▍| 625941/665252 [11:10<01:46, 367.47 examples/s]Map (num_proc=32):  94%|█████████▍| 626370/665252 [11:10<01:26, 451.96 examples/s]Map (num_proc=32):  94%|█████████▍| 626800/665252 [11:10<01:10, 544.21 examples/s]Map (num_proc=32):  94%|█████████▍| 627230/665252 [11:10<00:55, 683.00 examples/s]Map (num_proc=32):  94%|█████████▍| 627654/665252 [11:10<00:44, 852.19 examples/s]Map (num_proc=32):  94%|█████████▍| 628073/665252 [11:11<00:38, 971.66 examples/s]Map (num_proc=32):  94%|█████████▍| 628481/665252 [11:11<00:31, 1178.06 examples/s]Map (num_proc=32):  95%|█████████▍| 628798/665252 [11:11<00:29, 1241.47 examples/s]Map (num_proc=32):  95%|█████████▍| 629055/665252 [11:11<00:26, 1385.77 examples/s]Map (num_proc=32):  95%|█████████▍| 629457/665252 [11:11<00:21, 1637.59 examples/s]Map (num_proc=32):  95%|█████████▍| 629804/665252 [11:11<00:22, 1600.23 examples/s]Map (num_proc=32):  95%|█████████▍| 630213/665252 [11:12<00:18, 1848.76 examples/s]Map (num_proc=32):  95%|█████████▍| 630497/665252 [11:12<00:17, 2015.02 examples/s]Map (num_proc=32):  95%|█████████▍| 630806/665252 [11:12<00:18, 1859.49 examples/s]Map (num_proc=32):  95%|█████████▍| 631081/665252 [11:12<00:17, 1991.44 examples/s]Map (num_proc=32):  95%|█████████▍| 631359/665252 [11:12<00:15, 2150.89 examples/s]Map (num_proc=32):  95%|█████████▍| 631626/665252 [11:12<00:14, 2260.33 examples/s]Map (num_proc=32):  95%|█████████▍| 631950/665252 [11:12<00:16, 1961.29 examples/s]Map (num_proc=32):  95%|█████████▌| 632233/665252 [11:13<00:15, 2147.15 examples/s]Map (num_proc=32):  95%|█████████▌| 632512/665252 [11:13<00:14, 2295.98 examples/s]Map (num_proc=32):  95%|█████████▌| 632803/665252 [11:13<00:16, 1941.11 examples/s]Map (num_proc=32):  95%|█████████▌| 633214/665252 [11:13<00:14, 2186.02 examples/s]Map (num_proc=32):  95%|█████████▌| 633490/665252 [11:13<00:13, 2310.19 examples/s]Map (num_proc=32):  95%|█████████▌| 633800/665252 [11:13<00:18, 1744.93 examples/s]Map (num_proc=32):  95%|█████████▌| 634076/665252 [11:13<00:16, 1940.24 examples/s]Map (num_proc=32):  95%|█████████▌| 634370/665252 [11:14<00:14, 2151.31 examples/s]Map (num_proc=32):  95%|█████████▌| 634643/665252 [11:14<00:13, 2264.98 examples/s]Map (num_proc=32):  95%|█████████▌| 634952/665252 [11:14<00:15, 1901.30 examples/s]Map (num_proc=32):  96%|█████████▌| 635337/665252 [11:14<00:13, 2222.41 examples/s]Map (num_proc=32):  96%|█████████▌| 635625/665252 [11:14<00:12, 2370.54 examples/s]Map (num_proc=32):  96%|█████████▌| 635934/665252 [11:14<00:14, 1983.94 examples/s]Map (num_proc=32):  96%|█████████▌| 636201/665252 [11:14<00:13, 2130.73 examples/s]Map (num_proc=32):  96%|█████████▌| 636481/665252 [11:15<00:12, 2286.50 examples/s]Map (num_proc=32):  96%|█████████▌| 636800/665252 [11:15<00:14, 1948.01 examples/s]Map (num_proc=32):  96%|█████████▌| 637076/665252 [11:15<00:13, 2119.31 examples/s]Map (num_proc=32):  96%|█████████▌| 637343/665252 [11:15<00:12, 2248.94 examples/s]Map (num_proc=32):  96%|█████████▌| 637615/665252 [11:15<00:11, 2366.68 examples/s]Map (num_proc=32):  96%|█████████▌| 637946/665252 [11:15<00:13, 1979.26 examples/s]Map (num_proc=32):  96%|█████████▌| 638217/665252 [11:15<00:12, 2137.14 examples/s]Map (num_proc=32):  96%|█████████▌| 638496/665252 [11:15<00:11, 2292.67 examples/s]Map (num_proc=32):  96%|█████████▌| 638818/665252 [11:16<00:17, 1475.19 examples/s]Map (num_proc=32):  96%|█████████▌| 639099/665252 [11:16<00:15, 1708.90 examples/s]Map (num_proc=32):  96%|█████████▌| 639374/665252 [11:16<00:13, 1916.18 examples/s]Map (num_proc=32):  96%|█████████▌| 639642/665252 [11:16<00:12, 2085.91 examples/s]Map (num_proc=32):  96%|█████████▌| 639952/665252 [11:16<00:13, 1843.89 examples/s]Map (num_proc=32):  96%|█████████▌| 640213/665252 [11:16<00:12, 2006.79 examples/s]Map (num_proc=32):  96%|█████████▋| 640495/665252 [11:17<00:11, 2195.74 examples/s]Map (num_proc=32):  96%|█████████▋| 640791/665252 [11:17<00:13, 1876.44 examples/s]Map (num_proc=32):  96%|█████████▋| 641052/665252 [11:17<00:11, 2037.35 examples/s]Map (num_proc=32):  96%|█████████▋| 641331/665252 [11:17<00:10, 2212.46 examples/s]Map (num_proc=32):  96%|█████████▋| 641593/665252 [11:17<00:10, 2314.44 examples/s]Map (num_proc=32):  96%|█████████▋| 641963/665252 [11:17<00:11, 1976.52 examples/s]Map (num_proc=32):  97%|█████████▋| 642239/665252 [11:17<00:10, 2147.48 examples/s]Map (num_proc=32):  97%|█████████▋| 642493/665252 [11:18<00:10, 2239.00 examples/s]Map (num_proc=32):  97%|█████████▋| 642804/665252 [11:18<00:11, 1906.79 examples/s]Map (num_proc=32):  97%|█████████▋| 643078/665252 [11:18<00:10, 2087.75 examples/s]Map (num_proc=32):  97%|█████████▋| 643360/665252 [11:18<00:09, 2261.26 examples/s]Map (num_proc=32):  97%|█████████▋| 643626/665252 [11:18<00:09, 2360.88 examples/s]Map (num_proc=32):  97%|█████████▋| 643941/665252 [11:18<00:10, 1949.04 examples/s]Map (num_proc=32):  97%|█████████▋| 644223/665252 [11:18<00:09, 2142.84 examples/s]Map (num_proc=32):  97%|█████████▋| 644463/665252 [11:19<00:10, 1939.00 examples/s]Map (num_proc=32):  97%|█████████▋| 644732/665252 [11:29<04:06, 83.18 examples/s]  Map (num_proc=32):  97%|█████████▋| 645003/665252 [11:29<02:52, 117.16 examples/s]Map (num_proc=32):  97%|█████████▋| 645278/665252 [11:29<02:01, 164.85 examples/s]Map (num_proc=32):  97%|█████████▋| 645589/665252 [11:30<01:24, 232.17 examples/s]Map (num_proc=32):  97%|█████████▋| 645864/665252 [11:30<01:01, 317.31 examples/s]Map (num_proc=32):  97%|█████████▋| 646149/665252 [11:30<00:44, 434.03 examples/s]Map (num_proc=32):  97%|█████████▋| 646413/665252 [11:30<00:33, 570.10 examples/s]Map (num_proc=32):  97%|█████████▋| 646749/665252 [11:30<00:25, 723.54 examples/s]Map (num_proc=32):  97%|█████████▋| 647016/665252 [11:30<00:20, 908.69 examples/s]Map (num_proc=32):  97%|█████████▋| 647280/665252 [11:30<00:16, 1115.65 examples/s]Map (num_proc=32):  97%|█████████▋| 647589/665252 [11:30<00:15, 1166.15 examples/s]Map (num_proc=32):  97%|█████████▋| 647868/665252 [11:31<00:12, 1405.76 examples/s]Map (num_proc=32):  97%|█████████▋| 648134/665252 [11:31<00:10, 1622.85 examples/s]Map (num_proc=32):  97%|█████████▋| 648410/665252 [11:31<00:09, 1844.89 examples/s]Map (num_proc=32):  98%|█████████▊| 648743/665252 [11:31<00:09, 1722.93 examples/s]Map (num_proc=32):  98%|█████████▊| 649034/665252 [11:31<00:08, 1958.28 examples/s]Map (num_proc=32):  98%|█████████▊| 649320/665252 [11:31<00:07, 2155.32 examples/s]Map (num_proc=32):  98%|█████████▊| 649601/665252 [11:31<00:08, 1886.36 examples/s]Map (num_proc=32):  98%|█████████▊| 649875/665252 [11:31<00:07, 2072.71 examples/s]Map (num_proc=32):  98%|█████████▊| 650147/665252 [11:32<00:06, 2225.95 examples/s]Map (num_proc=32):  98%|█████████▊| 650439/665252 [11:32<00:06, 2398.87 examples/s]Map (num_proc=32):  98%|█████████▊| 650743/665252 [11:32<00:07, 2014.40 examples/s]Map (num_proc=32):  98%|█████████▊| 651013/665252 [11:32<00:06, 2170.60 examples/s]Map (num_proc=32):  98%|█████████▊| 651297/665252 [11:32<00:05, 2332.89 examples/s]Map (num_proc=32):  98%|█████████▊| 651611/665252 [11:32<00:06, 1991.14 examples/s]Map (num_proc=32):  98%|█████████▊| 651877/665252 [11:32<00:06, 2138.69 examples/s]Map (num_proc=32):  98%|█████████▊| 652177/665252 [11:33<00:05, 2346.16 examples/s]Map (num_proc=32):  98%|█████████▊| 652455/665252 [11:33<00:05, 2455.62 examples/s]Map (num_proc=32):  98%|█████████▊| 652729/665252 [11:33<00:06, 1990.74 examples/s]Map (num_proc=32):  98%|█████████▊| 653010/665252 [11:33<00:05, 2177.38 examples/s]Map (num_proc=32):  98%|█████████▊| 653271/665252 [11:33<00:05, 2281.90 examples/s]Map (num_proc=32):  98%|█████████▊| 653589/665252 [11:33<00:06, 1717.69 examples/s]Map (num_proc=32):  98%|█████████▊| 653876/665252 [11:33<00:05, 1950.11 examples/s]Map (num_proc=32):  98%|█████████▊| 654155/665252 [11:33<00:05, 2136.80 examples/s]Map (num_proc=32):  98%|█████████▊| 654426/665252 [11:34<00:04, 2275.85 examples/s]Map (num_proc=32):  98%|█████████▊| 654730/665252 [11:34<00:05, 1937.68 examples/s]Map (num_proc=32):  98%|█████████▊| 655013/665252 [11:34<00:04, 2134.73 examples/s]Map (num_proc=32):  99%|█████████▊| 655286/665252 [11:34<00:04, 2275.71 examples/s]Map (num_proc=32):  99%|█████████▊| 655597/665252 [11:34<00:04, 1939.39 examples/s]Map (num_proc=32):  99%|█████████▊| 655877/665252 [11:34<00:04, 2129.24 examples/s]Map (num_proc=32):  99%|█████████▊| 656168/665252 [11:34<00:03, 2315.32 examples/s]Map (num_proc=32):  99%|█████████▊| 656445/665252 [11:35<00:03, 2428.19 examples/s]Map (num_proc=32):  99%|█████████▊| 656726/665252 [11:35<00:04, 1982.68 examples/s]Map (num_proc=32):  99%|█████████▉| 657015/665252 [11:35<00:03, 2189.46 examples/s]Map (num_proc=32):  99%|█████████▉| 657278/665252 [11:35<00:03, 2293.30 examples/s]Map (num_proc=32):  99%|█████████▉| 657587/665252 [11:35<00:03, 1938.05 examples/s]Map (num_proc=32):  99%|█████████▉| 657851/665252 [11:35<00:03, 2092.76 examples/s]Map (num_proc=32):  99%|█████████▉| 658121/665252 [11:35<00:03, 2237.63 examples/s]Map (num_proc=32):  99%|█████████▉| 658387/665252 [11:35<00:02, 2343.14 examples/s]Map (num_proc=32):  99%|█████████▉| 658732/665252 [11:36<00:03, 1972.64 examples/s]Map (num_proc=32):  99%|█████████▉| 659017/665252 [11:36<00:02, 2167.06 examples/s]Map (num_proc=32):  99%|█████████▉| 659285/665252 [11:36<00:02, 2290.12 examples/s]Map (num_proc=32):  99%|█████████▉| 659589/665252 [11:36<00:03, 1436.11 examples/s]Map (num_proc=32):  99%|█████████▉| 659876/665252 [11:36<00:03, 1686.54 examples/s]Map (num_proc=32):  99%|█████████▉| 660159/665252 [11:36<00:02, 1913.51 examples/s]Map (num_proc=32):  99%|█████████▉| 660431/665252 [11:37<00:02, 2084.70 examples/s]Map (num_proc=32):  99%|█████████▉| 660733/665252 [11:37<00:02, 1827.08 examples/s]Map (num_proc=32):  99%|█████████▉| 661006/665252 [11:37<00:02, 2018.00 examples/s]Map (num_proc=32):  99%|█████████▉| 661282/665252 [11:37<00:01, 2189.20 examples/s]Map (num_proc=32):  99%|█████████▉| 661597/665252 [11:37<00:01, 1887.97 examples/s]Map (num_proc=32):  99%|█████████▉| 661891/665252 [11:37<00:01, 2115.30 examples/s]Map (num_proc=32): 100%|█████████▉| 662174/665252 [11:37<00:01, 2281.67 examples/s]Map (num_proc=32): 100%|█████████▉| 662447/665252 [11:37<00:01, 2393.61 examples/s]Map (num_proc=32): 100%|█████████▉| 662743/665252 [11:38<00:01, 2021.42 examples/s]Map (num_proc=32): 100%|█████████▉| 663027/665252 [11:38<00:01, 2209.86 examples/s]Map (num_proc=32): 100%|█████████▉| 663299/665252 [11:38<00:00, 2333.92 examples/s]Map (num_proc=32): 100%|█████████▉| 663602/665252 [11:38<00:00, 1957.30 examples/s]Map (num_proc=32): 100%|█████████▉| 663878/665252 [11:38<00:00, 2135.25 examples/s]Map (num_proc=32): 100%|█████████▉| 664164/665252 [11:38<00:00, 2309.83 examples/s]Map (num_proc=32): 100%|█████████▉| 664438/665252 [11:38<00:00, 2418.42 examples/s]Map (num_proc=32): 100%|█████████▉| 664737/665252 [11:39<00:00, 1997.57 examples/s]Map (num_proc=32): 100%|█████████▉| 665014/665252 [11:39<00:00, 2171.76 examples/s]Map (num_proc=32): 100%|██████████| 665252/665252 [11:43<00:00, 945.82 examples/s] 
06/29 18:29:05 - mmengine - WARNING - Dataset LLaVADataset has no metainfo. ``dataset_meta`` in visualizer will be None.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.17s/it]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.96s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.50s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.90s/it]
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
Processing zero checkpoint './work_dirs/llava_v15_7b_pretrain_copy/iter_2000.pth'
Load Checkpoints:   0%|          | 0/8 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Load Checkpoints: 100%|██████████| 8/8 [00:00<00:00, 85.26it/s]
Detected checkpoint of type zero stage 2, world_size: 8
Parsing checkpoint created by deepspeed==0.14.4
Reconstructed state dict with 4 params 20979712 elements
Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  4.84s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:10<00:00,  5.31s/it]
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.32s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.18s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.25s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.22s/it]You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
Processing zero checkpoint './work_dirs/llava_v15_7b_pretrain_copy/iter_2000.pth'
Load Checkpoints:   0%|          | 0/8 [00:00<?, ?it/s]Load Checkpoints:  62%|██████▎   | 5/8 [00:00<00:00, 46.91it/s]Load Checkpoints: 100%|██████████| 8/8 [00:00<00:00, 54.77it/s]
Detected checkpoint of type zero stage 2, world_size: 8
Parsing checkpoint created by deepspeed==0.14.4
Reconstructed state dict with 4 params 20979712 elements
06/29 18:29:20 - mmengine - INFO - Load pretrained weight from ./work_dirs/llava_v15_7b_pretrain_copy/iter_2000.pth
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.54s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.95s/it]
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.46s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.87s/it]
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.51s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.92s/it]
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.48s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.89s/it]
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.19s/it]You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
Processing zero checkpoint './work_dirs/llava_v15_7b_pretrain_copy/iter_2000.pth'
Load Checkpoints:   0%|          | 0/8 [00:00<?, ?it/s]You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
Processing zero checkpoint './work_dirs/llava_v15_7b_pretrain_copy/iter_2000.pth'
Load Checkpoints:   0%|          | 0/8 [00:00<?, ?it/s]Load Checkpoints: 100%|██████████| 8/8 [00:00<00:00, 87.59it/s]
Detected checkpoint of type zero stage 2, world_size: 8
Parsing checkpoint created by deepspeed==0.14.4
Reconstructed state dict with 4 params 20979712 elements
Load Checkpoints: 100%|██████████| 8/8 [00:00<00:00, 87.94it/s]
Detected checkpoint of type zero stage 2, world_size: 8
Parsing checkpoint created by deepspeed==0.14.4
Reconstructed state dict with 4 params 20979712 elements
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
Processing zero checkpoint './work_dirs/llava_v15_7b_pretrain_copy/iter_2000.pth'
Load Checkpoints:   0%|          | 0/8 [00:00<?, ?it/s]You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
Processing zero checkpoint './work_dirs/llava_v15_7b_pretrain_copy/iter_2000.pth'
Load Checkpoints:   0%|          | 0/8 [00:00<?, ?it/s]Load Checkpoints: 100%|██████████| 8/8 [00:00<00:00, 85.79it/s]
Detected checkpoint of type zero stage 2, world_size: 8
Parsing checkpoint created by deepspeed==0.14.4
Load Checkpoints: 100%|██████████| 8/8 [00:00<00:00, 81.79it/s]
Detected checkpoint of type zero stage 2, world_size: 8
Reconstructed state dict with 4 params 20979712 elements
Parsing checkpoint created by deepspeed==0.14.4
Reconstructed state dict with 4 params 20979712 elements
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.44s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.85s/it]
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
Loading checkpoint shards:  50%|█████     | 1/2 [00:07<00:07,  7.07s/it]You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
Processing zero checkpoint './work_dirs/llava_v15_7b_pretrain_copy/iter_2000.pth'
Load Checkpoints:   0%|          | 0/8 [00:00<?, ?it/s]Load Checkpoints: 100%|██████████| 8/8 [00:00<00:00, 90.38it/s]
Detected checkpoint of type zero stage 2, world_size: 8
Parsing checkpoint created by deepspeed==0.14.4
Reconstructed state dict with 4 params 20979712 elements
Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.39s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:09<00:00,  4.80s/it]
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`. This was detected when initializing the generation config instance, which means the corresponding file may hold incorrect parameterization and should be fixed.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:540: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/transformers/generation/configuration_utils.py:545: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.
  warnings.warn(
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
Processing zero checkpoint './work_dirs/llava_v15_7b_pretrain_copy/iter_2000.pth'
Load Checkpoints:   0%|          | 0/8 [00:00<?, ?it/s]Load Checkpoints: 100%|██████████| 8/8 [00:00<00:00, 98.66it/s]
Detected checkpoint of type zero stage 2, world_size: 8
Parsing checkpoint created by deepspeed==0.14.4
Reconstructed state dict with 4 params 20979712 elements
[2024-06-29 18:29:56,210] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
[2024-06-29 18:30:14,097] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-06-29 18:30:14,099] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-06-29 18:30:14,099] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-06-29 18:30:14,173] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-06-29 18:30:14,173] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-06-29 18:30:14,173] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-06-29 18:30:14,174] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 500,000,000
[2024-06-29 18:30:14,174] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500,000,000
[2024-06-29 18:30:14,174] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-06-29 18:30:14,174] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2024-06-29 18:30:30,048] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-06-29 18:30:30,049] [INFO] [utils.py:782:see_memory_usage] MA 16.3 GB         Max_MA 17.88 GB         CA 20.02 GB         Max_CA 20 GB 
[2024-06-29 18:30:30,050] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 185.82 GB, percent = 18.5%
[2024-06-29 18:30:30,263] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-06-29 18:30:30,264] [INFO] [utils.py:782:see_memory_usage] MA 16.3 GB         Max_MA 19.45 GB         CA 23.17 GB         Max_CA 23 GB 
[2024-06-29 18:30:30,264] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 176.66 GB, percent = 17.5%
[2024-06-29 18:30:30,264] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-06-29 18:30:30,457] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-06-29 18:30:30,458] [INFO] [utils.py:782:see_memory_usage] MA 16.3 GB         Max_MA 16.3 GB         CA 23.17 GB         Max_CA 23 GB 
[2024-06-29 18:30:30,458] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 161.23 GB, percent = 16.0%
[2024-06-29 18:30:30,460] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-06-29 18:30:30,460] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-06-29 18:30:30,460] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-06-29 18:30:30,460] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[2e-05], mom=[(0.9, 0.999)]
[2024-06-29 18:30:30,462] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-06-29 18:30:30,462] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-06-29 18:30:30,462] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-06-29 18:30:30,462] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-06-29 18:30:30,462] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-06-29 18:30:30,463] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-06-29 18:30:30,463] [INFO] [config.py:1001:print]   bfloat16_enabled ............. True
[2024-06-29 18:30:30,463] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-06-29 18:30:30,463] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-06-29 18:30:30,463] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-06-29 18:30:30,463] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-06-29 18:30:30,463] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5ef94c9210>
[2024-06-29 18:30:30,463] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-06-29 18:30:30,463] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-06-29 18:30:30,463] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-06-29 18:30:30,463] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-06-29 18:30:30,463] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-06-29 18:30:30,463] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-06-29 18:30:30,463] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-06-29 18:30:30,463] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-06-29 18:30:30,463] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-06-29 18:30:30,463] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   fp16_auto_cast ............... None
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   fp16_enabled ................. False
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 1
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   gradient_clipping ............ 1
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-06-29 18:30:30,464] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 1
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   loss_scale ................... 1.0
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-06-29 18:30:30,465] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-06-29 18:30:30,466] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-06-29 18:30:30,876] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-06-29 18:30:30,876] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-06-29 18:30:30,876] [INFO] [config.py:1001:print]   steps_per_print .............. 10000000000000
[2024-06-29 18:30:30,876] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-06-29 18:30:30,876] [INFO] [config.py:1001:print]   train_batch_size ............. 128
[2024-06-29 18:30:30,876] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  16
[2024-06-29 18:30:30,877] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-06-29 18:30:30,877] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-06-29 18:30:30,877] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-06-29 18:30:30,877] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-06-29 18:30:30,877] [INFO] [config.py:1001:print]   world_size ................... 8
[2024-06-29 18:30:30,877] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-06-29 18:30:30,877] [INFO] [config.py:1001:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-06-29 18:30:30,877] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-06-29 18:30:30,877] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. False
[2024-06-29 18:30:30,877] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 2
[2024-06-29 18:30:30,877] [INFO] [config.py:987:print_user_config]   json = {
    "gradient_accumulation_steps": 1, 
    "train_micro_batch_size_per_gpu": 16, 
    "gradient_clipping": 1, 
    "zero_allow_untested_optimizer": true, 
    "zero_force_ds_cpu_optimizer": false, 
    "zero_optimization": {
        "stage": 2, 
        "overlap_comm": true
    }, 
    "fp16": {
        "enabled": false, 
        "initial_scale_power": 16
    }, 
    "bf16": {
        "enabled": true
    }, 
    "steps_per_print": 1.000000e+13
}
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 18:30:31 - mmengine - INFO - Num train samples 665252
06/29 18:30:31 - mmengine - INFO - train example:
06/29 18:30:31 - mmengine - INFO - <s> USER: <image>
What are the colors of the bus in the image? ASSISTANT: The bus in the image is white and red.</s> 
<s> USER: What feature can be seen on the back of the bus? ASSISTANT: The back of the bus features an advertisement.</s> 
<s> USER: Is the bus driving down the street or pulled off to the side? ASSISTANT: The bus is driving down the street, which is crowded with people and other vehicles.</s> 

06/29 18:30:31 - mmengine - INFO - before_train in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 18:30:33 - mmengine - INFO - Sample output:
USER: <image>
请描述一下这张照片 ASSISTANT:a wooden dock on a lake with a reflection of the sky</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/29 18:30:34 - mmengine - INFO - Sample output:
USER: <image>
Please describe this picture ASSISTANT:a wooden dock on a lake with a reflection of the sky</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 18:30:34 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
06/29 18:30:34 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
06/29 18:30:34 - mmengine - INFO - Checkpoints will be saved to /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy.
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/29 18:31:48 - mmengine - INFO - Iter(train) [  10/5198]  lr: 1.1690e-06  eta: 10:41:02  time: 7.4137  data_time: 0.0033  memory: 40353  loss: 1.3716
06/29 18:32:47 - mmengine - INFO - Iter(train) [  20/5198]  lr: 2.4677e-06  eta: 9:33:33  time: 5.8784  data_time: 0.0034  memory: 36582  loss: 1.2822
06/29 18:33:37 - mmengine - INFO - Iter(train) [  30/5198]  lr: 3.7664e-06  eta: 8:45:29  time: 5.0108  data_time: 0.0034  memory: 33723  loss: 1.3216
06/29 18:34:24 - mmengine - INFO - Iter(train) [  40/5198]  lr: 5.0651e-06  eta: 8:15:54  time: 4.7716  data_time: 0.0034  memory: 33039  loss: 1.1481
06/29 18:35:10 - mmengine - INFO - Iter(train) [  50/5198]  lr: 6.3638e-06  eta: 7:54:02  time: 4.5503  data_time: 0.0035  memory: 32408  loss: 0.9612
06/29 18:36:23 - mmengine - INFO - Iter(train) [  60/5198]  lr: 7.6625e-06  eta: 8:18:39  time: 7.3148  data_time: 0.0034  memory: 46807  loss: 0.9614
06/29 18:37:22 - mmengine - INFO - Iter(train) [  70/5198]  lr: 8.9611e-06  eta: 8:18:05  time: 5.8552  data_time: 0.0035  memory: 36561  loss: 0.9937
06/29 18:38:12 - mmengine - INFO - Iter(train) [  80/5198]  lr: 1.0260e-05  eta: 8:08:18  time: 5.0018  data_time: 0.0034  memory: 33681  loss: 1.1612
06/29 18:38:59 - mmengine - INFO - Iter(train) [  90/5198]  lr: 1.1559e-05  eta: 7:58:18  time: 4.7682  data_time: 0.0034  memory: 33029  loss: 0.8701
06/29 18:39:45 - mmengine - INFO - Iter(train) [ 100/5198]  lr: 1.2857e-05  eta: 7:47:59  time: 4.5151  data_time: 0.0034  memory: 32418  loss: 1.0838
06/29 18:40:58 - mmengine - INFO - Iter(train) [ 110/5198]  lr: 1.4156e-05  eta: 8:01:07  time: 7.3301  data_time: 0.0034  memory: 46807  loss: 0.9562
06/29 18:41:57 - mmengine - INFO - Iter(train) [ 120/5198]  lr: 1.5455e-05  eta: 8:01:43  time: 5.8920  data_time: 0.0035  memory: 36582  loss: 0.9841
06/29 18:42:47 - mmengine - INFO - Iter(train) [ 130/5198]  lr: 1.6753e-05  eta: 7:56:20  time: 5.0104  data_time: 0.0034  memory: 33681  loss: 1.0830
06/29 18:43:35 - mmengine - INFO - Iter(train) [ 140/5198]  lr: 1.8052e-05  eta: 7:50:15  time: 4.7850  data_time: 0.0034  memory: 33029  loss: 0.9516
06/29 18:44:20 - mmengine - INFO - Iter(train) [ 150/5198]  lr: 1.9351e-05  eta: 7:43:21  time: 4.5137  data_time: 0.0034  memory: 32408  loss: 0.8752
06/29 18:45:33 - mmengine - INFO - Iter(train) [ 160/5198]  lr: 2.0000e-05  eta: 7:51:52  time: 7.3041  data_time: 0.0035  memory: 46807  loss: 0.9528
06/29 18:46:31 - mmengine - INFO - Iter(train) [ 170/5198]  lr: 2.0000e-05  eta: 7:52:03  time: 5.8471  data_time: 0.0035  memory: 36499  loss: 0.9396
06/29 18:47:21 - mmengine - INFO - Iter(train) [ 180/5198]  lr: 1.9999e-05  eta: 7:48:08  time: 4.9941  data_time: 0.0034  memory: 33681  loss: 1.1257
06/29 18:48:09 - mmengine - INFO - Iter(train) [ 190/5198]  lr: 1.9998e-05  eta: 7:43:33  time: 4.7661  data_time: 0.0035  memory: 33029  loss: 0.9146
06/29 18:48:54 - mmengine - INFO - Iter(train) [ 200/5198]  lr: 1.9996e-05  eta: 7:38:12  time: 4.4916  data_time: 0.0035  memory: 32418  loss: 1.1524
06/29 18:50:07 - mmengine - INFO - Iter(train) [ 210/5198]  lr: 1.9994e-05  eta: 7:44:21  time: 7.2859  data_time: 0.0034  memory: 46807  loss: 0.9859
06/29 18:51:05 - mmengine - INFO - Iter(train) [ 220/5198]  lr: 1.9992e-05  eta: 7:44:18  time: 5.8196  data_time: 0.0035  memory: 36437  loss: 1.0398
06/29 18:51:55 - mmengine - INFO - Iter(train) [ 230/5198]  lr: 1.9989e-05  eta: 7:41:11  time: 4.9899  data_time: 0.0035  memory: 33671  loss: 1.1290
06/29 18:52:42 - mmengine - INFO - Iter(train) [ 240/5198]  lr: 1.9986e-05  eta: 7:37:29  time: 4.7621  data_time: 0.0034  memory: 33018  loss: 0.9679
06/29 18:53:27 - mmengine - INFO - Iter(train) [ 250/5198]  lr: 1.9983e-05  eta: 7:33:09  time: 4.5039  data_time: 0.0035  memory: 32418  loss: 1.0125
06/29 18:54:40 - mmengine - INFO - Iter(train) [ 260/5198]  lr: 1.9979e-05  eta: 7:37:55  time: 7.2923  data_time: 0.0035  memory: 46807  loss: 0.9479
06/29 18:55:39 - mmengine - INFO - Iter(train) [ 270/5198]  lr: 1.9975e-05  eta: 7:37:45  time: 5.8126  data_time: 0.0035  memory: 36437  loss: 0.9274
06/29 18:56:29 - mmengine - INFO - Iter(train) [ 280/5198]  lr: 1.9970e-05  eta: 7:35:09  time: 5.0003  data_time: 0.0034  memory: 33609  loss: 1.1124
06/29 18:57:16 - mmengine - INFO - Iter(train) [ 290/5198]  lr: 1.9965e-05  eta: 7:32:02  time: 4.7788  data_time: 0.0035  memory: 33018  loss: 0.9588
06/29 18:58:01 - mmengine - INFO - Iter(train) [ 300/5198]  lr: 1.9960e-05  eta: 7:28:21  time: 4.5114  data_time: 0.0034  memory: 32408  loss: 0.9557
06/29 18:59:14 - mmengine - INFO - Iter(train) [ 310/5198]  lr: 1.9954e-05  eta: 7:32:08  time: 7.2785  data_time: 0.0035  memory: 46807  loss: 0.9165
06/29 19:00:12 - mmengine - INFO - Iter(train) [ 320/5198]  lr: 1.9948e-05  eta: 7:31:48  time: 5.7851  data_time: 0.0035  memory: 36292  loss: 0.9630
06/29 19:01:02 - mmengine - INFO - Iter(train) [ 330/5198]  lr: 1.9941e-05  eta: 7:29:29  time: 4.9922  data_time: 0.0034  memory: 33640  loss: 1.0893
06/29 19:01:50 - mmengine - INFO - Iter(train) [ 340/5198]  lr: 1.9934e-05  eta: 7:26:42  time: 4.7591  data_time: 0.0034  memory: 33018  loss: 0.9800
06/29 19:02:34 - mmengine - INFO - Iter(train) [ 350/5198]  lr: 1.9927e-05  eta: 7:23:24  time: 4.4878  data_time: 0.0035  memory: 32408  loss: 0.9544
06/29 19:03:47 - mmengine - INFO - Iter(train) [ 360/5198]  lr: 1.9919e-05  eta: 7:26:28  time: 7.2622  data_time: 0.0034  memory: 46807  loss: 0.9289
06/29 19:04:45 - mmengine - INFO - Iter(train) [ 370/5198]  lr: 1.9911e-05  eta: 7:26:05  time: 5.7859  data_time: 0.0034  memory: 36313  loss: 0.9908
06/29 19:05:35 - mmengine - INFO - Iter(train) [ 380/5198]  lr: 1.9903e-05  eta: 7:24:00  time: 4.9960  data_time: 0.0035  memory: 33671  loss: 1.1204
06/29 19:06:23 - mmengine - INFO - Iter(train) [ 390/5198]  lr: 1.9894e-05  eta: 7:21:31  time: 4.7697  data_time: 0.0034  memory: 33029  loss: 0.9524
06/29 19:07:08 - mmengine - INFO - Iter(train) [ 400/5198]  lr: 1.9885e-05  eta: 7:18:35  time: 4.5019  data_time: 0.0034  memory: 32408  loss: 0.8461
06/29 19:08:21 - mmengine - INFO - Iter(train) [ 410/5198]  lr: 1.9875e-05  eta: 7:21:14  time: 7.3130  data_time: 0.0034  memory: 46807  loss: 0.9326
06/29 19:09:19 - mmengine - INFO - Iter(train) [ 420/5198]  lr: 1.9865e-05  eta: 7:20:58  time: 5.8720  data_time: 0.0034  memory: 36561  loss: 0.9361
06/29 19:10:10 - mmengine - INFO - Iter(train) [ 430/5198]  lr: 1.9855e-05  eta: 7:19:06  time: 5.0313  data_time: 0.0034  memory: 33754  loss: 1.1872
06/29 19:10:58 - mmengine - INFO - Iter(train) [ 440/5198]  lr: 1.9844e-05  eta: 7:16:50  time: 4.7825  data_time: 0.0034  memory: 33039  loss: 0.9403
06/29 19:11:43 - mmengine - INFO - Iter(train) [ 450/5198]  lr: 1.9833e-05  eta: 7:14:11  time: 4.5172  data_time: 0.0034  memory: 32418  loss: 0.9245
06/29 19:12:56 - mmengine - INFO - Iter(train) [ 460/5198]  lr: 1.9821e-05  eta: 7:16:23  time: 7.3029  data_time: 0.0034  memory: 46383  loss: 0.8765
06/29 19:13:55 - mmengine - INFO - Iter(train) [ 470/5198]  lr: 1.9809e-05  eta: 7:16:08  time: 5.9265  data_time: 0.0035  memory: 36613  loss: 0.9649
06/29 19:14:45 - mmengine - INFO - Iter(train) [ 480/5198]  lr: 1.9797e-05  eta: 7:14:23  time: 5.0303  data_time: 0.0034  memory: 33806  loss: 1.1689
06/29 19:15:33 - mmengine - INFO - Iter(train) [ 490/5198]  lr: 1.9784e-05  eta: 7:12:16  time: 4.7795  data_time: 0.0034  memory: 33049  loss: 0.9305
06/29 19:16:18 - mmengine - INFO - Iter(train) [ 500/5198]  lr: 1.9771e-05  eta: 7:09:46  time: 4.4915  data_time: 0.0034  memory: 32418  loss: 0.9671
06/29 19:16:18 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 19:16:20 - mmengine - INFO - Sample output:
USER: <image>
请描述一下这张照片 ASSISTANT:The image depicts a wooden pier extending over a lake, with a few wooden planks visible in the water. The pier appears to be a popular spot for people to relax and enjoy the serene view of the lake. The water appears calm, making it an ideal spot for visitors to appreciate the natural beauty of the environment.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 19:16:22 - mmengine - INFO - Sample output:
USER: <image>
Please describe this picture ASSISTANT:The image depicts a wooden pier extending over a lake, with a few wooden planks visible in the water. The pier appears to be quite long, providing a scenic view of the lake and its surroundings. The water appears calm, making it an ideal spot for visitors to relax and appreciate the natural beauty of the area.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 19:16:22 - mmengine - INFO - Saving checkpoint at 500 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-29 19:16:23,385] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_500.pth is about to be saved!
[2024-06-29 19:16:23,397] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/mp_rank_00_model_states.pt
[2024-06-29 19:16:23,397] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/mp_rank_00_model_states.pt...
[2024-06-29 19:16:37,929] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/mp_rank_00_model_states.pt.
[2024-06-29 19:16:37,932] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-29 19:16:37,932] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-29 19:16:37,932] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-29 19:16:37,932] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-29 19:16:37,932] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-29 19:16:37,932] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-29 19:16:37,932] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-29 19:16:37,932] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-29 19:16:49,734] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-29 19:16:49,734] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-29 19:16:49,734] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-29 19:16:49,821] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-29 19:16:49,821] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-29 19:16:49,821] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-29 19:16:49,918] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-29 19:16:49,919] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-29 19:16:49,919] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-29 19:16:49,977] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-29 19:16:49,977] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-29 19:16:49,977] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-29 19:16:50,009] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-29 19:16:50,009] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-29 19:16:50,009] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-29 19:16:50,021] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-29 19:16:50,021] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-29 19:16:50,021] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-29 19:16:50,038] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-29 19:16:50,038] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-29 19:16:50,038] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-29 19:16:50,049] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-29 19:16:50,049] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-29 19:16:50,049] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/29 19:18:02 - mmengine - INFO - Iter(train) [ 510/5198]  lr: 1.9758e-05  eta: 7:16:22  time: 10.3945  data_time: 3.1501  memory: 46807  loss: 0.8897
06/29 19:19:00 - mmengine - INFO - Iter(train) [ 520/5198]  lr: 1.9744e-05  eta: 7:15:47  time: 5.8257  data_time: 0.0035  memory: 36458  loss: 0.9450
06/29 19:19:50 - mmengine - INFO - Iter(train) [ 530/5198]  lr: 1.9730e-05  eta: 7:14:00  time: 5.0059  data_time: 0.0034  memory: 33712  loss: 1.0995
06/29 19:20:38 - mmengine - INFO - Iter(train) [ 540/5198]  lr: 1.9715e-05  eta: 7:11:54  time: 4.7623  data_time: 0.0034  memory: 33039  loss: 0.9391
06/29 19:21:23 - mmengine - INFO - Iter(train) [ 550/5198]  lr: 1.9700e-05  eta: 7:09:28  time: 4.4896  data_time: 0.0034  memory: 32418  loss: 0.9250
06/29 19:22:36 - mmengine - INFO - Iter(train) [ 560/5198]  lr: 1.9685e-05  eta: 7:10:57  time: 7.2936  data_time: 0.0035  memory: 46807  loss: 0.8803
06/29 19:23:34 - mmengine - INFO - Iter(train) [ 570/5198]  lr: 1.9669e-05  eta: 7:10:24  time: 5.8583  data_time: 0.0034  memory: 36478  loss: 0.9413
06/29 19:24:25 - mmengine - INFO - Iter(train) [ 580/5198]  lr: 1.9653e-05  eta: 7:08:45  time: 5.0284  data_time: 0.0034  memory: 33733  loss: 1.1292
06/29 19:25:13 - mmengine - INFO - Iter(train) [ 590/5198]  lr: 1.9637e-05  eta: 7:06:48  time: 4.7879  data_time: 0.0034  memory: 33039  loss: 0.7120
06/29 19:25:58 - mmengine - INFO - Iter(train) [ 600/5198]  lr: 1.9620e-05  eta: 7:04:33  time: 4.5229  data_time: 0.0035  memory: 32418  loss: 0.8789
06/29 19:27:11 - mmengine - INFO - Iter(train) [ 610/5198]  lr: 1.9603e-05  eta: 7:05:50  time: 7.3034  data_time: 0.0035  memory: 46807  loss: 0.8908
06/29 19:28:09 - mmengine - INFO - Iter(train) [ 620/5198]  lr: 1.9585e-05  eta: 7:05:16  time: 5.8666  data_time: 0.0035  memory: 36458  loss: 0.9122
06/29 19:29:00 - mmengine - INFO - Iter(train) [ 630/5198]  lr: 1.9567e-05  eta: 7:03:40  time: 5.0193  data_time: 0.0035  memory: 33744  loss: 1.1435
06/29 19:29:47 - mmengine - INFO - Iter(train) [ 640/5198]  lr: 1.9549e-05  eta: 7:01:49  time: 4.7812  data_time: 0.0034  memory: 33060  loss: 0.7824
06/29 19:30:33 - mmengine - INFO - Iter(train) [ 650/5198]  lr: 1.9530e-05  eta: 6:59:40  time: 4.5071  data_time: 0.0034  memory: 32428  loss: 0.9031
06/29 19:31:46 - mmengine - INFO - Iter(train) [ 660/5198]  lr: 1.9511e-05  eta: 7:00:47  time: 7.3128  data_time: 0.0035  memory: 46807  loss: 0.8593
06/29 19:32:44 - mmengine - INFO - Iter(train) [ 670/5198]  lr: 1.9492e-05  eta: 7:00:07  time: 5.7993  data_time: 0.0035  memory: 36406  loss: 0.8972
06/29 19:33:34 - mmengine - INFO - Iter(train) [ 680/5198]  lr: 1.9472e-05  eta: 6:58:33  time: 4.9851  data_time: 0.0034  memory: 33629  loss: 1.1162
06/29 19:34:21 - mmengine - INFO - Iter(train) [ 690/5198]  lr: 1.9452e-05  eta: 6:56:45  time: 4.7643  data_time: 0.0034  memory: 33029  loss: 0.8978
06/29 19:35:06 - mmengine - INFO - Iter(train) [ 700/5198]  lr: 1.9431e-05  eta: 6:54:42  time: 4.4932  data_time: 0.0035  memory: 32408  loss: 0.8213
06/29 19:36:19 - mmengine - INFO - Iter(train) [ 710/5198]  lr: 1.9410e-05  eta: 6:55:39  time: 7.3055  data_time: 0.0035  memory: 46807  loss: 0.8818
06/29 19:37:18 - mmengine - INFO - Iter(train) [ 720/5198]  lr: 1.9389e-05  eta: 6:55:03  time: 5.8670  data_time: 0.0034  memory: 36561  loss: 0.9350
06/29 19:38:08 - mmengine - INFO - Iter(train) [ 730/5198]  lr: 1.9367e-05  eta: 6:53:34  time: 5.0185  data_time: 0.0035  memory: 33702  loss: 1.1054
06/29 19:38:56 - mmengine - INFO - Iter(train) [ 740/5198]  lr: 1.9345e-05  eta: 6:51:52  time: 4.7776  data_time: 0.0034  memory: 33029  loss: 0.8936
06/29 19:39:41 - mmengine - INFO - Iter(train) [ 750/5198]  lr: 1.9323e-05  eta: 6:49:55  time: 4.5112  data_time: 0.0034  memory: 32397  loss: 0.9685
06/29 19:40:54 - mmengine - INFO - Iter(train) [ 760/5198]  lr: 1.9300e-05  eta: 6:50:44  time: 7.3203  data_time: 0.0034  memory: 46807  loss: 0.8608
06/29 19:41:53 - mmengine - INFO - Iter(train) [ 770/5198]  lr: 1.9277e-05  eta: 6:50:06  time: 5.8537  data_time: 0.0035  memory: 36510  loss: 0.9423
06/29 19:42:43 - mmengine - INFO - Iter(train) [ 780/5198]  lr: 1.9254e-05  eta: 6:48:40  time: 5.0127  data_time: 0.0034  memory: 33702  loss: 1.0976
06/29 19:43:30 - mmengine - INFO - Iter(train) [ 790/5198]  lr: 1.9230e-05  eta: 6:47:01  time: 4.7717  data_time: 0.0034  memory: 33039  loss: 0.8308
06/29 19:44:15 - mmengine - INFO - Iter(train) [ 800/5198]  lr: 1.9206e-05  eta: 6:45:08  time: 4.5017  data_time: 0.0034  memory: 32418  loss: 0.7651
06/29 19:45:28 - mmengine - INFO - Iter(train) [ 810/5198]  lr: 1.9181e-05  eta: 6:45:48  time: 7.2816  data_time: 0.0035  memory: 46807  loss: 0.8763
06/29 19:46:27 - mmengine - INFO - Iter(train) [ 820/5198]  lr: 1.9157e-05  eta: 6:45:07  time: 5.8282  data_time: 0.0034  memory: 36499  loss: 0.9538
06/29 19:47:17 - mmengine - INFO - Iter(train) [ 830/5198]  lr: 1.9131e-05  eta: 6:43:42  time: 4.9907  data_time: 0.0034  memory: 33692  loss: 1.0783
06/29 19:48:04 - mmengine - INFO - Iter(train) [ 840/5198]  lr: 1.9106e-05  eta: 6:42:06  time: 4.7634  data_time: 0.0034  memory: 33029  loss: 0.8817
06/29 19:48:49 - mmengine - INFO - Iter(train) [ 850/5198]  lr: 1.9080e-05  eta: 6:40:18  time: 4.4928  data_time: 0.0034  memory: 32408  loss: 0.9697
06/29 19:50:03 - mmengine - INFO - Iter(train) [ 860/5198]  lr: 1.9054e-05  eta: 6:40:54  time: 7.3437  data_time: 0.0035  memory: 46807  loss: 0.8520
06/29 19:51:01 - mmengine - INFO - Iter(train) [ 870/5198]  lr: 1.9027e-05  eta: 6:40:12  time: 5.8115  data_time: 0.0034  memory: 36447  loss: 0.9549
06/29 19:51:51 - mmengine - INFO - Iter(train) [ 880/5198]  lr: 1.9000e-05  eta: 6:38:50  time: 5.0068  data_time: 0.0034  memory: 33661  loss: 1.0402
06/29 19:52:39 - mmengine - INFO - Iter(train) [ 890/5198]  lr: 1.8973e-05  eta: 6:37:18  time: 4.7864  data_time: 0.0034  memory: 33039  loss: 0.7581
06/29 19:53:24 - mmengine - INFO - Iter(train) [ 900/5198]  lr: 1.8945e-05  eta: 6:35:34  time: 4.5267  data_time: 0.0034  memory: 32408  loss: 1.0950
06/29 19:54:37 - mmengine - INFO - Iter(train) [ 910/5198]  lr: 1.8917e-05  eta: 6:36:04  time: 7.3189  data_time: 0.0034  memory: 46807  loss: 0.8976
06/29 19:55:36 - mmengine - INFO - Iter(train) [ 920/5198]  lr: 1.8889e-05  eta: 6:35:24  time: 5.8733  data_time: 0.0035  memory: 36510  loss: 0.9518
06/29 19:56:26 - mmengine - INFO - Iter(train) [ 930/5198]  lr: 1.8860e-05  eta: 6:34:04  time: 5.0150  data_time: 0.0034  memory: 33733  loss: 1.0194
06/29 19:57:14 - mmengine - INFO - Iter(train) [ 940/5198]  lr: 1.8831e-05  eta: 6:32:34  time: 4.7761  data_time: 0.0034  memory: 33039  loss: 0.8059
06/29 19:57:59 - mmengine - INFO - Iter(train) [ 950/5198]  lr: 1.8801e-05  eta: 6:30:52  time: 4.4963  data_time: 0.0034  memory: 32408  loss: 0.9069
06/29 19:59:12 - mmengine - INFO - Iter(train) [ 960/5198]  lr: 1.8772e-05  eta: 6:31:15  time: 7.2920  data_time: 0.0035  memory: 46807  loss: 0.8544
06/29 20:00:09 - mmengine - INFO - Iter(train) [ 970/5198]  lr: 1.8742e-05  eta: 6:30:29  time: 5.7520  data_time: 0.0034  memory: 36427  loss: 0.8731
06/29 20:00:59 - mmengine - INFO - Iter(train) [ 980/5198]  lr: 1.8711e-05  eta: 6:29:11  time: 5.0042  data_time: 0.0034  memory: 33712  loss: 1.0356
06/29 20:01:47 - mmengine - INFO - Iter(train) [ 990/5198]  lr: 1.8680e-05  eta: 6:27:45  time: 4.8172  data_time: 0.0035  memory: 33039  loss: 0.8264
06/29 20:02:32 - mmengine - INFO - Exp name: llava_v15_7b_finetune_copy_20240629_180208
06/29 20:02:32 - mmengine - INFO - Iter(train) [1000/5198]  lr: 1.8649e-05  eta: 6:26:07  time: 4.5123  data_time: 0.0034  memory: 32408  loss: 0.7596
06/29 20:02:32 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 20:02:34 - mmengine - INFO - Sample output:
USER: <image>
请描述一下这张照片 ASSISTANT:The image features a pier extending over a lake, surrounded by trees. The water is calm and still, creating a serene atmosphere. The pier provides a perfect spot for relaxation and reflection, allowing visitors to enjoy the beauty of nature.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 20:02:35 - mmengine - INFO - Sample output:
USER: <image>
Please describe this picture ASSISTANT:The image features a pier extending over a body of water, possibly a lake or a pond. The water surface is calm and serene, creating a peaceful atmosphere. The pier provides a perfect spot for relaxation and reflection. The surrounding area is surrounded by trees, adding to the picturesque beauty of the scene.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 20:02:35 - mmengine - INFO - Saving checkpoint at 1000 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-29 20:02:37,076] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_1000.pth is about to be saved!
[2024-06-29 20:02:37,087] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/mp_rank_00_model_states.pt
[2024-06-29 20:02:37,088] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/mp_rank_00_model_states.pt...
[2024-06-29 20:02:51,582] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/mp_rank_00_model_states.pt.
[2024-06-29 20:02:51,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-29 20:02:51,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-29 20:02:51,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-29 20:02:51,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-29 20:02:51,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-29 20:02:51,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-29 20:02:51,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-29 20:02:51,584] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-29 20:03:03,708] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-29 20:03:03,708] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-29 20:03:03,708] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-29 20:03:03,754] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-29 20:03:03,754] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-29 20:03:03,754] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-29 20:03:03,815] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-29 20:03:03,815] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-29 20:03:03,815] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-29 20:03:03,822] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-29 20:03:03,822] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-29 20:03:03,822] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-29 20:03:03,851] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-29 20:03:03,851] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-29 20:03:03,851] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-29 20:03:03,920] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-29 20:03:03,920] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-29 20:03:03,920] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-29 20:03:03,972] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-29 20:03:03,972] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-29 20:03:03,972] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-29 20:03:04,011] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-29 20:03:04,011] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-29 20:03:04,012] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/29 20:04:16 - mmengine - INFO - Iter(train) [1010/5198]  lr: 1.8618e-05  eta: 6:28:34  time: 10.3887  data_time: 3.1154  memory: 46807  loss: 0.8337
06/29 20:05:15 - mmengine - INFO - Iter(train) [1020/5198]  lr: 1.8586e-05  eta: 6:27:49  time: 5.8405  data_time: 0.0034  memory: 36510  loss: 0.8903
06/29 20:06:05 - mmengine - INFO - Iter(train) [1030/5198]  lr: 1.8554e-05  eta: 6:26:31  time: 5.0156  data_time: 0.0035  memory: 33712  loss: 1.0035
06/29 20:06:52 - mmengine - INFO - Iter(train) [1040/5198]  lr: 1.8522e-05  eta: 6:25:03  time: 4.7635  data_time: 0.0034  memory: 33029  loss: 0.8381
06/29 20:07:38 - mmengine - INFO - Iter(train) [1050/5198]  lr: 1.8489e-05  eta: 6:23:27  time: 4.5199  data_time: 0.0034  memory: 32408  loss: 0.8536
06/29 20:08:51 - mmengine - INFO - Iter(train) [1060/5198]  lr: 1.8456e-05  eta: 6:23:42  time: 7.3502  data_time: 0.0034  memory: 46807  loss: 0.8466
06/29 20:09:50 - mmengine - INFO - Iter(train) [1070/5198]  lr: 1.8422e-05  eta: 6:23:00  time: 5.9185  data_time: 0.0034  memory: 36592  loss: 0.9253
06/29 20:10:41 - mmengine - INFO - Iter(train) [1080/5198]  lr: 1.8389e-05  eta: 6:21:44  time: 5.0282  data_time: 0.0035  memory: 33744  loss: 1.0716
06/29 20:11:28 - mmengine - INFO - Iter(train) [1090/5198]  lr: 1.8354e-05  eta: 6:20:18  time: 4.7778  data_time: 0.0034  memory: 33039  loss: 0.8697
06/29 20:12:13 - mmengine - INFO - Iter(train) [1100/5198]  lr: 1.8320e-05  eta: 6:18:44  time: 4.5016  data_time: 0.0034  memory: 32418  loss: 0.7563
06/29 20:13:27 - mmengine - INFO - Iter(train) [1110/5198]  lr: 1.8285e-05  eta: 6:18:53  time: 7.3147  data_time: 0.0035  memory: 46807  loss: 0.8856
06/29 20:14:25 - mmengine - INFO - Iter(train) [1120/5198]  lr: 1.8250e-05  eta: 6:18:09  time: 5.8619  data_time: 0.0034  memory: 36541  loss: 0.9083
06/29 20:15:15 - mmengine - INFO - Iter(train) [1130/5198]  lr: 1.8215e-05  eta: 6:16:54  time: 5.0242  data_time: 0.0034  memory: 33754  loss: 1.0396
06/29 20:16:03 - mmengine - INFO - Iter(train) [1140/5198]  lr: 1.8179e-05  eta: 6:15:30  time: 4.7781  data_time: 0.0035  memory: 33049  loss: 0.7333
06/29 20:16:48 - mmengine - INFO - Iter(train) [1150/5198]  lr: 1.8143e-05  eta: 6:13:58  time: 4.4937  data_time: 0.0035  memory: 32408  loss: 1.0286
06/29 20:18:01 - mmengine - INFO - Iter(train) [1160/5198]  lr: 1.8107e-05  eta: 6:14:03  time: 7.2974  data_time: 0.0034  memory: 46807  loss: 0.8680
06/29 20:19:00 - mmengine - INFO - Iter(train) [1170/5198]  lr: 1.8070e-05  eta: 6:13:18  time: 5.8601  data_time: 0.0034  memory: 36510  loss: 0.8915
06/29 20:19:50 - mmengine - INFO - Iter(train) [1180/5198]  lr: 1.8033e-05  eta: 6:12:04  time: 5.0185  data_time: 0.0035  memory: 33702  loss: 1.0538
06/29 20:20:38 - mmengine - INFO - Iter(train) [1190/5198]  lr: 1.7996e-05  eta: 6:10:42  time: 4.7727  data_time: 0.0035  memory: 33029  loss: 0.7497
06/29 20:21:23 - mmengine - INFO - Iter(train) [1200/5198]  lr: 1.7959e-05  eta: 6:09:12  time: 4.5054  data_time: 0.0034  memory: 32397  loss: 0.9232
06/29 20:22:36 - mmengine - INFO - Iter(train) [1210/5198]  lr: 1.7921e-05  eta: 6:09:16  time: 7.3571  data_time: 0.0034  memory: 46807  loss: 0.8217
06/29 20:23:34 - mmengine - INFO - Iter(train) [1220/5198]  lr: 1.7883e-05  eta: 6:08:29  time: 5.8079  data_time: 0.0035  memory: 36437  loss: 0.8905
06/29 20:24:24 - mmengine - INFO - Iter(train) [1230/5198]  lr: 1.7844e-05  eta: 6:07:15  time: 4.9989  data_time: 0.0034  memory: 33640  loss: 1.0288
06/29 20:25:12 - mmengine - INFO - Iter(train) [1240/5198]  lr: 1.7805e-05  eta: 6:05:54  time: 4.7640  data_time: 0.0034  memory: 33018  loss: 0.8059
06/29 20:25:57 - mmengine - INFO - Iter(train) [1250/5198]  lr: 1.7766e-05  eta: 6:04:26  time: 4.4996  data_time: 0.0034  memory: 32408  loss: 0.8483
06/29 20:27:10 - mmengine - INFO - Iter(train) [1260/5198]  lr: 1.7727e-05  eta: 6:04:26  time: 7.3006  data_time: 0.0035  memory: 46807  loss: 0.8405
06/29 20:28:08 - mmengine - INFO - Iter(train) [1270/5198]  lr: 1.7687e-05  eta: 6:03:38  time: 5.8136  data_time: 0.0034  memory: 36551  loss: 0.9635
06/29 20:28:58 - mmengine - INFO - Iter(train) [1280/5198]  lr: 1.7647e-05  eta: 6:02:25  time: 4.9951  data_time: 0.0035  memory: 33640  loss: 1.0605
06/29 20:29:46 - mmengine - INFO - Iter(train) [1290/5198]  lr: 1.7607e-05  eta: 6:01:06  time: 4.7755  data_time: 0.0035  memory: 33039  loss: 0.7732
06/29 20:30:31 - mmengine - INFO - Iter(train) [1300/5198]  lr: 1.7566e-05  eta: 5:59:40  time: 4.4988  data_time: 0.0035  memory: 32428  loss: 0.8568
06/29 20:31:44 - mmengine - INFO - Iter(train) [1310/5198]  lr: 1.7525e-05  eta: 5:59:37  time: 7.3198  data_time: 0.0034  memory: 46807  loss: 0.8497
06/29 20:32:43 - mmengine - INFO - Iter(train) [1320/5198]  lr: 1.7484e-05  eta: 5:58:52  time: 5.8976  data_time: 0.0034  memory: 36582  loss: 0.8965
06/29 20:33:33 - mmengine - INFO - Iter(train) [1330/5198]  lr: 1.7443e-05  eta: 5:57:41  time: 5.0290  data_time: 0.0035  memory: 33754  loss: 1.0391
06/29 20:34:21 - mmengine - INFO - Iter(train) [1340/5198]  lr: 1.7401e-05  eta: 5:56:24  time: 4.7903  data_time: 0.0034  memory: 33039  loss: 0.8292
06/29 20:35:06 - mmengine - INFO - Iter(train) [1350/5198]  lr: 1.7359e-05  eta: 5:54:59  time: 4.5149  data_time: 0.0034  memory: 32418  loss: 0.9315
06/29 20:36:20 - mmengine - INFO - Iter(train) [1360/5198]  lr: 1.7317e-05  eta: 5:54:56  time: 7.3693  data_time: 0.0035  memory: 46807  loss: 0.8423
06/29 20:37:19 - mmengine - INFO - Iter(train) [1370/5198]  lr: 1.7274e-05  eta: 5:54:10  time: 5.8996  data_time: 0.0036  memory: 36582  loss: 0.9246
06/29 20:38:09 - mmengine - INFO - Iter(train) [1380/5198]  lr: 1.7231e-05  eta: 5:52:59  time: 5.0064  data_time: 0.0034  memory: 33702  loss: 1.0763
06/29 20:38:57 - mmengine - INFO - Iter(train) [1390/5198]  lr: 1.7188e-05  eta: 5:51:43  time: 4.7721  data_time: 0.0035  memory: 33029  loss: 0.8186
06/29 20:39:42 - mmengine - INFO - Iter(train) [1400/5198]  lr: 1.7144e-05  eta: 5:50:19  time: 4.4980  data_time: 0.0035  memory: 32408  loss: 0.7312
06/29 20:40:55 - mmengine - INFO - Iter(train) [1410/5198]  lr: 1.7101e-05  eta: 5:50:11  time: 7.3102  data_time: 0.0035  memory: 46807  loss: 0.8483
06/29 20:41:53 - mmengine - INFO - Iter(train) [1420/5198]  lr: 1.7057e-05  eta: 5:49:24  time: 5.8397  data_time: 0.0035  memory: 36561  loss: 0.8942
06/29 20:42:43 - mmengine - INFO - Iter(train) [1430/5198]  lr: 1.7012e-05  eta: 5:48:14  time: 5.0019  data_time: 0.0035  memory: 33712  loss: 1.0853
06/29 20:43:31 - mmengine - INFO - Iter(train) [1440/5198]  lr: 1.6968e-05  eta: 5:46:58  time: 4.7656  data_time: 0.0035  memory: 33039  loss: 0.9198
06/29 20:44:16 - mmengine - INFO - Iter(train) [1450/5198]  lr: 1.6923e-05  eta: 5:45:36  time: 4.4966  data_time: 0.0034  memory: 32408  loss: 0.9597
06/29 20:45:29 - mmengine - INFO - Iter(train) [1460/5198]  lr: 1.6878e-05  eta: 5:45:25  time: 7.2864  data_time: 0.0035  memory: 46807  loss: 0.8474
06/29 20:46:27 - mmengine - INFO - Iter(train) [1470/5198]  lr: 1.6833e-05  eta: 5:44:37  time: 5.8451  data_time: 0.0035  memory: 36541  loss: 0.8699
06/29 20:47:17 - mmengine - INFO - Iter(train) [1480/5198]  lr: 1.6787e-05  eta: 5:43:28  time: 5.0150  data_time: 0.0034  memory: 33692  loss: 1.1059
06/29 20:48:05 - mmengine - INFO - Iter(train) [1490/5198]  lr: 1.6741e-05  eta: 5:42:14  time: 4.7877  data_time: 0.0034  memory: 33029  loss: 0.7617
06/29 20:48:50 - mmengine - INFO - Iter(train) [1500/5198]  lr: 1.6695e-05  eta: 5:40:54  time: 4.5120  data_time: 0.0035  memory: 32408  loss: 0.6623
06/29 20:48:50 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 20:48:52 - mmengine - INFO - Sample output:
USER: <image>
请描述一下这张照片 ASSISTANT:The image features a wooden pier extending over a calm lake. The pier is situated near a dock, which is connected to a pier house. The serene setting creates a peaceful atmosphere around the lake.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 20:48:53 - mmengine - INFO - Sample output:
USER: <image>
Please describe this picture ASSISTANT:The image features a wooden pier extending over a calm lake. The pier is situated near a dock, which is connected to a pier house. The serene setting creates a peaceful atmosphere around the lake.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 20:48:53 - mmengine - INFO - Saving checkpoint at 1500 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-29 20:48:59,635] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_1500.pth is about to be saved!
[2024-06-29 20:48:59,646] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/mp_rank_00_model_states.pt
[2024-06-29 20:48:59,646] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/mp_rank_00_model_states.pt...
[2024-06-29 20:49:14,257] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/mp_rank_00_model_states.pt.
[2024-06-29 20:49:14,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-29 20:49:14,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-29 20:49:14,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-29 20:49:14,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-29 20:49:14,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-29 20:49:14,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-29 20:49:14,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-29 20:49:14,260] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-29 20:49:26,266] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-29 20:49:26,267] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-29 20:49:26,267] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-29 20:49:26,270] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-29 20:49:26,270] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-29 20:49:26,270] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-29 20:49:26,278] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-29 20:49:26,278] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-29 20:49:26,279] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-29 20:49:26,297] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-29 20:49:26,297] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-29 20:49:26,297] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-29 20:49:26,345] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-29 20:49:26,345] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-29 20:49:26,345] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-29 20:49:26,361] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-29 20:49:26,361] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-29 20:49:26,361] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-29 20:49:26,383] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-29 20:49:26,383] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-29 20:49:26,383] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-29 20:49:26,435] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-29 20:49:26,435] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_1500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-29 20:49:26,435] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/29 20:50:39 - mmengine - INFO - Iter(train) [1510/5198]  lr: 1.6649e-05  eta: 5:42:08  time: 10.8328  data_time: 3.5575  memory: 46807  loss: 0.8129
06/29 20:51:37 - mmengine - INFO - Iter(train) [1520/5198]  lr: 1.6602e-05  eta: 5:41:19  time: 5.8423  data_time: 0.0034  memory: 36354  loss: 0.8825
06/29 20:52:27 - mmengine - INFO - Iter(train) [1530/5198]  lr: 1.6555e-05  eta: 5:40:09  time: 4.9971  data_time: 0.0034  memory: 33702  loss: 1.0470
06/29 20:53:15 - mmengine - INFO - Iter(train) [1540/5198]  lr: 1.6508e-05  eta: 5:38:54  time: 4.7506  data_time: 0.0034  memory: 33018  loss: 0.8180
06/29 20:53:59 - mmengine - INFO - Iter(train) [1550/5198]  lr: 1.6460e-05  eta: 5:37:34  time: 4.4873  data_time: 0.0035  memory: 32386  loss: 0.7160
06/29 20:55:12 - mmengine - INFO - Iter(train) [1560/5198]  lr: 1.6413e-05  eta: 5:37:18  time: 7.2751  data_time: 0.0034  memory: 46807  loss: 0.8231
06/29 20:56:10 - mmengine - INFO - Iter(train) [1570/5198]  lr: 1.6365e-05  eta: 5:36:28  time: 5.8005  data_time: 0.0035  memory: 36406  loss: 0.8804
06/29 20:57:00 - mmengine - INFO - Iter(train) [1580/5198]  lr: 1.6317e-05  eta: 5:35:19  time: 4.9834  data_time: 0.0034  memory: 33650  loss: 1.0365
06/29 20:57:48 - mmengine - INFO - Iter(train) [1590/5198]  lr: 1.6268e-05  eta: 5:34:05  time: 4.7472  data_time: 0.0034  memory: 33018  loss: 0.8727
06/29 20:58:32 - mmengine - INFO - Iter(train) [1600/5198]  lr: 1.6220e-05  eta: 5:32:46  time: 4.4935  data_time: 0.0034  memory: 32397  loss: 0.9503
06/29 20:59:45 - mmengine - INFO - Iter(train) [1610/5198]  lr: 1.6171e-05  eta: 5:32:29  time: 7.2984  data_time: 0.0034  memory: 46807  loss: 0.8305
06/29 21:00:44 - mmengine - INFO - Iter(train) [1620/5198]  lr: 1.6122e-05  eta: 5:31:40  time: 5.8633  data_time: 0.0035  memory: 36520  loss: 0.9025
06/29 21:01:34 - mmengine - INFO - Iter(train) [1630/5198]  lr: 1.6072e-05  eta: 5:30:33  time: 5.0272  data_time: 0.0034  memory: 33692  loss: 1.0716
06/29 21:02:22 - mmengine - INFO - Iter(train) [1640/5198]  lr: 1.6023e-05  eta: 5:29:21  time: 4.7930  data_time: 0.0034  memory: 33029  loss: 0.7662
06/29 21:03:08 - mmengine - INFO - Iter(train) [1650/5198]  lr: 1.5973e-05  eta: 5:28:03  time: 4.5229  data_time: 0.0034  memory: 32428  loss: 0.7578
06/29 21:04:21 - mmengine - INFO - Iter(train) [1660/5198]  lr: 1.5923e-05  eta: 5:27:46  time: 7.3348  data_time: 0.0035  memory: 46807  loss: 0.8328
06/29 21:05:20 - mmengine - INFO - Iter(train) [1670/5198]  lr: 1.5872e-05  eta: 5:26:56  time: 5.8649  data_time: 0.0034  memory: 36520  loss: 0.9165
06/29 21:06:10 - mmengine - INFO - Iter(train) [1680/5198]  lr: 1.5822e-05  eta: 5:25:49  time: 5.0031  data_time: 0.0034  memory: 33692  loss: 1.0209
06/29 21:06:57 - mmengine - INFO - Iter(train) [1690/5198]  lr: 1.5771e-05  eta: 5:24:37  time: 4.7708  data_time: 0.0035  memory: 33039  loss: 0.7220
06/29 21:07:42 - mmengine - INFO - Iter(train) [1700/5198]  lr: 1.5720e-05  eta: 5:23:20  time: 4.4984  data_time: 0.0034  memory: 32428  loss: 0.7709
06/29 21:08:55 - mmengine - INFO - Iter(train) [1710/5198]  lr: 1.5669e-05  eta: 5:22:59  time: 7.2510  data_time: 0.0034  memory: 46807  loss: 0.8413
06/29 21:09:52 - mmengine - INFO - Iter(train) [1720/5198]  lr: 1.5617e-05  eta: 5:22:08  time: 5.7586  data_time: 0.0035  memory: 36240  loss: 0.9472
06/29 21:10:42 - mmengine - INFO - Iter(train) [1730/5198]  lr: 1.5566e-05  eta: 5:21:01  time: 4.9824  data_time: 0.0034  memory: 33609  loss: 0.9767
06/29 21:11:30 - mmengine - INFO - Iter(train) [1740/5198]  lr: 1.5514e-05  eta: 5:19:50  time: 4.7740  data_time: 0.0033  memory: 33008  loss: 0.7712
06/29 21:12:15 - mmengine - INFO - Iter(train) [1750/5198]  lr: 1.5462e-05  eta: 5:18:34  time: 4.5044  data_time: 0.0034  memory: 32408  loss: 0.9887
06/29 21:13:40 - mmengine - INFO - Iter(train) [1760/5198]  lr: 1.5410e-05  eta: 5:18:36  time: 8.4805  data_time: 0.0035  memory: 40851  loss: 0.5497
06/29 21:14:46 - mmengine - INFO - Iter(train) [1770/5198]  lr: 1.5357e-05  eta: 5:18:02  time: 6.6697  data_time: 0.0034  memory: 39110  loss: 0.4240
06/29 21:15:36 - mmengine - INFO - Iter(train) [1780/5198]  lr: 1.5304e-05  eta: 5:16:53  time: 4.9086  data_time: 0.0034  memory: 34562  loss: 0.3454
06/29 21:16:15 - mmengine - INFO - Iter(train) [1790/5198]  lr: 1.5251e-05  eta: 5:15:26  time: 3.9078  data_time: 0.0034  memory: 31382  loss: 0.2275
06/29 21:16:49 - mmengine - INFO - Iter(train) [1800/5198]  lr: 1.5198e-05  eta: 5:13:50  time: 3.4175  data_time: 0.0034  memory: 29724  loss: 0.1733
06/29 21:18:02 - mmengine - INFO - Iter(train) [1810/5198]  lr: 1.5145e-05  eta: 5:13:29  time: 7.3422  data_time: 0.0034  memory: 46807  loss: 0.8371
06/29 21:19:01 - mmengine - INFO - Iter(train) [1820/5198]  lr: 1.5091e-05  eta: 5:12:39  time: 5.8666  data_time: 0.0034  memory: 36561  loss: 0.8780
06/29 21:19:51 - mmengine - INFO - Iter(train) [1830/5198]  lr: 1.5038e-05  eta: 5:11:33  time: 5.0124  data_time: 0.0035  memory: 33712  loss: 1.1022
06/29 21:20:39 - mmengine - INFO - Iter(train) [1840/5198]  lr: 1.4984e-05  eta: 5:10:24  time: 4.7763  data_time: 0.0034  memory: 33039  loss: 0.8007
06/29 21:21:24 - mmengine - INFO - Iter(train) [1850/5198]  lr: 1.4930e-05  eta: 5:09:09  time: 4.4971  data_time: 0.0034  memory: 32418  loss: 0.8171
06/29 21:22:37 - mmengine - INFO - Iter(train) [1860/5198]  lr: 1.4875e-05  eta: 5:08:45  time: 7.2982  data_time: 0.0035  memory: 46807  loss: 0.8146
06/29 21:23:35 - mmengine - INFO - Iter(train) [1870/5198]  lr: 1.4821e-05  eta: 5:07:55  time: 5.8387  data_time: 0.0035  memory: 36458  loss: 0.8830
06/29 21:24:25 - mmengine - INFO - Iter(train) [1880/5198]  lr: 1.4766e-05  eta: 5:06:50  time: 4.9934  data_time: 0.0034  memory: 33650  loss: 1.0519
06/29 21:25:13 - mmengine - INFO - Iter(train) [1890/5198]  lr: 1.4711e-05  eta: 5:05:41  time: 4.7685  data_time: 0.0034  memory: 33018  loss: 0.7500
06/29 21:25:58 - mmengine - INFO - Iter(train) [1900/5198]  lr: 1.4656e-05  eta: 5:04:27  time: 4.5048  data_time: 0.0034  memory: 32418  loss: 0.8534
06/29 21:27:11 - mmengine - INFO - Iter(train) [1910/5198]  lr: 1.4601e-05  eta: 5:04:02  time: 7.3051  data_time: 0.0035  memory: 46807  loss: 0.8208
06/29 21:28:09 - mmengine - INFO - Iter(train) [1920/5198]  lr: 1.4546e-05  eta: 5:03:12  time: 5.8576  data_time: 0.0034  memory: 36520  loss: 0.8310
06/29 21:29:00 - mmengine - INFO - Iter(train) [1930/5198]  lr: 1.4490e-05  eta: 5:02:07  time: 5.0164  data_time: 0.0035  memory: 33744  loss: 1.0520
06/29 21:29:47 - mmengine - INFO - Iter(train) [1940/5198]  lr: 1.4434e-05  eta: 5:00:59  time: 4.7819  data_time: 0.0035  memory: 33029  loss: 0.7595
06/29 21:30:33 - mmengine - INFO - Iter(train) [1950/5198]  lr: 1.4378e-05  eta: 4:59:46  time: 4.5107  data_time: 0.0034  memory: 32397  loss: 0.6975
06/29 21:31:46 - mmengine - INFO - Iter(train) [1960/5198]  lr: 1.4322e-05  eta: 4:59:21  time: 7.3545  data_time: 0.0034  memory: 46807  loss: 0.8230
06/29 21:32:45 - mmengine - INFO - Iter(train) [1970/5198]  lr: 1.4266e-05  eta: 4:58:31  time: 5.8989  data_time: 0.0035  memory: 36572  loss: 0.8394
06/29 21:33:35 - mmengine - INFO - Iter(train) [1980/5198]  lr: 1.4210e-05  eta: 4:57:27  time: 5.0210  data_time: 0.0034  memory: 33723  loss: 1.0430
06/29 21:34:23 - mmengine - INFO - Iter(train) [1990/5198]  lr: 1.4153e-05  eta: 4:56:19  time: 4.7823  data_time: 0.0034  memory: 33049  loss: 0.8364
06/29 21:35:08 - mmengine - INFO - Exp name: llava_v15_7b_finetune_copy_20240629_180208
06/29 21:35:08 - mmengine - INFO - Iter(train) [2000/5198]  lr: 1.4096e-05  eta: 4:55:07  time: 4.5035  data_time: 0.0034  memory: 32428  loss: 0.7144
06/29 21:35:08 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 21:35:13 - mmengine - INFO - Sample output:
USER: <image>
请描述一下这张照片 ASSISTANT:这张照片是一个美丽的湖景，展现了一个小木桥横跨湖水，提供了一个漂亮的景观。桥的两端分别是一个小山和一个小山，背景中还有一个远处的山脉。

水面上有许多小船，让人感到这是一个适合游船的湖。整个景观充满了舒适和恬静的氛围，让人感到放松和愉悦。</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 21:35:14 - mmengine - INFO - Sample output:
USER: <image>
Please describe this picture ASSISTANT:The image features a wooden dock extending over a calm lake, providing a peaceful atmosphere. The dock is situated near a mountain range, adding a picturesque backdrop to the scene. The water is clear and reflective, creating a serene environment.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 21:35:14 - mmengine - INFO - Saving checkpoint at 2000 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-29 21:35:20,838] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_2000.pth is about to be saved!
[2024-06-29 21:35:20,849] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/mp_rank_00_model_states.pt
[2024-06-29 21:35:20,849] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/mp_rank_00_model_states.pt...
[2024-06-29 21:35:35,412] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/mp_rank_00_model_states.pt.
[2024-06-29 21:35:35,414] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-29 21:35:35,414] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-29 21:35:35,414] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-29 21:35:35,414] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-29 21:35:35,414] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-29 21:35:35,414] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-29 21:35:35,414] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-29 21:35:35,415] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-29 21:35:47,602] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-29 21:35:47,602] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-29 21:35:47,603] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-29 21:35:47,607] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-29 21:35:47,607] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-29 21:35:47,607] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-29 21:35:47,629] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-29 21:35:47,629] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-29 21:35:47,629] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-29 21:35:47,632] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-29 21:35:47,632] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-29 21:35:47,632] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-29 21:35:47,637] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-29 21:35:47,637] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-29 21:35:47,637] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-29 21:35:47,690] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-29 21:35:47,690] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-29 21:35:47,690] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-29 21:35:47,740] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-29 21:35:47,740] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-29 21:35:47,740] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-29 21:35:47,742] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-29 21:35:47,742] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-29 21:35:47,742] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/29 21:37:00 - mmengine - INFO - Iter(train) [2010/5198]  lr: 1.4039e-05  eta: 4:55:42  time: 11.1963  data_time: 3.9162  memory: 46807  loss: 0.8416
06/29 21:37:58 - mmengine - INFO - Iter(train) [2020/5198]  lr: 1.3982e-05  eta: 4:54:50  time: 5.8307  data_time: 0.0035  memory: 36510  loss: 0.8666
06/29 21:38:48 - mmengine - INFO - Iter(train) [2030/5198]  lr: 1.3925e-05  eta: 4:53:46  time: 5.0013  data_time: 0.0035  memory: 33692  loss: 1.0501
06/29 21:39:36 - mmengine - INFO - Iter(train) [2040/5198]  lr: 1.3868e-05  eta: 4:52:38  time: 4.7779  data_time: 0.0034  memory: 33039  loss: 0.6973
06/29 21:40:21 - mmengine - INFO - Iter(train) [2050/5198]  lr: 1.3810e-05  eta: 4:51:26  time: 4.5081  data_time: 0.0034  memory: 32438  loss: 0.8237
06/29 21:41:45 - mmengine - INFO - Iter(train) [2060/5198]  lr: 1.3753e-05  eta: 4:51:14  time: 8.4147  data_time: 0.0035  memory: 40851  loss: 0.4641
06/29 21:42:50 - mmengine - INFO - Iter(train) [2070/5198]  lr: 1.3695e-05  eta: 4:50:33  time: 6.4903  data_time: 0.0035  memory: 38665  loss: 0.4247
06/29 21:43:38 - mmengine - INFO - Iter(train) [2080/5198]  lr: 1.3637e-05  eta: 4:49:25  time: 4.7833  data_time: 0.0034  memory: 34262  loss: 0.3192
06/29 21:44:17 - mmengine - INFO - Iter(train) [2090/5198]  lr: 1.3579e-05  eta: 4:48:04  time: 3.8813  data_time: 0.0035  memory: 31216  loss: 0.2298
06/29 21:44:51 - mmengine - INFO - Iter(train) [2100/5198]  lr: 1.3520e-05  eta: 4:46:37  time: 3.4203  data_time: 0.0034  memory: 29724  loss: 0.1343
06/29 21:46:05 - mmengine - INFO - Iter(train) [2110/5198]  lr: 1.3462e-05  eta: 4:46:08  time: 7.3378  data_time: 0.0034  memory: 46807  loss: 0.8206
06/29 21:47:04 - mmengine - INFO - Iter(train) [2120/5198]  lr: 1.3404e-05  eta: 4:45:17  time: 5.9018  data_time: 0.0035  memory: 36551  loss: 0.9046
06/29 21:47:54 - mmengine - INFO - Iter(train) [2130/5198]  lr: 1.3345e-05  eta: 4:44:14  time: 5.0234  data_time: 0.0035  memory: 33733  loss: 1.0347
06/29 21:48:41 - mmengine - INFO - Iter(train) [2140/5198]  lr: 1.3286e-05  eta: 4:43:07  time: 4.7683  data_time: 0.0035  memory: 33018  loss: 0.7280
06/29 21:49:26 - mmengine - INFO - Iter(train) [2150/5198]  lr: 1.3227e-05  eta: 4:41:56  time: 4.4967  data_time: 0.0035  memory: 32397  loss: 1.0216
06/29 21:50:39 - mmengine - INFO - Iter(train) [2160/5198]  lr: 1.3168e-05  eta: 4:41:25  time: 7.3038  data_time: 0.0036  memory: 46807  loss: 0.8271
06/29 21:51:38 - mmengine - INFO - Iter(train) [2170/5198]  lr: 1.3109e-05  eta: 4:40:34  time: 5.8849  data_time: 0.0035  memory: 36551  loss: 0.9101
06/29 21:52:29 - mmengine - INFO - Iter(train) [2180/5198]  lr: 1.3050e-05  eta: 4:39:31  time: 5.0253  data_time: 0.0035  memory: 33764  loss: 0.9970
06/29 21:53:16 - mmengine - INFO - Iter(train) [2190/5198]  lr: 1.2990e-05  eta: 4:38:25  time: 4.7693  data_time: 0.0035  memory: 33049  loss: 0.7078
06/29 21:54:01 - mmengine - INFO - Iter(train) [2200/5198]  lr: 1.2931e-05  eta: 4:37:15  time: 4.4944  data_time: 0.0035  memory: 32408  loss: 0.7879
06/29 21:55:14 - mmengine - INFO - Iter(train) [2210/5198]  lr: 1.2871e-05  eta: 4:36:43  time: 7.2827  data_time: 0.0035  memory: 46807  loss: 0.8263
06/29 21:56:12 - mmengine - INFO - Iter(train) [2220/5198]  lr: 1.2812e-05  eta: 4:35:51  time: 5.8100  data_time: 0.0035  memory: 36375  loss: 0.8635
06/29 21:57:02 - mmengine - INFO - Iter(train) [2230/5198]  lr: 1.2752e-05  eta: 4:34:48  time: 5.0101  data_time: 0.0035  memory: 33712  loss: 1.0258
06/29 21:57:50 - mmengine - INFO - Iter(train) [2240/5198]  lr: 1.2692e-05  eta: 4:33:42  time: 4.7754  data_time: 0.0035  memory: 33039  loss: 0.6941
06/29 21:58:35 - mmengine - INFO - Iter(train) [2250/5198]  lr: 1.2632e-05  eta: 4:32:33  time: 4.5154  data_time: 0.0035  memory: 32408  loss: 0.8327
06/29 21:59:49 - mmengine - INFO - Iter(train) [2260/5198]  lr: 1.2572e-05  eta: 4:32:01  time: 7.3421  data_time: 0.0035  memory: 46807  loss: 0.8201
06/29 22:00:46 - mmengine - INFO - Iter(train) [2270/5198]  lr: 1.2511e-05  eta: 4:31:08  time: 5.7869  data_time: 0.0035  memory: 36510  loss: 0.8472
06/29 22:01:37 - mmengine - INFO - Iter(train) [2280/5198]  lr: 1.2451e-05  eta: 4:30:06  time: 5.0521  data_time: 0.0035  memory: 33661  loss: 1.0270
06/29 22:02:25 - mmengine - INFO - Iter(train) [2290/5198]  lr: 1.2391e-05  eta: 4:29:01  time: 4.7928  data_time: 0.0035  memory: 33039  loss: 0.8623
06/29 22:03:10 - mmengine - INFO - Iter(train) [2300/5198]  lr: 1.2330e-05  eta: 4:27:53  time: 4.5322  data_time: 0.0035  memory: 32418  loss: 0.7168
06/29 22:04:23 - mmengine - INFO - Iter(train) [2310/5198]  lr: 1.2269e-05  eta: 4:27:19  time: 7.3092  data_time: 0.0036  memory: 46807  loss: 0.8067
06/29 22:05:21 - mmengine - INFO - Iter(train) [2320/5198]  lr: 1.2209e-05  eta: 4:26:27  time: 5.7985  data_time: 0.0035  memory: 36416  loss: 0.8979
06/29 22:06:11 - mmengine - INFO - Iter(train) [2330/5198]  lr: 1.2148e-05  eta: 4:25:24  time: 4.9569  data_time: 0.0034  memory: 33619  loss: 0.9990
06/29 22:06:58 - mmengine - INFO - Iter(train) [2340/5198]  lr: 1.2087e-05  eta: 4:24:19  time: 4.7551  data_time: 0.0035  memory: 33018  loss: 0.7835
06/29 22:07:43 - mmengine - INFO - Iter(train) [2350/5198]  lr: 1.2026e-05  eta: 4:23:10  time: 4.4945  data_time: 0.0035  memory: 32386  loss: 0.9117
06/29 22:08:56 - mmengine - INFO - Iter(train) [2360/5198]  lr: 1.1965e-05  eta: 4:22:36  time: 7.2631  data_time: 0.0035  memory: 46807  loss: 0.7619
06/29 22:09:54 - mmengine - INFO - Iter(train) [2370/5198]  lr: 1.1904e-05  eta: 4:21:43  time: 5.8328  data_time: 0.0035  memory: 36458  loss: 0.8749
06/29 22:10:44 - mmengine - INFO - Iter(train) [2380/5198]  lr: 1.1843e-05  eta: 4:20:41  time: 5.0180  data_time: 0.0035  memory: 33723  loss: 0.9993
06/29 22:11:32 - mmengine - INFO - Iter(train) [2390/5198]  lr: 1.1781e-05  eta: 4:19:37  time: 4.7770  data_time: 0.0035  memory: 33029  loss: 0.7595
06/29 22:12:17 - mmengine - INFO - Iter(train) [2400/5198]  lr: 1.1720e-05  eta: 4:18:29  time: 4.5160  data_time: 0.0034  memory: 32418  loss: 0.6371
06/29 22:13:31 - mmengine - INFO - Iter(train) [2410/5198]  lr: 1.1659e-05  eta: 4:17:55  time: 7.3342  data_time: 0.0035  memory: 46807  loss: 0.7967
06/29 22:14:30 - mmengine - INFO - Iter(train) [2420/5198]  lr: 1.1597e-05  eta: 4:17:03  time: 5.8743  data_time: 0.0036  memory: 36510  loss: 0.8659
06/29 22:15:20 - mmengine - INFO - Iter(train) [2430/5198]  lr: 1.1536e-05  eta: 4:16:01  time: 5.0227  data_time: 0.0035  memory: 33733  loss: 1.0791
06/29 22:16:08 - mmengine - INFO - Iter(train) [2440/5198]  lr: 1.1474e-05  eta: 4:14:57  time: 4.7921  data_time: 0.0035  memory: 33049  loss: 0.8091
06/29 22:16:53 - mmengine - INFO - Iter(train) [2450/5198]  lr: 1.1412e-05  eta: 4:13:50  time: 4.5090  data_time: 0.0035  memory: 32428  loss: 0.9406
06/29 22:18:06 - mmengine - INFO - Iter(train) [2460/5198]  lr: 1.1351e-05  eta: 4:13:14  time: 7.3038  data_time: 0.0036  memory: 46787  loss: 0.7990
06/29 22:19:03 - mmengine - INFO - Iter(train) [2470/5198]  lr: 1.1289e-05  eta: 4:12:21  time: 5.7670  data_time: 0.0035  memory: 36416  loss: 0.8557
06/29 22:19:53 - mmengine - INFO - Iter(train) [2480/5198]  lr: 1.1227e-05  eta: 4:11:20  time: 4.9840  data_time: 0.0035  memory: 33598  loss: 0.9672
06/29 22:20:41 - mmengine - INFO - Iter(train) [2490/5198]  lr: 1.1165e-05  eta: 4:10:15  time: 4.7457  data_time: 0.0035  memory: 33008  loss: 0.8168
06/29 22:21:26 - mmengine - INFO - Iter(train) [2500/5198]  lr: 1.1104e-05  eta: 4:09:08  time: 4.4853  data_time: 0.0035  memory: 32397  loss: 0.9669
06/29 22:21:26 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 22:21:27 - mmengine - INFO - Sample output:
USER: <image>
请描述一下这张照片 ASSISTANT:The image features a wooden dock extending over a body of water, providing a serene view of the lake. The dock appears to be empty, giving a sense of tranquility to the scene. The surrounding landscape consists of trees, adding to the natural beauty of the setting.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 22:21:29 - mmengine - INFO - Sample output:
USER: <image>
Please describe this picture ASSISTANT:The image features a wooden dock extending over a large body of water, such as a lake or a river. The dock appears to be empty, providing a serene and peaceful atmosphere. The surrounding landscape consists of trees, adding to the natural beauty of the scene.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 22:21:29 - mmengine - INFO - Saving checkpoint at 2500 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-29 22:21:35,475] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_2500.pth is about to be saved!
[2024-06-29 22:21:35,487] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/mp_rank_00_model_states.pt
[2024-06-29 22:21:35,487] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/mp_rank_00_model_states.pt...
[2024-06-29 22:21:50,365] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/mp_rank_00_model_states.pt.
[2024-06-29 22:21:50,367] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-29 22:21:50,367] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-29 22:21:50,367] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-29 22:21:50,367] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-29 22:21:50,367] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-29 22:21:50,367] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-29 22:21:50,368] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-29 22:21:50,368] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-29 22:22:02,346] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-29 22:22:02,346] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-29 22:22:02,346] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2500.pth is ready now!
[2024-06-29 22:22:02,381] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-29 22:22:02,381] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-29 22:22:02,381] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2500.pth is ready now!
[2024-06-29 22:22:02,382] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-29 22:22:02,383] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-29 22:22:02,383] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2500.pth is ready now!
[2024-06-29 22:22:02,438] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-29 22:22:02,438] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-29 22:22:02,438] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2500.pth is ready now!
[2024-06-29 22:22:02,444] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-29 22:22:02,444] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-29 22:22:02,444] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2500.pth is ready now!
[2024-06-29 22:22:02,446] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-29 22:22:02,446] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-29 22:22:02,446] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2500.pth is ready now!
[2024-06-29 22:22:02,487] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-29 22:22:02,487] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-29 22:22:02,487] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2500.pth is ready now!
[2024-06-29 22:22:02,540] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-29 22:22:02,540] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_2500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-29 22:22:02,540] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2500.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/29 22:23:15 - mmengine - INFO - Iter(train) [2510/5198]  lr: 1.1042e-05  eta: 4:09:11  time: 10.9166  data_time: 3.6466  memory: 46807  loss: 0.7812
06/29 22:24:13 - mmengine - INFO - Iter(train) [2520/5198]  lr: 1.0980e-05  eta: 4:08:18  time: 5.8720  data_time: 0.0034  memory: 36582  loss: 0.8616
06/29 22:25:04 - mmengine - INFO - Iter(train) [2530/5198]  lr: 1.0918e-05  eta: 4:07:17  time: 5.0136  data_time: 0.0034  memory: 33733  loss: 0.9812
06/29 22:25:52 - mmengine - INFO - Iter(train) [2540/5198]  lr: 1.0856e-05  eta: 4:06:13  time: 4.7929  data_time: 0.0042  memory: 33049  loss: 0.7891
06/29 22:26:37 - mmengine - INFO - Iter(train) [2550/5198]  lr: 1.0793e-05  eta: 4:05:07  time: 4.5076  data_time: 0.0035  memory: 32418  loss: 0.9413
06/29 22:27:50 - mmengine - INFO - Iter(train) [2560/5198]  lr: 1.0731e-05  eta: 4:04:29  time: 7.3237  data_time: 0.0034  memory: 46807  loss: 0.7682
06/29 22:28:48 - mmengine - INFO - Iter(train) [2570/5198]  lr: 1.0669e-05  eta: 4:03:37  time: 5.8586  data_time: 0.0035  memory: 36478  loss: 0.8577
06/29 22:29:38 - mmengine - INFO - Iter(train) [2580/5198]  lr: 1.0607e-05  eta: 4:02:36  time: 5.0014  data_time: 0.0035  memory: 33671  loss: 0.9701
06/29 22:30:26 - mmengine - INFO - Iter(train) [2590/5198]  lr: 1.0545e-05  eta: 4:01:32  time: 4.7633  data_time: 0.0036  memory: 33008  loss: 0.6957
06/29 22:31:11 - mmengine - INFO - Iter(train) [2600/5198]  lr: 1.0483e-05  eta: 4:00:26  time: 4.4986  data_time: 0.0035  memory: 32408  loss: 0.5036
06/29 22:32:24 - mmengine - INFO - Iter(train) [2610/5198]  lr: 1.0420e-05  eta: 3:59:47  time: 7.2916  data_time: 0.0037  memory: 46807  loss: 0.7914
06/29 22:33:22 - mmengine - INFO - Iter(train) [2620/5198]  lr: 1.0358e-05  eta: 3:58:54  time: 5.8109  data_time: 0.0035  memory: 36406  loss: 0.8696
06/29 22:34:12 - mmengine - INFO - Iter(train) [2630/5198]  lr: 1.0296e-05  eta: 3:57:53  time: 4.9844  data_time: 0.0035  memory: 33650  loss: 0.9657
06/29 22:35:00 - mmengine - INFO - Iter(train) [2640/5198]  lr: 1.0234e-05  eta: 3:56:50  time: 4.7673  data_time: 0.0035  memory: 33018  loss: 0.8307
06/29 22:35:45 - mmengine - INFO - Iter(train) [2650/5198]  lr: 1.0171e-05  eta: 3:55:44  time: 4.4912  data_time: 0.0035  memory: 32397  loss: 0.6706
06/29 22:37:09 - mmengine - INFO - Iter(train) [2660/5198]  lr: 1.0109e-05  eta: 3:55:16  time: 8.4757  data_time: 0.0035  memory: 40851  loss: 0.4769
06/29 22:38:16 - mmengine - INFO - Iter(train) [2670/5198]  lr: 1.0047e-05  eta: 3:54:31  time: 6.6824  data_time: 0.0035  memory: 39266  loss: 0.3671
06/29 22:39:05 - mmengine - INFO - Iter(train) [2680/5198]  lr: 9.9844e-06  eta: 3:53:29  time: 4.8950  data_time: 0.0035  memory: 34572  loss: 0.2837
06/29 22:39:44 - mmengine - INFO - Iter(train) [2690/5198]  lr: 9.9221e-06  eta: 3:52:19  time: 3.9326  data_time: 0.0035  memory: 31496  loss: 0.2171
06/29 22:40:19 - mmengine - INFO - Iter(train) [2700/5198]  lr: 9.8598e-06  eta: 3:51:03  time: 3.4184  data_time: 0.0035  memory: 29724  loss: 0.1709
06/29 22:41:32 - mmengine - INFO - Iter(train) [2710/5198]  lr: 9.7976e-06  eta: 3:50:24  time: 7.3182  data_time: 0.0035  memory: 46807  loss: 0.8216
06/29 22:42:30 - mmengine - INFO - Iter(train) [2720/5198]  lr: 9.7353e-06  eta: 3:49:31  time: 5.8621  data_time: 0.0035  memory: 36468  loss: 0.8518
06/29 22:43:21 - mmengine - INFO - Iter(train) [2730/5198]  lr: 9.6730e-06  eta: 3:48:31  time: 5.0195  data_time: 0.0035  memory: 33723  loss: 1.0043
06/29 22:44:08 - mmengine - INFO - Iter(train) [2740/5198]  lr: 9.6107e-06  eta: 3:47:28  time: 4.7779  data_time: 0.0035  memory: 33029  loss: 0.6526
06/29 22:44:53 - mmengine - INFO - Iter(train) [2750/5198]  lr: 9.5485e-06  eta: 3:46:23  time: 4.5078  data_time: 0.0035  memory: 32408  loss: 0.8855
06/29 22:46:07 - mmengine - INFO - Iter(train) [2760/5198]  lr: 9.4863e-06  eta: 3:45:44  time: 7.3189  data_time: 0.0036  memory: 46807  loss: 0.7990
06/29 22:47:05 - mmengine - INFO - Iter(train) [2770/5198]  lr: 9.4241e-06  eta: 3:44:51  time: 5.8663  data_time: 0.0035  memory: 36551  loss: 0.8961
06/29 22:47:55 - mmengine - INFO - Iter(train) [2780/5198]  lr: 9.3619e-06  eta: 3:43:50  time: 5.0131  data_time: 0.0035  memory: 33712  loss: 1.0218
06/29 22:48:43 - mmengine - INFO - Iter(train) [2790/5198]  lr: 9.2997e-06  eta: 3:42:48  time: 4.7710  data_time: 0.0035  memory: 33039  loss: 0.6926
06/29 22:49:28 - mmengine - INFO - Iter(train) [2800/5198]  lr: 9.2376e-06  eta: 3:41:44  time: 4.5021  data_time: 0.0035  memory: 32418  loss: 0.6649
06/29 22:50:53 - mmengine - INFO - Iter(train) [2810/5198]  lr: 9.1755e-06  eta: 3:41:13  time: 8.4764  data_time: 0.0035  memory: 40851  loss: 0.4525
06/29 22:52:00 - mmengine - INFO - Iter(train) [2820/5198]  lr: 9.1134e-06  eta: 3:40:27  time: 6.6935  data_time: 0.0035  memory: 39286  loss: 0.3825
06/29 22:52:49 - mmengine - INFO - Iter(train) [2830/5198]  lr: 9.0514e-06  eta: 3:39:26  time: 4.9213  data_time: 0.0035  memory: 34583  loss: 0.2991
06/29 22:53:28 - mmengine - INFO - Iter(train) [2840/5198]  lr: 8.9894e-06  eta: 3:38:17  time: 3.9291  data_time: 0.0034  memory: 31464  loss: 0.2328
06/29 22:54:03 - mmengine - INFO - Iter(train) [2850/5198]  lr: 8.9275e-06  eta: 3:37:04  time: 3.4363  data_time: 0.0034  memory: 29724  loss: 0.1292
06/29 22:55:16 - mmengine - INFO - Iter(train) [2860/5198]  lr: 8.8655e-06  eta: 3:36:23  time: 7.3360  data_time: 0.0035  memory: 46807  loss: 0.7567
06/29 22:56:16 - mmengine - INFO - Iter(train) [2870/5198]  lr: 8.8037e-06  eta: 3:35:31  time: 5.9504  data_time: 0.0035  memory: 36634  loss: 0.8869
06/29 22:57:06 - mmengine - INFO - Iter(train) [2880/5198]  lr: 8.7418e-06  eta: 3:34:31  time: 5.0407  data_time: 0.0035  memory: 33816  loss: 1.0387
06/29 22:57:54 - mmengine - INFO - Iter(train) [2890/5198]  lr: 8.6801e-06  eta: 3:33:29  time: 4.7895  data_time: 0.0034  memory: 33049  loss: 0.7370
06/29 22:58:39 - mmengine - INFO - Iter(train) [2900/5198]  lr: 8.6183e-06  eta: 3:32:26  time: 4.5056  data_time: 0.0035  memory: 32428  loss: 0.6998
06/29 22:59:52 - mmengine - INFO - Iter(train) [2910/5198]  lr: 8.5567e-06  eta: 3:31:44  time: 7.2936  data_time: 0.0036  memory: 46807  loss: 0.8063
06/29 23:00:49 - mmengine - INFO - Iter(train) [2920/5198]  lr: 8.4951e-06  eta: 3:30:50  time: 5.7286  data_time: 0.0035  memory: 36437  loss: 0.8576
06/29 23:01:40 - mmengine - INFO - Iter(train) [2930/5198]  lr: 8.4335e-06  eta: 3:29:50  time: 5.0407  data_time: 0.0035  memory: 33681  loss: 1.0139
06/29 23:02:28 - mmengine - INFO - Iter(train) [2940/5198]  lr: 8.3720e-06  eta: 3:28:49  time: 4.8028  data_time: 0.0036  memory: 33039  loss: 0.7893
06/29 23:03:13 - mmengine - INFO - Iter(train) [2950/5198]  lr: 8.3106e-06  eta: 3:27:46  time: 4.5175  data_time: 0.0036  memory: 32408  loss: 1.0307
06/29 23:04:26 - mmengine - INFO - Iter(train) [2960/5198]  lr: 8.2492e-06  eta: 3:27:03  time: 7.3006  data_time: 0.0035  memory: 46807  loss: 0.7749
06/29 23:05:24 - mmengine - INFO - Iter(train) [2970/5198]  lr: 8.1879e-06  eta: 3:26:10  time: 5.8560  data_time: 0.0035  memory: 36520  loss: 0.8445
06/29 23:06:15 - mmengine - INFO - Iter(train) [2980/5198]  lr: 8.1267e-06  eta: 3:25:11  time: 5.0160  data_time: 0.0035  memory: 33723  loss: 0.9581
06/29 23:07:02 - mmengine - INFO - Iter(train) [2990/5198]  lr: 8.0655e-06  eta: 3:24:09  time: 4.7706  data_time: 0.0035  memory: 33039  loss: 0.7219
06/29 23:07:47 - mmengine - INFO - Exp name: llava_v15_7b_finetune_copy_20240629_180208
06/29 23:07:47 - mmengine - INFO - Iter(train) [3000/5198]  lr: 8.0044e-06  eta: 3:23:06  time: 4.5104  data_time: 0.0034  memory: 32418  loss: 0.7026
06/29 23:07:47 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 23:07:49 - mmengine - INFO - Sample output:
USER: <image>
请描述一下这张照片 ASSISTANT:The image features a pier extending out over a calm lake, with a dock situated at the end of the pier. The pier is surrounded by a serene landscape, with trees and mountains in the background. The water is clear and inviting, creating a picturesque scene.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 23:07:50 - mmengine - INFO - Sample output:
USER: <image>
Please describe this picture ASSISTANT:The image features a wooden pier extending out over a calm lake. The pier is surrounded by a serene landscape with trees and mountains in the background. The water is clear and inviting, creating a picturesque scene.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 23:07:50 - mmengine - INFO - Saving checkpoint at 3000 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-29 23:07:56,484] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_3000.pth is about to be saved!
[2024-06-29 23:07:56,495] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/mp_rank_00_model_states.pt
[2024-06-29 23:07:56,495] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/mp_rank_00_model_states.pt...
[2024-06-29 23:08:11,260] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/mp_rank_00_model_states.pt.
[2024-06-29 23:08:11,263] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-29 23:08:11,263] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-29 23:08:11,263] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-29 23:08:11,263] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-29 23:08:11,263] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-29 23:08:11,263] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-29 23:08:11,263] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-29 23:08:11,263] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-29 23:08:23,472] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-29 23:08:23,472] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-29 23:08:23,473] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_3000.pth is ready now!
[2024-06-29 23:08:23,505] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-29 23:08:23,505] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-29 23:08:23,505] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_3000.pth is ready now!
[2024-06-29 23:08:23,546] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-29 23:08:23,546] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-29 23:08:23,547] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_3000.pth is ready now!
[2024-06-29 23:08:23,555] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-29 23:08:23,555] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-29 23:08:23,555] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_3000.pth is ready now!
[2024-06-29 23:08:23,589] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-29 23:08:23,589] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-29 23:08:23,590] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_3000.pth is ready now!
[2024-06-29 23:08:23,598] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-29 23:08:23,598] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-29 23:08:23,599] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_3000.pth is ready now!
[2024-06-29 23:08:23,613] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-29 23:08:23,613] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-29 23:08:23,613] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-29 23:08:23,613] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_3000.pth is ready now!
[2024-06-29 23:08:23,613] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-29 23:08:23,613] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_3000.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/29 23:09:36 - mmengine - INFO - Iter(train) [3010/5198]  lr: 7.9434e-06  eta: 3:22:50  time: 10.8566  data_time: 3.5825  memory: 46807  loss: 0.7760
06/29 23:10:34 - mmengine - INFO - Iter(train) [3020/5198]  lr: 7.8825e-06  eta: 3:21:56  time: 5.8216  data_time: 0.0036  memory: 36375  loss: 0.7786
06/29 23:11:24 - mmengine - INFO - Iter(train) [3030/5198]  lr: 7.8217e-06  eta: 3:20:56  time: 5.0114  data_time: 0.0035  memory: 33702  loss: 0.9937
06/29 23:12:12 - mmengine - INFO - Iter(train) [3040/5198]  lr: 7.7609e-06  eta: 3:19:55  time: 4.7804  data_time: 0.0035  memory: 33039  loss: 0.8386
06/29 23:12:57 - mmengine - INFO - Iter(train) [3050/5198]  lr: 7.7002e-06  eta: 3:18:52  time: 4.5108  data_time: 0.0034  memory: 32418  loss: 0.8974
06/29 23:14:10 - mmengine - INFO - Iter(train) [3060/5198]  lr: 7.6397e-06  eta: 3:18:09  time: 7.2808  data_time: 0.0036  memory: 46807  loss: 0.7606
06/29 23:15:09 - mmengine - INFO - Iter(train) [3070/5198]  lr: 7.5792e-06  eta: 3:17:15  time: 5.8619  data_time: 0.0035  memory: 36520  loss: 0.8400
06/29 23:15:59 - mmengine - INFO - Iter(train) [3080/5198]  lr: 7.5188e-06  eta: 3:16:16  time: 5.0029  data_time: 0.0034  memory: 33692  loss: 0.9841
06/29 23:16:46 - mmengine - INFO - Iter(train) [3090/5198]  lr: 7.4585e-06  eta: 3:15:15  time: 4.7621  data_time: 0.0035  memory: 33029  loss: 0.7923
06/29 23:17:31 - mmengine - INFO - Iter(train) [3100/5198]  lr: 7.3983e-06  eta: 3:14:12  time: 4.5025  data_time: 0.0035  memory: 32408  loss: 0.6886
06/29 23:18:44 - mmengine - INFO - Iter(train) [3110/5198]  lr: 7.3382e-06  eta: 3:13:28  time: 7.3090  data_time: 0.0036  memory: 46807  loss: 0.7765
06/29 23:19:42 - mmengine - INFO - Iter(train) [3120/5198]  lr: 7.2782e-06  eta: 3:12:34  time: 5.8165  data_time: 0.0035  memory: 36572  loss: 0.8392
06/29 23:20:32 - mmengine - INFO - Iter(train) [3130/5198]  lr: 7.2183e-06  eta: 3:11:35  time: 4.9911  data_time: 0.0035  memory: 33629  loss: 0.9907
06/29 23:21:20 - mmengine - INFO - Iter(train) [3140/5198]  lr: 7.1585e-06  eta: 3:10:34  time: 4.7813  data_time: 0.0035  memory: 33029  loss: 0.8213
06/29 23:22:05 - mmengine - INFO - Iter(train) [3150/5198]  lr: 7.0988e-06  eta: 3:09:32  time: 4.5108  data_time: 0.0035  memory: 32418  loss: 0.7969
06/29 23:23:19 - mmengine - INFO - Iter(train) [3160/5198]  lr: 7.0393e-06  eta: 3:08:48  time: 7.3228  data_time: 0.0035  memory: 46807  loss: 0.7754
06/29 23:24:17 - mmengine - INFO - Iter(train) [3170/5198]  lr: 6.9798e-06  eta: 3:07:54  time: 5.8814  data_time: 0.0035  memory: 36520  loss: 0.8506
06/29 23:25:08 - mmengine - INFO - Iter(train) [3180/5198]  lr: 6.9205e-06  eta: 3:06:55  time: 5.0183  data_time: 0.0036  memory: 33712  loss: 1.0431
06/29 23:25:55 - mmengine - INFO - Iter(train) [3190/5198]  lr: 6.8613e-06  eta: 3:05:55  time: 4.7931  data_time: 0.0035  memory: 33039  loss: 0.6931
06/29 23:26:40 - mmengine - INFO - Iter(train) [3200/5198]  lr: 6.8022e-06  eta: 3:04:53  time: 4.4980  data_time: 0.0035  memory: 32408  loss: 0.7869
06/29 23:27:53 - mmengine - INFO - Iter(train) [3210/5198]  lr: 6.7432e-06  eta: 3:04:08  time: 7.2837  data_time: 0.0035  memory: 46807  loss: 0.7985
06/29 23:28:51 - mmengine - INFO - Iter(train) [3220/5198]  lr: 6.6844e-06  eta: 3:03:14  time: 5.7906  data_time: 0.0036  memory: 36375  loss: 0.8960
06/29 23:29:41 - mmengine - INFO - Iter(train) [3230/5198]  lr: 6.6257e-06  eta: 3:02:15  time: 5.0017  data_time: 0.0035  memory: 33692  loss: 1.0037
06/29 23:30:29 - mmengine - INFO - Iter(train) [3240/5198]  lr: 6.5671e-06  eta: 3:01:14  time: 4.7628  data_time: 0.0035  memory: 33018  loss: 0.7438
06/29 23:31:14 - mmengine - INFO - Iter(train) [3250/5198]  lr: 6.5087e-06  eta: 3:00:12  time: 4.5002  data_time: 0.0035  memory: 32408  loss: 0.6553
06/29 23:32:27 - mmengine - INFO - Iter(train) [3260/5198]  lr: 6.4504e-06  eta: 2:59:28  time: 7.3284  data_time: 0.0035  memory: 46807  loss: 0.7777
06/29 23:33:26 - mmengine - INFO - Iter(train) [3270/5198]  lr: 6.3922e-06  eta: 2:58:34  time: 5.9188  data_time: 0.0035  memory: 36603  loss: 0.8466
06/29 23:34:17 - mmengine - INFO - Iter(train) [3280/5198]  lr: 6.3342e-06  eta: 2:57:35  time: 5.0293  data_time: 0.0035  memory: 33816  loss: 1.0252
06/29 23:35:05 - mmengine - INFO - Iter(train) [3290/5198]  lr: 6.2763e-06  eta: 2:56:35  time: 4.7887  data_time: 0.0036  memory: 33060  loss: 0.8694
06/29 23:35:50 - mmengine - INFO - Iter(train) [3300/5198]  lr: 6.2185e-06  eta: 2:55:34  time: 4.5184  data_time: 0.0035  memory: 32428  loss: 0.6375
06/29 23:37:03 - mmengine - INFO - Iter(train) [3310/5198]  lr: 6.1609e-06  eta: 2:54:48  time: 7.2995  data_time: 0.0035  memory: 46807  loss: 0.7894
06/29 23:38:01 - mmengine - INFO - Iter(train) [3320/5198]  lr: 6.1035e-06  eta: 2:53:54  time: 5.8019  data_time: 0.0035  memory: 36261  loss: 0.8497
06/29 23:38:51 - mmengine - INFO - Iter(train) [3330/5198]  lr: 6.0462e-06  eta: 2:52:56  time: 5.0075  data_time: 0.0036  memory: 33640  loss: 1.0069
06/29 23:39:38 - mmengine - INFO - Iter(train) [3340/5198]  lr: 5.9891e-06  eta: 2:51:56  time: 4.7616  data_time: 0.0035  memory: 33018  loss: 0.7574
06/29 23:40:23 - mmengine - INFO - Iter(train) [3350/5198]  lr: 5.9321e-06  eta: 2:50:54  time: 4.4969  data_time: 0.0035  memory: 32397  loss: 0.7411
06/29 23:41:36 - mmengine - INFO - Iter(train) [3360/5198]  lr: 5.8752e-06  eta: 2:50:08  time: 7.2984  data_time: 0.0035  memory: 46807  loss: 0.7702
06/29 23:42:35 - mmengine - INFO - Iter(train) [3370/5198]  lr: 5.8186e-06  eta: 2:49:14  time: 5.8491  data_time: 0.0036  memory: 36499  loss: 0.8597
06/29 23:43:25 - mmengine - INFO - Iter(train) [3380/5198]  lr: 5.7621e-06  eta: 2:48:16  time: 5.0038  data_time: 0.0035  memory: 33712  loss: 0.9763
06/29 23:44:12 - mmengine - INFO - Iter(train) [3390/5198]  lr: 5.7057e-06  eta: 2:47:16  time: 4.7555  data_time: 0.0036  memory: 33029  loss: 0.7540
06/29 23:44:57 - mmengine - INFO - Iter(train) [3400/5198]  lr: 5.6495e-06  eta: 2:46:15  time: 4.4974  data_time: 0.0035  memory: 32397  loss: 0.7519
06/29 23:46:10 - mmengine - INFO - Iter(train) [3410/5198]  lr: 5.5935e-06  eta: 2:45:29  time: 7.2953  data_time: 0.0035  memory: 46807  loss: 0.7589
06/29 23:47:09 - mmengine - INFO - Iter(train) [3420/5198]  lr: 5.5377e-06  eta: 2:44:35  time: 5.8798  data_time: 0.0035  memory: 36551  loss: 0.7895
06/29 23:47:59 - mmengine - INFO - Iter(train) [3430/5198]  lr: 5.4820e-06  eta: 2:43:37  time: 5.0198  data_time: 0.0036  memory: 33806  loss: 0.9393
06/29 23:48:47 - mmengine - INFO - Iter(train) [3440/5198]  lr: 5.4265e-06  eta: 2:42:37  time: 4.7859  data_time: 0.0035  memory: 33039  loss: 0.6912
06/29 23:49:32 - mmengine - INFO - Iter(train) [3450/5198]  lr: 5.3712e-06  eta: 2:41:36  time: 4.5135  data_time: 0.0034  memory: 32428  loss: 0.7293
06/29 23:50:46 - mmengine - INFO - Iter(train) [3460/5198]  lr: 5.3161e-06  eta: 2:40:50  time: 7.3261  data_time: 0.0037  memory: 46807  loss: 0.7864
06/29 23:51:44 - mmengine - INFO - Iter(train) [3470/5198]  lr: 5.2612e-06  eta: 2:39:56  time: 5.8527  data_time: 0.0038  memory: 36489  loss: 0.8064
06/29 23:52:34 - mmengine - INFO - Iter(train) [3480/5198]  lr: 5.2064e-06  eta: 2:38:58  time: 5.0059  data_time: 0.0037  memory: 33671  loss: 0.9660
06/29 23:53:22 - mmengine - INFO - Iter(train) [3490/5198]  lr: 5.1518e-06  eta: 2:37:58  time: 4.7750  data_time: 0.0037  memory: 33029  loss: 0.6599
06/29 23:54:07 - mmengine - INFO - Iter(train) [3500/5198]  lr: 5.0974e-06  eta: 2:36:58  time: 4.5060  data_time: 0.0036  memory: 32397  loss: 0.7652
06/29 23:54:07 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 23:54:10 - mmengine - INFO - Sample output:
USER: <image>
请描述一下这张照片 ASSISTANT:The image features a serene scene of a dock extending out into a large body of water, possibly a lake. The dock is surrounded by a lush forest, creating a peaceful and picturesque setting. The water is calm, reflecting the tranquility of the environment.

There are several benches placed along the dock, providing a place for visitors to sit and enjoy the view. The benches are positioned at various distances from the water's edge, offering different perspectives of the surrounding landscape.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 23:54:13 - mmengine - INFO - Sample output:
USER: <image>
Please describe this picture ASSISTANT:The image features a serene scene of a dock extending out into a large body of water, possibly a lake. The dock is surrounded by a lush forest, creating a picturesque view. The water is calm and still, reflecting the tranquility of the surroundings.

There are several benches placed along the dock, providing a place for visitors to sit and enjoy the view. The benches are positioned at various distances from the water, offering different perspectives of the lake and the forest.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/29 23:54:13 - mmengine - INFO - Saving checkpoint at 3500 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-29 23:54:19,597] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_3500.pth is about to be saved!
[2024-06-29 23:54:19,609] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/mp_rank_00_model_states.pt
[2024-06-29 23:54:19,609] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/mp_rank_00_model_states.pt...
[2024-06-29 23:54:34,731] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/mp_rank_00_model_states.pt.
[2024-06-29 23:54:34,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-29 23:54:34,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-29 23:54:34,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-29 23:54:34,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-29 23:54:34,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-29 23:54:34,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-29 23:54:34,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-29 23:54:34,733] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-29 23:54:46,657] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-29 23:54:46,657] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-29 23:54:46,657] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_3500.pth is ready now!
[2024-06-29 23:54:46,745] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-29 23:54:46,745] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-29 23:54:46,745] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_3500.pth is ready now!
[2024-06-29 23:54:46,775] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-29 23:54:46,775] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-29 23:54:46,775] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_3500.pth is ready now!
[2024-06-29 23:54:46,776] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-29 23:54:46,776] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-29 23:54:46,776] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_3500.pth is ready now!
[2024-06-29 23:54:46,785] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-29 23:54:46,785] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-29 23:54:46,785] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_3500.pth is ready now!
[2024-06-29 23:54:46,916] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-29 23:54:46,916] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-29 23:54:46,916] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-29 23:54:46,916] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-29 23:54:46,916] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_3500.pth is ready now!
[2024-06-29 23:54:46,916] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_3500.pth is ready now!
[2024-06-29 23:54:46,917] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-29 23:54:46,917] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_3500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-29 23:54:46,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_3500.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/29 23:55:59 - mmengine - INFO - Iter(train) [3510/5198]  lr: 5.0432e-06  eta: 2:36:29  time: 11.2110  data_time: 3.9448  memory: 46807  loss: 0.7716
06/29 23:56:58 - mmengine - INFO - Iter(train) [3520/5198]  lr: 4.9892e-06  eta: 2:35:35  time: 5.8801  data_time: 0.0037  memory: 36541  loss: 0.8353
06/29 23:57:48 - mmengine - INFO - Iter(train) [3530/5198]  lr: 4.9354e-06  eta: 2:34:37  time: 5.0162  data_time: 0.0036  memory: 33764  loss: 0.9221
06/29 23:58:36 - mmengine - INFO - Iter(train) [3540/5198]  lr: 4.8818e-06  eta: 2:33:38  time: 4.7703  data_time: 0.0037  memory: 33039  loss: 0.8579
06/29 23:59:21 - mmengine - INFO - Iter(train) [3550/5198]  lr: 4.8284e-06  eta: 2:32:37  time: 4.5157  data_time: 0.0043  memory: 32408  loss: 0.6721
06/30 00:00:33 - mmengine - INFO - Iter(train) [3560/5198]  lr: 4.7751e-06  eta: 2:31:49  time: 7.2333  data_time: 0.0037  memory: 46807  loss: 0.7736
06/30 00:01:32 - mmengine - INFO - Iter(train) [3570/5198]  lr: 4.7221e-06  eta: 2:30:55  time: 5.8505  data_time: 0.0036  memory: 36427  loss: 0.8132
06/30 00:02:22 - mmengine - INFO - Iter(train) [3580/5198]  lr: 4.6693e-06  eta: 2:29:57  time: 5.0071  data_time: 0.0035  memory: 33671  loss: 0.9798
06/30 00:03:10 - mmengine - INFO - Iter(train) [3590/5198]  lr: 4.6167e-06  eta: 2:28:58  time: 4.7952  data_time: 0.0036  memory: 33018  loss: 0.7225
06/30 00:03:55 - mmengine - INFO - Iter(train) [3600/5198]  lr: 4.5643e-06  eta: 2:27:58  time: 4.5117  data_time: 0.0035  memory: 32408  loss: 0.7149
06/30 00:05:08 - mmengine - INFO - Iter(train) [3610/5198]  lr: 4.5121e-06  eta: 2:27:10  time: 7.3381  data_time: 0.0036  memory: 46807  loss: 0.7822
06/30 00:06:08 - mmengine - INFO - Iter(train) [3620/5198]  lr: 4.4602e-06  eta: 2:26:16  time: 5.9250  data_time: 0.0036  memory: 36561  loss: 0.8263
06/30 00:06:58 - mmengine - INFO - Iter(train) [3630/5198]  lr: 4.4084e-06  eta: 2:25:18  time: 5.0322  data_time: 0.0036  memory: 33764  loss: 0.9495
06/30 00:07:46 - mmengine - INFO - Iter(train) [3640/5198]  lr: 4.3569e-06  eta: 2:24:19  time: 4.7763  data_time: 0.0035  memory: 33039  loss: 0.7405
06/30 00:08:31 - mmengine - INFO - Iter(train) [3650/5198]  lr: 4.3056e-06  eta: 2:23:19  time: 4.4974  data_time: 0.0036  memory: 32397  loss: 0.8127
06/30 00:09:44 - mmengine - INFO - Iter(train) [3660/5198]  lr: 4.2545e-06  eta: 2:22:31  time: 7.3092  data_time: 0.0036  memory: 46807  loss: 0.7681
06/30 00:10:42 - mmengine - INFO - Iter(train) [3670/5198]  lr: 4.2036e-06  eta: 2:21:37  time: 5.8455  data_time: 0.0036  memory: 36530  loss: 0.8010
06/30 00:11:32 - mmengine - INFO - Iter(train) [3680/5198]  lr: 4.1529e-06  eta: 2:20:39  time: 5.0173  data_time: 0.0036  memory: 33712  loss: 1.0057
06/30 00:12:20 - mmengine - INFO - Iter(train) [3690/5198]  lr: 4.1025e-06  eta: 2:19:40  time: 4.7820  data_time: 0.0036  memory: 33049  loss: 0.7108
06/30 00:13:05 - mmengine - INFO - Iter(train) [3700/5198]  lr: 4.0523e-06  eta: 2:18:40  time: 4.4964  data_time: 0.0035  memory: 32418  loss: 0.6831
06/30 00:14:18 - mmengine - INFO - Iter(train) [3710/5198]  lr: 4.0024e-06  eta: 2:17:51  time: 7.2842  data_time: 0.0036  memory: 46424  loss: 0.7707
06/30 00:15:16 - mmengine - INFO - Iter(train) [3720/5198]  lr: 3.9526e-06  eta: 2:16:57  time: 5.8314  data_time: 0.0036  memory: 36375  loss: 0.8530
06/30 00:16:06 - mmengine - INFO - Iter(train) [3730/5198]  lr: 3.9031e-06  eta: 2:15:59  time: 5.0179  data_time: 0.0036  memory: 33733  loss: 0.9950
06/30 00:16:54 - mmengine - INFO - Iter(train) [3740/5198]  lr: 3.8539e-06  eta: 2:15:01  time: 4.7812  data_time: 0.0035  memory: 33039  loss: 0.7518
06/30 00:17:39 - mmengine - INFO - Iter(train) [3750/5198]  lr: 3.8048e-06  eta: 2:14:01  time: 4.5052  data_time: 0.0035  memory: 32418  loss: 0.6588
06/30 00:18:53 - mmengine - INFO - Iter(train) [3760/5198]  lr: 3.7561e-06  eta: 2:13:12  time: 7.3262  data_time: 0.0036  memory: 46807  loss: 0.7502
06/30 00:19:51 - mmengine - INFO - Iter(train) [3770/5198]  lr: 3.7075e-06  eta: 2:12:18  time: 5.8787  data_time: 0.0035  memory: 36520  loss: 0.8055
06/30 00:20:42 - mmengine - INFO - Iter(train) [3780/5198]  lr: 3.6592e-06  eta: 2:11:20  time: 5.0329  data_time: 0.0036  memory: 33692  loss: 0.9836
06/30 00:21:30 - mmengine - INFO - Iter(train) [3790/5198]  lr: 3.6112e-06  eta: 2:10:22  time: 4.7838  data_time: 0.0035  memory: 33039  loss: 0.8682
06/30 00:22:15 - mmengine - INFO - Iter(train) [3800/5198]  lr: 3.5634e-06  eta: 2:09:22  time: 4.5048  data_time: 0.0035  memory: 32408  loss: 0.6766
06/30 00:23:28 - mmengine - INFO - Iter(train) [3810/5198]  lr: 3.5158e-06  eta: 2:08:33  time: 7.3115  data_time: 0.0036  memory: 46807  loss: 0.7501
06/30 00:24:26 - mmengine - INFO - Iter(train) [3820/5198]  lr: 3.4685e-06  eta: 2:07:39  time: 5.7972  data_time: 0.0035  memory: 36416  loss: 0.8597
06/30 00:25:16 - mmengine - INFO - Iter(train) [3830/5198]  lr: 3.4215e-06  eta: 2:06:41  time: 4.9911  data_time: 0.0036  memory: 33661  loss: 0.9223
06/30 00:26:03 - mmengine - INFO - Iter(train) [3840/5198]  lr: 3.3747e-06  eta: 2:05:43  time: 4.7615  data_time: 0.0035  memory: 33018  loss: 0.8817
06/30 00:26:48 - mmengine - INFO - Iter(train) [3850/5198]  lr: 3.3282e-06  eta: 2:04:43  time: 4.4861  data_time: 0.0035  memory: 32397  loss: 0.6407
06/30 00:28:01 - mmengine - INFO - Iter(train) [3860/5198]  lr: 3.2819e-06  eta: 2:03:54  time: 7.2699  data_time: 0.0035  memory: 46807  loss: 0.7920
06/30 00:28:59 - mmengine - INFO - Iter(train) [3870/5198]  lr: 3.2359e-06  eta: 2:02:59  time: 5.7749  data_time: 0.0035  memory: 36302  loss: 0.8082
06/30 00:29:48 - mmengine - INFO - Iter(train) [3880/5198]  lr: 3.1901e-06  eta: 2:02:01  time: 4.9841  data_time: 0.0035  memory: 33640  loss: 0.9081
06/30 00:30:36 - mmengine - INFO - Iter(train) [3890/5198]  lr: 3.1446e-06  eta: 2:01:03  time: 4.7706  data_time: 0.0035  memory: 33029  loss: 0.8634
06/30 00:31:21 - mmengine - INFO - Iter(train) [3900/5198]  lr: 3.0994e-06  eta: 2:00:04  time: 4.4931  data_time: 0.0036  memory: 32408  loss: 0.7221
06/30 00:32:34 - mmengine - INFO - Iter(train) [3910/5198]  lr: 3.0545e-06  eta: 1:59:14  time: 7.2895  data_time: 0.0035  memory: 46807  loss: 0.7742
06/30 00:33:32 - mmengine - INFO - Iter(train) [3920/5198]  lr: 3.0098e-06  eta: 1:58:20  time: 5.8263  data_time: 0.0035  memory: 36292  loss: 0.8371
06/30 00:34:22 - mmengine - INFO - Iter(train) [3930/5198]  lr: 2.9654e-06  eta: 1:57:22  time: 5.0144  data_time: 0.0035  memory: 33661  loss: 0.9094
06/30 00:35:10 - mmengine - INFO - Iter(train) [3940/5198]  lr: 2.9212e-06  eta: 1:56:24  time: 4.7797  data_time: 0.0036  memory: 33018  loss: 0.7278
06/30 00:35:55 - mmengine - INFO - Iter(train) [3950/5198]  lr: 2.8774e-06  eta: 1:55:26  time: 4.5103  data_time: 0.0035  memory: 32408  loss: 0.6931
06/30 00:37:08 - mmengine - INFO - Iter(train) [3960/5198]  lr: 2.8338e-06  eta: 1:54:36  time: 7.3136  data_time: 0.0035  memory: 46807  loss: 0.7579
06/30 00:38:07 - mmengine - INFO - Iter(train) [3970/5198]  lr: 2.7905e-06  eta: 1:53:41  time: 5.8505  data_time: 0.0035  memory: 36510  loss: 0.8232
06/30 00:38:57 - mmengine - INFO - Iter(train) [3980/5198]  lr: 2.7474e-06  eta: 1:52:44  time: 5.0010  data_time: 0.0035  memory: 33692  loss: 1.0087
06/30 00:39:44 - mmengine - INFO - Iter(train) [3990/5198]  lr: 2.7047e-06  eta: 1:51:46  time: 4.7561  data_time: 0.0035  memory: 33029  loss: 0.7983
06/30 00:40:29 - mmengine - INFO - Exp name: llava_v15_7b_finetune_copy_20240629_180208
06/30 00:40:29 - mmengine - INFO - Iter(train) [4000/5198]  lr: 2.6622e-06  eta: 1:50:47  time: 4.4851  data_time: 0.0035  memory: 32397  loss: 0.6010
06/30 00:40:29 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 00:40:31 - mmengine - INFO - Sample output:
USER: <image>
请描述一下这张照片 ASSISTANT:The image features a serene scene of a dock extending out over a lake. The dock is made of wood and is situated near the water's edge. The lake is surrounded by a lush forest, creating a peaceful and picturesque setting. The sky above the lake is cloudy, adding to the tranquil atmosphere of the scene.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 00:40:33 - mmengine - INFO - Sample output:
USER: <image>
Please describe this picture ASSISTANT:The image features a wooden dock extending out over a calm lake. The dock is situated near the water's edge, providing a serene view of the lake. The scene is further enhanced by the presence of mountains in the background, creating a picturesque landscape. The tranquil atmosphere of the lake and the surrounding nature make it an ideal spot for relaxation and contemplation.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 00:40:33 - mmengine - INFO - Saving checkpoint at 4000 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-30 00:40:39,717] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_4000.pth is about to be saved!
[2024-06-30 00:40:39,729] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/mp_rank_00_model_states.pt
[2024-06-30 00:40:39,729] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/mp_rank_00_model_states.pt...
[2024-06-30 00:40:54,702] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/mp_rank_00_model_states.pt.
[2024-06-30 00:40:54,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-30 00:40:54,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-30 00:40:54,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-30 00:40:54,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-30 00:40:54,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-30 00:40:54,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-30 00:40:54,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-30 00:40:54,705] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-30 00:41:06,811] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-30 00:41:06,811] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-30 00:41:06,811] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_4000.pth is ready now!
[2024-06-30 00:41:06,842] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-30 00:41:06,842] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-30 00:41:06,842] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_4000.pth is ready now!
[2024-06-30 00:41:06,842] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-30 00:41:06,842] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-30 00:41:06,842] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_4000.pth is ready now!
[2024-06-30 00:41:06,961] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-30 00:41:06,961] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-30 00:41:06,961] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_4000.pth is ready now!
[2024-06-30 00:41:06,984] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-30 00:41:06,984] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-30 00:41:06,984] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_4000.pth is ready now!
[2024-06-30 00:41:07,078] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-30 00:41:07,078] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-30 00:41:07,078] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_4000.pth is ready now!
[2024-06-30 00:41:07,111] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-30 00:41:07,111] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-30 00:41:07,111] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_4000.pth is ready now!
[2024-06-30 00:41:07,204] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-30 00:41:07,204] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-30 00:41:07,204] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_4000.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/30 00:42:31 - mmengine - INFO - Iter(train) [4010/5198]  lr: 2.6200e-06  eta: 1:50:11  time: 12.1517  data_time: 3.7487  memory: 40851  loss: 0.4294
06/30 00:43:37 - mmengine - INFO - Iter(train) [4020/5198]  lr: 2.5781e-06  eta: 1:49:18  time: 6.5849  data_time: 0.0035  memory: 38924  loss: 0.3792
06/30 00:44:25 - mmengine - INFO - Iter(train) [4030/5198]  lr: 2.5365e-06  eta: 1:48:21  time: 4.8430  data_time: 0.0035  memory: 34448  loss: 0.3194
06/30 00:45:04 - mmengine - INFO - Iter(train) [4040/5198]  lr: 2.4952e-06  eta: 1:47:20  time: 3.8960  data_time: 0.0035  memory: 31350  loss: 0.2017
06/30 00:45:38 - mmengine - INFO - Iter(train) [4050/5198]  lr: 2.4542e-06  eta: 1:46:19  time: 3.4089  data_time: 0.0035  memory: 29724  loss: 0.1059
06/30 00:46:51 - mmengine - INFO - Iter(train) [4060/5198]  lr: 2.4135e-06  eta: 1:45:28  time: 7.2901  data_time: 0.0035  memory: 46807  loss: 0.7518
06/30 00:47:50 - mmengine - INFO - Iter(train) [4070/5198]  lr: 2.3730e-06  eta: 1:44:33  time: 5.8687  data_time: 0.0035  memory: 36510  loss: 0.8642
06/30 00:48:40 - mmengine - INFO - Iter(train) [4080/5198]  lr: 2.3329e-06  eta: 1:43:36  time: 5.0381  data_time: 0.0035  memory: 33744  loss: 0.9480
06/30 00:49:28 - mmengine - INFO - Iter(train) [4090/5198]  lr: 2.2930e-06  eta: 1:42:38  time: 4.8028  data_time: 0.0035  memory: 33039  loss: 0.7924
06/30 00:50:13 - mmengine - INFO - Iter(train) [4100/5198]  lr: 2.2535e-06  eta: 1:41:40  time: 4.5057  data_time: 0.0035  memory: 32408  loss: 0.9918
06/30 00:51:26 - mmengine - INFO - Iter(train) [4110/5198]  lr: 2.2143e-06  eta: 1:40:49  time: 7.3122  data_time: 0.0036  memory: 46807  loss: 0.7438
06/30 00:52:25 - mmengine - INFO - Iter(train) [4120/5198]  lr: 2.1753e-06  eta: 1:39:54  time: 5.8485  data_time: 0.0036  memory: 36541  loss: 0.8155
06/30 00:53:15 - mmengine - INFO - Iter(train) [4130/5198]  lr: 2.1367e-06  eta: 1:38:57  time: 4.9985  data_time: 0.0036  memory: 33692  loss: 0.9577
06/30 00:54:02 - mmengine - INFO - Iter(train) [4140/5198]  lr: 2.0983e-06  eta: 1:37:59  time: 4.7650  data_time: 0.0035  memory: 33029  loss: 0.6717
06/30 00:54:47 - mmengine - INFO - Iter(train) [4150/5198]  lr: 2.0603e-06  eta: 1:37:01  time: 4.4907  data_time: 0.0035  memory: 32397  loss: 0.4889
06/30 00:56:00 - mmengine - INFO - Iter(train) [4160/5198]  lr: 2.0226e-06  eta: 1:36:10  time: 7.2939  data_time: 0.0036  memory: 46807  loss: 0.7644
06/30 00:56:59 - mmengine - INFO - Iter(train) [4170/5198]  lr: 1.9852e-06  eta: 1:35:15  time: 5.8799  data_time: 0.0036  memory: 36551  loss: 0.8092
06/30 00:57:49 - mmengine - INFO - Iter(train) [4180/5198]  lr: 1.9481e-06  eta: 1:34:18  time: 5.0166  data_time: 0.0035  memory: 33723  loss: 1.0055
06/30 00:58:37 - mmengine - INFO - Iter(train) [4190/5198]  lr: 1.9113e-06  eta: 1:33:21  time: 4.7728  data_time: 0.0035  memory: 33029  loss: 0.7753
06/30 00:59:22 - mmengine - INFO - Iter(train) [4200/5198]  lr: 1.8748e-06  eta: 1:32:23  time: 4.5040  data_time: 0.0036  memory: 32408  loss: 0.6729
06/30 01:00:34 - mmengine - INFO - Iter(train) [4210/5198]  lr: 1.8387e-06  eta: 1:31:31  time: 7.2331  data_time: 0.0036  memory: 46807  loss: 0.7684
06/30 01:01:33 - mmengine - INFO - Iter(train) [4220/5198]  lr: 1.8028e-06  eta: 1:30:36  time: 5.9041  data_time: 0.0036  memory: 36510  loss: 0.8623
06/30 01:02:24 - mmengine - INFO - Iter(train) [4230/5198]  lr: 1.7673e-06  eta: 1:29:40  time: 5.0416  data_time: 0.0036  memory: 33712  loss: 0.9965
06/30 01:03:12 - mmengine - INFO - Iter(train) [4240/5198]  lr: 1.7321e-06  eta: 1:28:42  time: 4.8180  data_time: 0.0036  memory: 33039  loss: 0.7752
06/30 01:03:57 - mmengine - INFO - Iter(train) [4250/5198]  lr: 1.6972e-06  eta: 1:27:44  time: 4.5348  data_time: 0.0035  memory: 32418  loss: 0.7632
06/30 01:05:10 - mmengine - INFO - Iter(train) [4260/5198]  lr: 1.6627e-06  eta: 1:26:53  time: 7.2978  data_time: 0.0035  memory: 46807  loss: 0.7563
06/30 01:06:08 - mmengine - INFO - Iter(train) [4270/5198]  lr: 1.6284e-06  eta: 1:25:58  time: 5.7903  data_time: 0.0035  memory: 36313  loss: 0.8049
06/30 01:06:58 - mmengine - INFO - Iter(train) [4280/5198]  lr: 1.5945e-06  eta: 1:25:01  time: 4.9962  data_time: 0.0036  memory: 33671  loss: 0.9597
06/30 01:07:46 - mmengine - INFO - Iter(train) [4290/5198]  lr: 1.5609e-06  eta: 1:24:04  time: 4.7679  data_time: 0.0035  memory: 33029  loss: 0.7553
06/30 01:08:31 - mmengine - INFO - Iter(train) [4300/5198]  lr: 1.5277e-06  eta: 1:23:06  time: 4.4961  data_time: 0.0035  memory: 32418  loss: 0.7126
06/30 01:09:44 - mmengine - INFO - Iter(train) [4310/5198]  lr: 1.4947e-06  eta: 1:22:14  time: 7.3262  data_time: 0.0035  memory: 46807  loss: 0.7379
06/30 01:10:43 - mmengine - INFO - Iter(train) [4320/5198]  lr: 1.4621e-06  eta: 1:21:19  time: 5.9410  data_time: 0.0036  memory: 36592  loss: 0.7731
06/30 01:11:34 - mmengine - INFO - Iter(train) [4330/5198]  lr: 1.4299e-06  eta: 1:20:23  time: 5.0445  data_time: 0.0035  memory: 33837  loss: 0.9865
06/30 01:12:22 - mmengine - INFO - Iter(train) [4340/5198]  lr: 1.3979e-06  eta: 1:19:26  time: 4.8023  data_time: 0.0035  memory: 33070  loss: 0.7117
06/30 01:13:07 - mmengine - INFO - Iter(train) [4350/5198]  lr: 1.3663e-06  eta: 1:18:28  time: 4.5055  data_time: 0.0034  memory: 32428  loss: 0.6644
06/30 01:14:20 - mmengine - INFO - Iter(train) [4360/5198]  lr: 1.3351e-06  eta: 1:17:36  time: 7.2997  data_time: 0.0035  memory: 46807  loss: 0.7636
06/30 01:15:18 - mmengine - INFO - Iter(train) [4370/5198]  lr: 1.3041e-06  eta: 1:16:41  time: 5.8169  data_time: 0.0035  memory: 36427  loss: 0.8134
06/30 01:16:08 - mmengine - INFO - Iter(train) [4380/5198]  lr: 1.2736e-06  eta: 1:15:44  time: 5.0164  data_time: 0.0036  memory: 33681  loss: 0.9237
06/30 01:16:56 - mmengine - INFO - Iter(train) [4390/5198]  lr: 1.2433e-06  eta: 1:14:47  time: 4.7915  data_time: 0.0035  memory: 33029  loss: 0.6998
06/30 01:17:41 - mmengine - INFO - Iter(train) [4400/5198]  lr: 1.2134e-06  eta: 1:13:50  time: 4.5277  data_time: 0.0035  memory: 32408  loss: 0.9459
06/30 01:18:55 - mmengine - INFO - Iter(train) [4410/5198]  lr: 1.1838e-06  eta: 1:12:57  time: 7.3255  data_time: 0.0035  memory: 46807  loss: 0.7453
06/30 01:19:54 - mmengine - INFO - Iter(train) [4420/5198]  lr: 1.1546e-06  eta: 1:12:02  time: 5.8788  data_time: 0.0035  memory: 36530  loss: 0.7928
06/30 01:20:44 - mmengine - INFO - Iter(train) [4430/5198]  lr: 1.1257e-06  eta: 1:11:06  time: 5.0185  data_time: 0.0035  memory: 33774  loss: 0.9515
06/30 01:21:32 - mmengine - INFO - Iter(train) [4440/5198]  lr: 1.0972e-06  eta: 1:10:09  time: 4.7826  data_time: 0.0035  memory: 33039  loss: 0.7387
06/30 01:22:17 - mmengine - INFO - Iter(train) [4450/5198]  lr: 1.0690e-06  eta: 1:09:12  time: 4.5010  data_time: 0.0035  memory: 32428  loss: 0.6645
06/30 01:23:41 - mmengine - INFO - Iter(train) [4460/5198]  lr: 1.0411e-06  eta: 1:08:21  time: 8.4248  data_time: 0.0035  memory: 40851  loss: 0.4202
06/30 01:24:47 - mmengine - INFO - Iter(train) [4470/5198]  lr: 1.0136e-06  eta: 1:07:27  time: 6.5709  data_time: 0.0035  memory: 38789  loss: 0.3841
06/30 01:25:35 - mmengine - INFO - Iter(train) [4480/5198]  lr: 9.8645e-07  eta: 1:06:30  time: 4.8393  data_time: 0.0035  memory: 34407  loss: 0.3158
06/30 01:26:14 - mmengine - INFO - Iter(train) [4490/5198]  lr: 9.5965e-07  eta: 1:05:32  time: 3.9079  data_time: 0.0035  memory: 31392  loss: 0.2333
06/30 01:26:48 - mmengine - INFO - Iter(train) [4500/5198]  lr: 9.3320e-07  eta: 1:04:33  time: 3.3994  data_time: 0.0035  memory: 29724  loss: 0.1775
06/30 01:26:48 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 01:26:51 - mmengine - INFO - Sample output:
USER: <image>
请描述一下这张照片 ASSISTANT:The image features a serene scene of a dock extending out over a lake. The dock is surrounded by a lush green forest, creating a peaceful atmosphere. The water is calm and reflective, adding to the tranquil ambiance.

There are several benches placed along the dock, providing a place for visitors to sit and enjoy the view. The benches are positioned at various locations along the dock, offering different perspectives of the lake and the surrounding nature.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 01:26:53 - mmengine - INFO - Sample output:
USER: <image>
Please describe this picture ASSISTANT:The image features a serene scene with a wooden dock extending out over a calm lake. The dock is surrounded by a lush forest, creating a peaceful atmosphere. The water is crystal clear, reflecting the beauty of the surroundings.

There are several benches placed along the dock, inviting visitors to sit and enjoy the view. The benches are positioned at various points along the dock, providing ample seating options for those who wish to relax and take in the picturesque scenery.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 01:26:53 - mmengine - INFO - Saving checkpoint at 4500 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-30 01:27:00,251] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_4500.pth is about to be saved!
[2024-06-30 01:27:00,263] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/mp_rank_00_model_states.pt
[2024-06-30 01:27:00,263] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/mp_rank_00_model_states.pt...
[2024-06-30 01:27:15,291] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/mp_rank_00_model_states.pt.
[2024-06-30 01:27:15,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-30 01:27:15,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-30 01:27:15,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-30 01:27:15,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-30 01:27:15,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-30 01:27:15,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-30 01:27:15,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-30 01:27:15,294] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-30 01:27:27,187] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-30 01:27:27,188] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-30 01:27:27,188] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_4500.pth is ready now!
[2024-06-30 01:27:27,285] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-30 01:27:27,285] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-30 01:27:27,285] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_4500.pth is ready now!
[2024-06-30 01:27:27,322] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-30 01:27:27,322] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-30 01:27:27,322] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_4500.pth is ready now!
[2024-06-30 01:27:27,372] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-30 01:27:27,372] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-30 01:27:27,372] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_4500.pth is ready now!
[2024-06-30 01:27:27,408] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-30 01:27:27,409] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-30 01:27:27,409] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_4500.pth is ready now!
[2024-06-30 01:27:27,441] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-30 01:27:27,441] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-30 01:27:27,441] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_4500.pth is ready now!
[2024-06-30 01:27:27,513] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-30 01:27:27,513] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-30 01:27:27,513] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_4500.pth is ready now!
[2024-06-30 01:27:27,520] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-30 01:27:27,520] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_4500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-30 01:27:27,520] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_4500.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/30 01:28:40 - mmengine - INFO - Iter(train) [4510/5198]  lr: 9.0709e-07  eta: 1:03:46  time: 11.1863  data_time: 3.9071  memory: 46807  loss: 0.7591
06/30 01:29:38 - mmengine - INFO - Iter(train) [4520/5198]  lr: 8.8135e-07  eta: 1:02:51  time: 5.8431  data_time: 0.0035  memory: 36478  loss: 0.7947
06/30 01:30:29 - mmengine - INFO - Iter(train) [4530/5198]  lr: 8.5595e-07  eta: 1:01:55  time: 5.0307  data_time: 0.0034  memory: 33712  loss: 0.9381
06/30 01:31:16 - mmengine - INFO - Iter(train) [4540/5198]  lr: 8.3091e-07  eta: 1:00:58  time: 4.7905  data_time: 0.0034  memory: 33029  loss: 0.7526
06/30 01:32:02 - mmengine - INFO - Iter(train) [4550/5198]  lr: 8.0623e-07  eta: 1:00:01  time: 4.5129  data_time: 0.0034  memory: 32418  loss: 0.6445
06/30 01:33:15 - mmengine - INFO - Iter(train) [4560/5198]  lr: 7.8190e-07  eta: 0:59:08  time: 7.3026  data_time: 0.0035  memory: 46807  loss: 0.7525
06/30 01:34:13 - mmengine - INFO - Iter(train) [4570/5198]  lr: 7.5793e-07  eta: 0:58:13  time: 5.8640  data_time: 0.0035  memory: 36551  loss: 0.7723
06/30 01:35:03 - mmengine - INFO - Iter(train) [4580/5198]  lr: 7.3432e-07  eta: 0:57:16  time: 5.0114  data_time: 0.0035  memory: 33712  loss: 0.9274
06/30 01:35:51 - mmengine - INFO - Iter(train) [4590/5198]  lr: 7.1107e-07  eta: 0:56:20  time: 4.7789  data_time: 0.0035  memory: 33029  loss: 0.7223
06/30 01:36:36 - mmengine - INFO - Iter(train) [4600/5198]  lr: 6.8818e-07  eta: 0:55:23  time: 4.5079  data_time: 0.0035  memory: 32428  loss: 0.6964
06/30 01:37:49 - mmengine - INFO - Iter(train) [4610/5198]  lr: 6.6565e-07  eta: 0:54:29  time: 7.3067  data_time: 0.0035  memory: 46807  loss: 0.7385
06/30 01:38:48 - mmengine - INFO - Iter(train) [4620/5198]  lr: 6.4348e-07  eta: 0:53:34  time: 5.8602  data_time: 0.0035  memory: 36551  loss: 0.7342
06/30 01:39:38 - mmengine - INFO - Iter(train) [4630/5198]  lr: 6.2168e-07  eta: 0:52:38  time: 5.0012  data_time: 0.0035  memory: 33702  loss: 0.9452
06/30 01:40:26 - mmengine - INFO - Iter(train) [4640/5198]  lr: 6.0024e-07  eta: 0:51:41  time: 4.7798  data_time: 0.0035  memory: 33039  loss: 0.7426
06/30 01:41:11 - mmengine - INFO - Iter(train) [4650/5198]  lr: 5.7916e-07  eta: 0:50:44  time: 4.5009  data_time: 0.0035  memory: 32418  loss: 0.5797
06/30 01:42:24 - mmengine - INFO - Iter(train) [4660/5198]  lr: 5.5845e-07  eta: 0:49:51  time: 7.3270  data_time: 0.0035  memory: 46807  loss: 0.7600
06/30 01:43:22 - mmengine - INFO - Iter(train) [4670/5198]  lr: 5.3811e-07  eta: 0:48:56  time: 5.8227  data_time: 0.0035  memory: 36427  loss: 0.8224
06/30 01:44:12 - mmengine - INFO - Iter(train) [4680/5198]  lr: 5.1813e-07  eta: 0:47:59  time: 5.0079  data_time: 0.0035  memory: 33692  loss: 0.9274
06/30 01:45:00 - mmengine - INFO - Iter(train) [4690/5198]  lr: 4.9852e-07  eta: 0:47:03  time: 4.7827  data_time: 0.0036  memory: 33018  loss: 0.7371
06/30 01:45:45 - mmengine - INFO - Iter(train) [4700/5198]  lr: 4.7928e-07  eta: 0:46:06  time: 4.5103  data_time: 0.0035  memory: 32418  loss: 0.6811
06/30 01:46:58 - mmengine - INFO - Iter(train) [4710/5198]  lr: 4.6041e-07  eta: 0:45:12  time: 7.3232  data_time: 0.0035  memory: 46807  loss: 0.7442
06/30 01:47:57 - mmengine - INFO - Iter(train) [4720/5198]  lr: 4.4191e-07  eta: 0:44:17  time: 5.8091  data_time: 0.0035  memory: 36354  loss: 0.8254
06/30 01:48:47 - mmengine - INFO - Iter(train) [4730/5198]  lr: 4.2379e-07  eta: 0:43:21  time: 5.0042  data_time: 0.0035  memory: 33671  loss: 0.9539
06/30 01:49:34 - mmengine - INFO - Iter(train) [4740/5198]  lr: 4.0603e-07  eta: 0:42:25  time: 4.7697  data_time: 0.0035  memory: 33029  loss: 0.6981
06/30 01:50:19 - mmengine - INFO - Iter(train) [4750/5198]  lr: 3.8864e-07  eta: 0:41:28  time: 4.5015  data_time: 0.0035  memory: 32408  loss: 0.5529
06/30 01:51:32 - mmengine - INFO - Iter(train) [4760/5198]  lr: 3.7163e-07  eta: 0:40:34  time: 7.2832  data_time: 0.0035  memory: 46807  loss: 0.7119
06/30 01:52:31 - mmengine - INFO - Iter(train) [4770/5198]  lr: 3.5499e-07  eta: 0:39:39  time: 5.8421  data_time: 0.0035  memory: 36427  loss: 0.7415
06/30 01:53:21 - mmengine - INFO - Iter(train) [4780/5198]  lr: 3.3873e-07  eta: 0:38:43  time: 5.0164  data_time: 0.0035  memory: 33712  loss: 1.0026
06/30 01:54:09 - mmengine - INFO - Iter(train) [4790/5198]  lr: 3.2284e-07  eta: 0:37:46  time: 4.7742  data_time: 0.0034  memory: 33039  loss: 0.7403
06/30 01:54:54 - mmengine - INFO - Iter(train) [4800/5198]  lr: 3.0733e-07  eta: 0:36:50  time: 4.5039  data_time: 0.0035  memory: 32428  loss: 0.7879
06/30 01:56:07 - mmengine - INFO - Iter(train) [4810/5198]  lr: 2.9219e-07  eta: 0:35:56  time: 7.3212  data_time: 0.0036  memory: 46807  loss: 0.7233
06/30 01:57:06 - mmengine - INFO - Iter(train) [4820/5198]  lr: 2.7743e-07  eta: 0:35:01  time: 5.9278  data_time: 0.0035  memory: 36592  loss: 0.7768
06/30 01:57:56 - mmengine - INFO - Iter(train) [4830/5198]  lr: 2.6305e-07  eta: 0:34:05  time: 5.0405  data_time: 0.0035  memory: 33774  loss: 0.9785
06/30 01:58:44 - mmengine - INFO - Iter(train) [4840/5198]  lr: 2.4904e-07  eta: 0:33:09  time: 4.8058  data_time: 0.0035  memory: 33049  loss: 0.7135
06/30 01:59:30 - mmengine - INFO - Iter(train) [4850/5198]  lr: 2.3541e-07  eta: 0:32:12  time: 4.5158  data_time: 0.0035  memory: 32428  loss: 0.8599
06/30 02:00:42 - mmengine - INFO - Iter(train) [4860/5198]  lr: 2.2217e-07  eta: 0:31:18  time: 7.2266  data_time: 0.0035  memory: 46807  loss: 0.7594
06/30 02:01:40 - mmengine - INFO - Iter(train) [4870/5198]  lr: 2.0930e-07  eta: 0:30:22  time: 5.8350  data_time: 0.0036  memory: 36447  loss: 0.8570
06/30 02:02:30 - mmengine - INFO - Iter(train) [4880/5198]  lr: 1.9681e-07  eta: 0:29:27  time: 5.0183  data_time: 0.0036  memory: 33640  loss: 0.9646
06/30 02:03:18 - mmengine - INFO - Iter(train) [4890/5198]  lr: 1.8470e-07  eta: 0:28:30  time: 4.7888  data_time: 0.0036  memory: 33029  loss: 0.7832
06/30 02:04:03 - mmengine - INFO - Iter(train) [4900/5198]  lr: 1.7297e-07  eta: 0:27:34  time: 4.5109  data_time: 0.0036  memory: 32408  loss: 0.6173
06/30 02:05:17 - mmengine - INFO - Iter(train) [4910/5198]  lr: 1.6163e-07  eta: 0:26:40  time: 7.3255  data_time: 0.0035  memory: 46807  loss: 0.7722
06/30 02:06:15 - mmengine - INFO - Iter(train) [4920/5198]  lr: 1.5066e-07  eta: 0:25:44  time: 5.8671  data_time: 0.0035  memory: 36541  loss: 0.8008
06/30 02:07:05 - mmengine - INFO - Iter(train) [4930/5198]  lr: 1.4008e-07  eta: 0:24:49  time: 4.9803  data_time: 0.0035  memory: 33712  loss: 0.9356
06/30 02:07:53 - mmengine - INFO - Iter(train) [4940/5198]  lr: 1.2988e-07  eta: 0:23:53  time: 4.7692  data_time: 0.0036  memory: 33039  loss: 0.7206
06/30 02:08:38 - mmengine - INFO - Iter(train) [4950/5198]  lr: 1.2007e-07  eta: 0:22:56  time: 4.4991  data_time: 0.0036  memory: 32408  loss: 0.8310
06/30 02:09:51 - mmengine - INFO - Iter(train) [4960/5198]  lr: 1.1063e-07  eta: 0:22:02  time: 7.3195  data_time: 0.0036  memory: 46807  loss: 0.7565
06/30 02:10:50 - mmengine - INFO - Iter(train) [4970/5198]  lr: 1.0158e-07  eta: 0:21:06  time: 5.8879  data_time: 0.0036  memory: 36572  loss: 0.7896
06/30 02:11:40 - mmengine - INFO - Iter(train) [4980/5198]  lr: 9.2919e-08  eta: 0:20:11  time: 5.0309  data_time: 0.0036  memory: 33723  loss: 0.9826
06/30 02:12:28 - mmengine - INFO - Iter(train) [4990/5198]  lr: 8.4639e-08  eta: 0:19:15  time: 4.7920  data_time: 0.0035  memory: 33049  loss: 0.6952
06/30 02:13:13 - mmengine - INFO - Exp name: llava_v15_7b_finetune_copy_20240629_180208
06/30 02:13:13 - mmengine - INFO - Iter(train) [5000/5198]  lr: 7.6744e-08  eta: 0:18:19  time: 4.5209  data_time: 0.0035  memory: 32428  loss: 0.6746
06/30 02:13:13 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 02:13:15 - mmengine - INFO - Sample output:
USER: <image>
请描述一下这张照片 ASSISTANT:The image features a serene scene with a wooden dock extending out over a calm lake. The dock is surrounded by a lush forest, creating a peaceful atmosphere. The water is crystal clear, reflecting the beauty of the natural surroundings. The dock appears to be empty, inviting viewers to imagine themselves enjoying a quiet moment by the lake.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 02:13:17 - mmengine - INFO - Sample output:
USER: <image>
Please describe this picture ASSISTANT:The image features a wooden dock extending out over a calm lake. The dock is surrounded by a serene landscape with trees and mountains in the background. The water is still and reflective, creating a peaceful atmosphere. The dock appears to be empty, with no people or boats visible in the scene.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 02:13:17 - mmengine - INFO - Saving checkpoint at 5000 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-30 02:13:23,371] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_5000.pth is about to be saved!
[2024-06-30 02:13:23,383] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/mp_rank_00_model_states.pt
[2024-06-30 02:13:23,383] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/mp_rank_00_model_states.pt...
[2024-06-30 02:13:38,398] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/mp_rank_00_model_states.pt.
[2024-06-30 02:13:38,401] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-30 02:13:38,401] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-30 02:13:38,401] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-30 02:13:38,401] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-30 02:13:38,401] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-30 02:13:38,401] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-30 02:13:38,401] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-30 02:13:38,401] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-30 02:13:50,546] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-30 02:13:50,546] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-30 02:13:50,546] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_5000.pth is ready now!
[2024-06-30 02:13:50,547] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-30 02:13:50,547] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-30 02:13:50,547] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_5000.pth is ready now!
[2024-06-30 02:13:50,551] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-30 02:13:50,552] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-30 02:13:50,552] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_5000.pth is ready now!
[2024-06-30 02:13:50,629] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-30 02:13:50,630] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-30 02:13:50,630] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_5000.pth is ready now!
[2024-06-30 02:13:50,710] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-30 02:13:50,710] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-30 02:13:50,711] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_5000.pth is ready now!
[2024-06-30 02:13:50,911] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-30 02:13:50,911] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-30 02:13:50,911] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_5000.pth is ready now!
[2024-06-30 02:13:50,917] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-30 02:13:50,917] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-30 02:13:50,917] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_5000.pth is ready now!
[2024-06-30 02:13:50,929] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-30 02:13:50,929] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-30 02:13:50,929] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_5000.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/30 02:15:03 - mmengine - INFO - Iter(train) [5010/5198]  lr: 6.9233e-08  eta: 0:17:25  time: 10.9996  data_time: 3.7088  memory: 46807  loss: 0.7735
06/30 02:16:02 - mmengine - INFO - Iter(train) [5020/5198]  lr: 6.2108e-08  eta: 0:16:30  time: 5.8732  data_time: 0.0036  memory: 36478  loss: 0.7825
06/30 02:16:52 - mmengine - INFO - Iter(train) [5030/5198]  lr: 5.5369e-08  eta: 0:15:34  time: 5.0210  data_time: 0.0036  memory: 33723  loss: 0.9900
06/30 02:17:40 - mmengine - INFO - Iter(train) [5040/5198]  lr: 4.9015e-08  eta: 0:14:38  time: 4.7860  data_time: 0.0036  memory: 33039  loss: 0.7959
06/30 02:18:25 - mmengine - INFO - Iter(train) [5050/5198]  lr: 4.3048e-08  eta: 0:13:42  time: 4.5010  data_time: 0.0035  memory: 32428  loss: 0.6952
06/30 02:19:38 - mmengine - INFO - Iter(train) [5060/5198]  lr: 3.7467e-08  eta: 0:12:47  time: 7.2906  data_time: 0.0035  memory: 46807  loss: 0.7626
06/30 02:20:36 - mmengine - INFO - Iter(train) [5070/5198]  lr: 3.2273e-08  eta: 0:11:52  time: 5.8368  data_time: 0.0035  memory: 36447  loss: 0.8294
06/30 02:21:27 - mmengine - INFO - Iter(train) [5080/5198]  lr: 2.7465e-08  eta: 0:10:56  time: 5.0099  data_time: 0.0035  memory: 33692  loss: 0.9586
06/30 02:22:14 - mmengine - INFO - Iter(train) [5090/5198]  lr: 2.3045e-08  eta: 0:10:00  time: 4.7720  data_time: 0.0035  memory: 33029  loss: 0.6242
06/30 02:22:59 - mmengine - INFO - Iter(train) [5100/5198]  lr: 1.9012e-08  eta: 0:09:04  time: 4.5129  data_time: 0.0035  memory: 32418  loss: 0.7779
06/30 02:24:13 - mmengine - INFO - Iter(train) [5110/5198]  lr: 1.5366e-08  eta: 0:08:09  time: 7.3204  data_time: 0.0035  memory: 46807  loss: 0.7644
06/30 02:25:11 - mmengine - INFO - Iter(train) [5120/5198]  lr: 1.2108e-08  eta: 0:07:13  time: 5.8615  data_time: 0.0035  memory: 36541  loss: 0.8117
06/30 02:26:02 - mmengine - INFO - Iter(train) [5130/5198]  lr: 9.2368e-09  eta: 0:06:18  time: 5.0270  data_time: 0.0035  memory: 33712  loss: 0.9635
06/30 02:26:49 - mmengine - INFO - Iter(train) [5140/5198]  lr: 6.7538e-09  eta: 0:05:22  time: 4.7808  data_time: 0.0035  memory: 33029  loss: 0.7342
06/30 02:27:35 - mmengine - INFO - Iter(train) [5150/5198]  lr: 4.6585e-09  eta: 0:04:26  time: 4.5197  data_time: 0.0035  memory: 32418  loss: 0.6742
06/30 02:29:15 - mmengine - INFO - Iter(train) [5160/5198]  lr: 2.9512e-09  eta: 0:03:31  time: 10.0931  data_time: 0.0035  memory: 46935  loss: 0.4174
06/30 02:30:29 - mmengine - INFO - Iter(train) [5170/5198]  lr: 1.6318e-09  eta: 0:02:35  time: 7.3203  data_time: 0.0035  memory: 38389  loss: 0.5196
06/30 02:31:27 - mmengine - INFO - Iter(train) [5180/5198]  lr: 7.0048e-10  eta: 0:01:40  time: 5.8803  data_time: 0.0035  memory: 35855  loss: 0.4887
06/30 02:32:18 - mmengine - INFO - Iter(train) [5190/5198]  lr: 1.5717e-10  eta: 0:00:44  time: 5.1051  data_time: 0.0035  memory: 33367  loss: 0.6323
06/30 02:33:05 - mmengine - INFO - Exp name: llava_v15_7b_finetune_copy_20240629_180208
06/30 02:33:05 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 02:33:08 - mmengine - INFO - Sample output:
USER: <image>
请描述一下这张照片 ASSISTANT:The image features a serene scene of a dock extending out into a large body of water, possibly a lake. The dock is surrounded by a lush forest, creating a peaceful and picturesque setting. The water is calm and reflective, adding to the tranquil atmosphere.

There are several benches placed along the dock, providing a place for visitors to sit and enjoy the view. The benches are positioned at various points along the dock, offering different vantage points to appreciate the beauty of the lake and the surrounding nature.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 02:33:10 - mmengine - INFO - Sample output:
USER: <image>
Please describe this picture ASSISTANT:The image features a wooden pier extending out over a calm lake, with a beautiful view of the mountains in the background. The pier is situated near the water's edge, providing a serene spot for relaxation and contemplation. The scene is further enhanced by the presence of a few boats floating on the lake, adding to the tranquil atmosphere.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 02:33:10 - mmengine - INFO - Saving checkpoint at 5198 iterations
[2024-06-30 02:33:16,132] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_5198.pth is about to be saved!
[2024-06-30 02:33:16,224] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/mp_rank_00_model_states.pt
[2024-06-30 02:33:16,225] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/mp_rank_00_model_states.pt...
[2024-06-30 02:33:31,046] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/mp_rank_00_model_states.pt.
[2024-06-30 02:33:31,049] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-30 02:33:31,049] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-30 02:33:31,049] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-30 02:33:31,049] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-30 02:33:31,049] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-30 02:33:31,049] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-30 02:33:31,049] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-30 02:33:31,049] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-30 02:33:43,142] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-30 02:33:43,142] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-30 02:33:43,142] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_5198.pth is ready now!
[2024-06-30 02:33:43,146] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-30 02:33:43,146] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-30 02:33:43,146] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_5198.pth is ready now!
[2024-06-30 02:33:43,164] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-30 02:33:43,164] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-30 02:33:43,164] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_5198.pth is ready now!
[2024-06-30 02:33:43,165] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-30 02:33:43,166] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-30 02:33:43,166] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_5198.pth is ready now!
[2024-06-30 02:33:43,166] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-30 02:33:43,166] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-30 02:33:43,166] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_5198.pth is ready now!
[2024-06-30 02:33:43,196] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-30 02:33:43,197] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-30 02:33:43,197] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_5198.pth is ready now!
[2024-06-30 02:33:43,245] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-30 02:33:43,246] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-30 02:33:43,246] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_5198.pth is ready now!
[2024-06-30 02:33:43,309] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-30 02:33:43,309] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_v15_7b_finetune_copy/iter_5198.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-30 02:33:43,309] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_5198.pth is ready now!
06/30 02:33:43 - mmengine - INFO - after_train in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 02:33:46 - mmengine - INFO - Sample output:
USER: <image>
请描述一下这张照片 ASSISTANT:The image features a serene scene with a wooden dock extending out over a calm lake. The dock is surrounded by a lush forest, creating a peaceful atmosphere. The water is a deep blue, reflecting the tranquility of the surroundings.

There are several benches placed along the dock, providing a place for visitors to sit and enjoy the view. The benches are positioned at various points along the dock, offering different perspectives of the lake and the forest.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 02:33:48 - mmengine - INFO - Sample output:
USER: <image>
Please describe this picture ASSISTANT:The image features a wooden dock extending out over a large body of water, possibly a lake. The dock is surrounded by a serene landscape with trees in the background, creating a peaceful atmosphere. The water is calm and reflective, adding to the tranquil setting.

There are several benches placed along the dock, providing a place for people to sit and enjoy the view. The benches are positioned at various points along the dock, offering different vantage points for observing the surrounding nature.</s>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
wandb: - 5.149 MB of 5.149 MB uploaded (5.128 MB deduped)wandb: \ 5.149 MB of 5.149 MB uploaded (5.128 MB deduped)wandb: | 5.149 MB of 5.149 MB uploaded (5.128 MB deduped)wandb: / 5.149 MB of 5.149 MB uploaded (5.128 MB deduped)wandb: - 5.149 MB of 5.149 MB uploaded (5.128 MB deduped)wandb: \ 5.149 MB of 5.149 MB uploaded (5.128 MB deduped)wandb: | 5.149 MB of 5.149 MB uploaded (5.128 MB deduped)wandb: / 5.149 MB of 5.149 MB uploaded (5.128 MB deduped)wandb: - 5.149 MB of 5.149 MB uploaded (5.128 MB deduped)wandb: \ 5.184 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: | 5.331 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: / 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: - 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: \ 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: | 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: / 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: - 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: \ 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: | 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: / 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: - 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: \ 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: | 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: / 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: - 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: \ 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: | 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: / 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: - 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: \ 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: | 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: / 5.335 MB of 5.341 MB uploaded (5.132 MB deduped)wandb: - 5.341 MB of 5.341 MB uploaded (5.132 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 96.1%             
wandb: 
wandb: Run history:
wandb: data_time ▁▃▃▁▄▂▁▃▂▂▄▄▂▃▁▂▂▄▂▄▄▄▄▄▅▅▅█▆▆▃▅▆▅▃▃▄▃▆▃
wandb:      iter ▁▁▁▁▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:      loss █▆▆▅▆▆▅▅▅▆▅▆▆▃▅▄▁▄▄▆▅▅▇▅▅▇▄▅▇▅▅▆▅▅▆▅▅▅▅▅
wandb:        lr ▅███████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▄▃▃▃▂▂▂▂▂▂▁▁▁▁▁▁
wandb:    memory ▂█▃▁█▃▁█▁▁█▁▁▅▁▁▂▁▁▂▁▃▂▁▃▂▁▃▂█▃▂█▃▁█▃▁█▃
wandb:      time ▂▆▃▁▆▃▁▆▁▁▆▁▁█▁▁▂▁▁▂▁▃▂▂▃▂▂▃▂▆▃▂▆▄▁▆▃▁▆▃
wandb: 
wandb: Run summary:
wandb: data_time 0.00353
wandb:      iter 5190
wandb:      loss 0.63229
wandb:        lr 0.0
wandb:    memory 33367
wandb:      time 5.10513
wandb: 
wandb: 🚀 View run worthy-cherry-4 at: https://wandb.ai/jzyztzn/lava-clipL32-vicuna7b/runs/wfwfvnuj
wandb: ⭐️ View project at: https://wandb.ai/jzyztzn/lava-clipL32-vicuna7b
wandb: Synced 6 W&B file(s), 0 media file(s), 764 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./work_dirs/llava_v15_7b_finetune_copy/20240629_180208/vis_data/wandb/run-20240629_180215-wfwfvnuj/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
