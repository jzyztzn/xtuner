nohup: ignoring input
06/30 11:21:08 - mmengine - WARNING - Urun_llavaphi3.sh: 4: xtuner: not found
:09,605] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 11:21:17,615] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 11:21:17,701] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 11:21:17,706] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 11:21:17,753] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 11:21:17,755] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 11:21:17,756] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 11:21:17,793] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[2024-06-30 11:21:17,798] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m async_io: please install the libaio-dev package with apt

[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.

[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH

[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[2024-06-30 11:21:19,784] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 11:21:19,784] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-06-30 11:21:20,497] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 11:21:20,696] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 11:21:20,699] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 11:21:20,746] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 11:21:20,750] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 11:21:20,754] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 11:21:20,757] [INFO] [comm.py:637:init_distributed] cdb=None
06/30 11:21:24 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 576542663
    GPU 0,1,2,3,4,5,6,7: NVIDIA A800 80GB PCIe
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 12.1, V12.1.105
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.3.1+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.18.1+cu121
    OpenCV: 4.10.0
    MMEngine: 0.10.4

Runtime environment:
    launcher: pytorch
    randomness: {'seed': None, 'deterministic': False}
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: None
    deterministic: False
    Distributed launcher: pytorch
    Distributed training: True
    GPU number: 8
------------------------------------------------------------

06/30 11:21:25 - mmengine - INFO - Config:
SYSTEM = ''
accumulative_counts = 1
batch_size = 32
betas = (
    0.9,
    0.999,
)
custom_hooks = [
    dict(
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.engine.hooks.DatasetInfoHook'),
    dict(
        evaluation_images='view.jpg',
        evaluation_inputs=[
            '请描述一下这张照片',
            'Please describe this picture',
        ],
        every_n_iters=500,
        image_processor=dict(
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
            trust_remote_code=True,
            type='transformers.CLIPImageProcessor.from_pretrained'),
        prompt_template='xtuner.utils.PROMPT_TEMPLATE.phi3_chat',
        system='',
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.engine.hooks.EvaluateChatHook'),
]
data_path = './data/llava_data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json'
data_root = './data/llava_data/'
dataloader_num_workers = 4
default_hooks = dict(
    checkpoint=dict(
        by_epoch=False,
        interval=500,
        max_keep_ckpts=2,
        type='mmengine.hooks.CheckpointHook'),
    logger=dict(
        interval=10,
        log_metric_by_epoch=False,
        type='mmengine.hooks.LoggerHook'),
    param_scheduler=dict(type='mmengine.hooks.ParamSchedulerHook'),
    sampler_seed=dict(type='mmengine.hooks.DistSamplerSeedHook'),
    timer=dict(type='mmengine.hooks.IterTimerHook'))
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
evaluation_freq = 500
evaluation_images = 'view.jpg'
evaluation_inputs = [
    '请描述一下这张照片',
    'Please describe this picture',
]
image_folder = './data/llava_data/LLaVA-Pretrain/images'
image_processor = dict(
    pretrained_model_name_or_path=
    '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
    trust_remote_code=True,
    type='transformers.CLIPImageProcessor.from_pretrained')
launcher = 'pytorch'
llava_dataset = dict(
    data_path='./data/llava_data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json',
    dataset_map_fn='xtuner.dataset.map_fns.llava_map_fn',
    image_folder='./data/llava_data/LLaVA-Pretrain/images',
    image_processor=dict(
        pretrained_model_name_or_path=
        '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
        trust_remote_code=True,
        type='transformers.CLIPImageProcessor.from_pretrained'),
    max_length=1472,
    pad_image_to_square=False,
    template_map_fn=dict(
        template='xtuner.utils.PROMPT_TEMPLATE.phi3_chat',
        type='xtuner.dataset.map_fns.template_map_fn_factory'),
    tokenizer=dict(
        padding_side='right',
        pretrained_model_name_or_path=
        '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
        trust_remote_code=True,
        type='transformers.AutoTokenizer.from_pretrained'),
    type='xtuner.dataset.LLaVADataset')
llm_name_or_path = '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct'
load_from = None
log_level = 'INFO'
log_processor = dict(by_epoch=False)
lr = 0.001
max_epochs = 1
max_length = 1472
max_norm = 1
model = dict(
    freeze_llm=True,
    freeze_visual_encoder=True,
    llm=dict(
        pretrained_model_name_or_path=
        '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
        trust_remote_code=True,
        type='transformers.AutoModelForCausalLM.from_pretrained'),
    type='xtuner.model.LLaVAModel',
    visual_encoder=dict(
        pretrained_model_name_or_path=
        '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
        type='transformers.CLIPVisionModel.from_pretrained'))
optim_type = 'torch.optim.AdamW'
optim_wrapper = dict(
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        lr=0.001,
        type='torch.optim.AdamW',
        weight_decay=0),
    type='DeepSpeedOptimWrapper')
param_scheduler = [
    dict(
        begin=0,
        by_epoch=True,
        convert_to_iter_based=True,
        end=0.03,
        start_factor=1e-05,
        type='mmengine.optim.LinearLR'),
    dict(
        begin=0.03,
        by_epoch=True,
        convert_to_iter_based=True,
        end=1,
        eta_min=0.0,
        type='mmengine.optim.CosineAnnealingLR'),
]
prompt_template = 'xtuner.utils.PROMPT_TEMPLATE.phi3_chat'
randomness = dict(deterministic=False, seed=None)
resume = False
runner_type = 'FlexibleRunner'
save_steps = 500
save_total_limit = 2
strategy = dict(
    config=dict(
        bf16=dict(enabled=True),
        fp16=dict(enabled=False, initial_scale_power=16),
        gradient_accumulation_steps='auto',
        gradient_clipping='auto',
        train_micro_batch_size_per_gpu='auto',
        zero_allow_untested_optimizer=True,
        zero_force_ds_cpu_optimizer=False,
        zero_optimization=dict(overlap_comm=True, stage=2)),
    exclude_frozen_parameters=True,
    gradient_accumulation_steps=1,
    gradient_clipping=1,
    sequence_parallel_size=1,
    train_micro_batch_size_per_gpu=32,
    type='xtuner.engine.DeepSpeedStrategy')
tokenizer = dict(
    padding_side='right',
    pretrained_model_name_or_path=
    '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
    trust_remote_code=True,
    type='transformers.AutoTokenizer.from_pretrained')
train_cfg = dict(max_epochs=1, type='xtuner.engine.runner.TrainLoop')
train_dataloader = dict(
    batch_size=32,
    collate_fn=dict(type='xtuner.dataset.collate_fns.default_collate_fn'),
    dataset=dict(
        data_path=
        './data/llava_data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json',
        dataset_map_fn='xtuner.dataset.map_fns.llava_map_fn',
        image_folder='./data/llava_data/LLaVA-Pretrain/images',
        image_processor=dict(
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
            trust_remote_code=True,
            type='transformers.CLIPImageProcessor.from_pretrained'),
        max_length=1472,
        pad_image_to_square=False,
        template_map_fn=dict(
            template='xtuner.utils.PROMPT_TEMPLATE.phi3_chat',
            type='xtuner.dataset.map_fns.template_map_fn_factory'),
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.dataset.LLaVADataset'),
    num_workers=4,
    pin_memory=True,
    sampler=dict(shuffle=True, type='mmengine.dataset.DefaultSampler'))
use_wandb = True
visual_encoder_name_or_path = '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336'
visualizer = dict(
    type='mmengine.visualization.Visualizer',
    vis_backends=[
        dict(
            init_kwargs=dict(project='lava-clipL32-Phi-3-mini'),
            type='mmengine.visualization.WandbVisBackend'),
    ])
wandb_name = 'lava-clipL32-Phi-3-mini'
warmup_ratio = 0.03
weight_decay = 0
work_dir = './work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy'

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
wandb: Currently logged in as: jzyztzn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/20240630_112119/vis_data/wandb/run-20240630_112126-r4hb3aac
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-durian-8
wandb: ⭐️ View project at https://wandb.ai/jzyztzn/lava-clipL32-Phi-3-mini
wandb: 🚀 View run at https://wandb.ai/jzyztzn/lava-clipL32-Phi-3-mini/runs/r4hb3aac
06/30 11:21:32 - mmengine - WARNING - Failed to search registry with scope "mmengine" in the "builder" registry tree. As a workaround, the current "builder" registry in "xtuner" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether "mmengine" is a correct scope, or whether the registry is initialized.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
06/30 11:21:33 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DatasetInfoHook                    
(LOW         ) EvaluateChatHook                   
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(LOW         ) EvaluateChatHook                   
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) DatasetInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
(LOW         ) EvaluateChatHook                   
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(LOW         ) EvaluateChatHook                   
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) DatasetInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
06/30 11:21:38 - mmengine - INFO - xtuner_dataset_timeout = 1:00:00
Map (num_proc=32):   0%|          | 0/558128 [00:00<?, ? examples/s]Map (num_proc=32):   0%|          | 987/558128 [00:00<07:16, 1275.14 examples/s]Map (num_proc=32):   0%|          | 2620/558128 [00:00<02:46, 3335.15 examples/s]Map (num_proc=32):   1%|          | 4341/558128 [00:01<01:39, 5569.19 examples/s]Map (num_proc=32):   1%|▏         | 7977/558128 [00:01<00:49, 11155.42 examples/s]Map (num_proc=32):   2%|▏         | 10757/558128 [00:01<00:41, 13111.69 examples/s]Map (num_proc=32):   2%|▏         | 13404/558128 [00:01<00:35, 15174.29 examples/s]Map (num_proc=32):   4%|▍         | 22943/558128 [00:01<00:17, 30302.91 examples/s]Map (num_proc=32):   5%|▍         | 27353/558128 [00:01<00:18, 28819.21 examples/s]Map (num_proc=32):   6%|▋         | 35549/558128 [00:01<00:15, 33962.00 examples/s]Map (num_proc=32):   8%|▊         | 43362/558128 [00:02<00:12, 42358.49 examples/s]Map (num_proc=32):   9%|▊         | 48307/558128 [00:02<00:13, 38633.77 examples/s]Map (num_proc=32):  10%|▉         | 53884/558128 [00:02<00:12, 40810.77 examples/s]Map (num_proc=32):  11%|█         | 59401/558128 [00:02<00:12, 40772.61 examples/s]Map (num_proc=32):  11%|█▏        | 63904/558128 [00:02<00:12, 39469.98 examples/s]Map (num_proc=32):  13%|█▎        | 71343/558128 [00:02<00:10, 47156.13 examples/s]Map (num_proc=32):  14%|█▍        | 77522/558128 [00:02<00:10, 45290.38 examples/s]Map (num_proc=32):  15%|█▍        | 82935/558128 [00:02<00:10, 44274.95 examples/s]Map (num_proc=32):  17%|█▋        | 92138/558128 [00:03<00:08, 54928.34 examples/s]Map (num_proc=32):  18%|█▊        | 98708/558128 [00:03<00:11, 40554.07 examples/s]Map (num_proc=32):  20%|█▉        | 108998/558128 [00:03<00:08, 52166.62 examples/s]Map (num_proc=32):  21%|██        | 115256/558128 [00:03<00:11, 39886.67 examples/s]Map (num_proc=32):  22%|██▏       | 123429/558128 [00:03<00:09, 46013.22 examples/s]Map (num_proc=32):  23%|██▎       | 129702/558128 [00:03<00:09, 44991.36 examples/s]Map (num_proc=32):  25%|██▍       | 138403/558128 [00:04<00:07, 53233.08 examples/s]Map (num_proc=32):  26%|██▌       | 145254/558128 [00:04<00:12, 34212.31 examples/s]Map (num_proc=32):  28%|██▊       | 158536/558128 [00:04<00:08, 48660.59 examples/s]Map (num_proc=32):  30%|██▉       | 165180/558128 [00:04<00:08, 47887.58 examples/s]Map (num_proc=32):  31%|███       | 171173/558128 [00:04<00:07, 49470.90 examples/s]Map (num_proc=32):  32%|███▏      | 177583/558128 [00:04<00:07, 50495.75 examples/s]Map (num_proc=32):  33%|███▎      | 183875/558128 [00:05<00:09, 39441.93 examples/s]Map (num_proc=32):  35%|███▍      | 193420/558128 [00:05<00:08, 42616.14 examples/s]Map (num_proc=32):  36%|███▌      | 198521/558128 [00:05<00:09, 38322.38 examples/s]Map (num_proc=32):  38%|███▊      | 210862/558128 [00:05<00:09, 35270.39 examples/s]Map (num_proc=32):  41%|████      | 228262/558128 [00:06<00:06, 47612.55 examples/s]Map (num_proc=32):  42%|████▏     | 233598/558128 [00:06<00:07, 44988.89 examples/s]Map (num_proc=32):  44%|████▍     | 245545/558128 [00:06<00:07, 39156.02 examples/s]Map (num_proc=32):  48%|████▊     | 265171/558128 [00:07<00:06, 41982.40 examples/s]Map (num_proc=32):  50%|█████     | 281630/558128 [00:07<00:04, 56853.32 examples/s]Map (num_proc=32):  52%|█████▏    | 290367/558128 [00:07<00:05, 50001.38 examples/s]Map (num_proc=32):  54%|█████▎    | 299078/558128 [00:07<00:04, 53948.09 examples/s]Map (num_proc=32):  55%|█████▍    | 306229/558128 [00:07<00:05, 48029.47 examples/s]Map (num_proc=32):  56%|█████▌    | 313646/558128 [00:07<00:04, 50993.50 examples/s]Map (num_proc=32):  57%|█████▋    | 320091/558128 [00:08<00:05, 45970.20 examples/s]Map (num_proc=32):  58%|█████▊    | 326101/558128 [00:08<00:04, 48303.73 examples/s]Map (num_proc=32):  60%|█████▉    | 332973/558128 [00:08<00:06, 33168.07 examples/s]Map (num_proc=32):  63%|██████▎   | 349206/558128 [00:09<00:05, 37471.84 examples/s]Map (num_proc=32):  66%|██████▌   | 367836/558128 [00:09<00:04, 47137.57 examples/s]Map (num_proc=32):  67%|██████▋   | 373596/558128 [00:09<00:03, 48435.47 examples/s]Map (num_proc=32):  69%|██████▉   | 386738/558128 [00:09<00:04, 42166.24 examples/s]Map (num_proc=32):  72%|███████▏  | 402718/558128 [00:10<00:03, 42689.58 examples/s]Map (num_proc=32):  75%|███████▌  | 421159/558128 [00:10<00:03, 44785.57 examples/s]Map (num_proc=32):  79%|███████▊  | 439352/558128 [00:10<00:01, 59798.26 examples/s]Map (num_proc=32):  80%|████████  | 447601/558128 [00:10<00:02, 51971.92 examples/s]Map (num_proc=32):  82%|████████▏ | 455041/558128 [00:11<00:02, 39284.96 examples/s]Map (num_proc=32):  85%|████████▍ | 474373/558128 [00:11<00:01, 58488.57 examples/s]Map (num_proc=32):  87%|████████▋ | 484068/558128 [00:11<00:01, 51327.51 examples/s]Map (num_proc=32):  88%|████████▊ | 492001/558128 [00:11<00:01, 39749.74 examples/s]Map (num_proc=32):  91%|█████████▏| 509977/558128 [00:12<00:00, 57601.31 examples/s]Map (num_proc=32):  93%|█████████▎| 519856/558128 [00:12<00:00, 55370.53 examples/s]Map (num_proc=32):  95%|█████████▍| 528657/558128 [00:12<00:00, 49167.88 examples/s]Map (num_proc=32):  96%|█████████▌| 535612/558128 [00:12<00:00, 50534.00 examples/s]Map (num_proc=32):  97%|█████████▋| 542246/558128 [00:12<00:00, 50952.96 examples/s]Map (num_proc=32):  98%|█████████▊| 548775/558128 [00:12<00:00, 44591.76 examples/s]Map (num_proc=32):  99%|█████████▉| 554771/558128 [00:13<00:00, 39945.91 examples/s]Map (num_proc=32): 100%|██████████| 558128/558128 [00:13<00:00, 40866.48 examples/s]
Map (num_proc=32):   0%|          | 0/558128 [00:00<?, ? examples/s]Map (num_proc=32):   0%|          | 886/558128 [00:00<01:55, 4832.88 examples/s]Map (num_proc=32):   1%|          | 3686/558128 [00:00<00:40, 13774.32 examples/s]Map (num_proc=32):   2%|▏         | 9294/558128 [00:00<00:19, 28611.93 examples/s]Map (num_proc=32):   3%|▎         | 16580/558128 [00:00<00:12, 42868.54 examples/s]Map (num_proc=32):   5%|▍         | 25939/558128 [00:00<00:09, 57418.93 examples/s]Map (num_proc=32):   6%|▋         | 35731/558128 [00:00<00:07, 68556.75 examples/s]Map (num_proc=32):  10%|▉         | 53895/558128 [00:00<00:05, 100627.12 examples/s]Map (num_proc=32):  12%|█▏        | 66513/558128 [00:00<00:04, 105678.19 examples/s]Map (num_proc=32):  16%|█▌        | 89029/558128 [00:01<00:03, 137893.91 examples/s]Map (num_proc=32):  19%|█▉        | 105267/558128 [00:01<00:03, 140402.07 examples/s]Map (num_proc=32):  24%|██▎       | 131606/558128 [00:01<00:02, 174866.22 examples/s]Map (num_proc=32):  27%|██▋       | 150332/558128 [00:01<00:02, 176573.12 examples/s]Map (num_proc=32):  31%|███▏      | 175670/558128 [00:01<00:02, 137691.38 examples/s]Map (num_proc=32):  37%|███▋      | 208300/558128 [00:01<00:02, 134272.90 examples/s]Map (num_proc=32):  41%|████      | 229958/558128 [00:02<00:02, 119723.98 examples/s]Map (num_proc=32):  44%|████▎     | 243355/558128 [00:02<00:03, 93161.06 examples/s] Map (num_proc=32):  46%|████▌     | 254562/558128 [00:02<00:03, 87947.59 examples/s]Map (num_proc=32):  49%|████▉     | 276166/558128 [00:02<00:02, 106355.27 examples/s]Map (num_proc=32):  52%|█████▏    | 288207/558128 [00:02<00:02, 104987.64 examples/s]Map (num_proc=32):  54%|█████▍    | 302766/558128 [00:02<00:02, 112704.52 examples/s]Map (num_proc=32):  56%|█████▋    | 315060/558128 [00:02<00:02, 111870.19 examples/s]Map (num_proc=32):  62%|██████▏   | 343870/558128 [00:03<00:01, 138359.05 examples/s]Map (num_proc=32):  65%|██████▍   | 362653/558128 [00:03<00:01, 137067.77 examples/s]Map (num_proc=32):  68%|██████▊   | 377252/558128 [00:03<00:01, 136828.88 examples/s]Map (num_proc=32):  73%|███████▎  | 406315/558128 [00:03<00:00, 171502.71 examples/s]Map (num_proc=32):  76%|███████▌  | 424402/558128 [00:03<00:00, 173086.51 examples/s]Map (num_proc=32):  79%|███████▉  | 442275/558128 [00:03<00:00, 136492.66 examples/s]Map (num_proc=32):  86%|████████▋ | 482600/558128 [00:03<00:00, 198033.27 examples/s]Map (num_proc=32):  91%|█████████ | 505612/558128 [00:04<00:00, 203640.73 examples/s]Map (num_proc=32):  95%|█████████▍| 528312/558128 [00:04<00:00, 123132.65 examples/s]Map (num_proc=32):  98%|█████████▊| 545864/558128 [00:04<00:00, 101187.57 examples/s]Map (num_proc=32): 100%|██████████| 558128/558128 [00:05<00:00, 103277.01 examples/s]
Filter (num_proc=32):   0%|          | 0/558128 [00:00<?, ? examples/s]Filter (num_proc=32):   1%|          | 3000/558128 [00:00<00:43, 12802.89 examples/s]Filter (num_proc=32):   2%|▏         | 12000/558128 [00:00<00:14, 38241.92 examples/s]Filter (num_proc=32):   5%|▍         | 27000/558128 [00:00<00:07, 72830.53 examples/s]Filter (num_proc=32):   9%|▉         | 50442/558128 [00:00<00:04, 122562.81 examples/s]Filter (num_proc=32):  13%|█▎        | 70884/558128 [00:00<00:03, 142019.66 examples/s]Filter (num_proc=32):  16%|█▋        | 91326/558128 [00:00<00:03, 154684.00 examples/s]Filter (num_proc=32):  20%|█▉        | 111210/558128 [00:00<00:02, 164101.64 examples/s]Filter (num_proc=32):  25%|██▍       | 137652/558128 [00:01<00:02, 187185.37 examples/s]Filter (num_proc=32):  28%|██▊       | 158094/558128 [00:01<00:02, 189717.93 examples/s]Filter (num_proc=32):  32%|███▏      | 178536/558128 [00:01<00:02, 187941.27 examples/s]Filter (num_proc=32):  36%|███▌      | 198420/558128 [00:01<00:01, 185679.53 examples/s]Filter (num_proc=32):  39%|███▉      | 218862/558128 [00:01<00:02, 167290.81 examples/s]Filter (num_proc=32):  43%|████▎     | 241304/558128 [00:01<00:01, 179704.23 examples/s]Filter (num_proc=32):  47%|████▋     | 261746/558128 [00:01<00:02, 138094.36 examples/s]Filter (num_proc=32):  53%|█████▎    | 294072/558128 [00:01<00:01, 169134.97 examples/s]Filter (num_proc=32):  56%|█████▋    | 314513/558128 [00:02<00:01, 157352.13 examples/s]Filter (num_proc=32):  60%|██████    | 335513/558128 [00:02<00:01, 159467.43 examples/s]Filter (num_proc=32):  64%|██████▎   | 355395/558128 [00:02<00:01, 164677.46 examples/s]Filter (num_proc=32):  68%|██████▊   | 381836/558128 [00:02<00:00, 184248.68 examples/s]Filter (num_proc=32):  72%|███████▏  | 402277/558128 [00:02<00:00, 185462.07 examples/s]Filter (num_proc=32):  76%|███████▌  | 421718/558128 [00:02<00:00, 167180.22 examples/s]Filter (num_proc=32):  80%|████████  | 447600/558128 [00:02<00:00, 184552.40 examples/s]Filter (num_proc=32):  84%|████████▍ | 467600/558128 [00:02<00:00, 184091.44 examples/s]Filter (num_proc=32):  88%|████████▊ | 491923/558128 [00:03<00:00, 190237.87 examples/s]Filter (num_proc=32):  92%|█████████▏| 515364/558128 [00:03<00:00, 200730.82 examples/s]Filter (num_proc=32):  97%|█████████▋| 538805/558128 [00:03<00:00, 195409.23 examples/s]Filter (num_proc=32): 100%|██████████| 558128/558128 [00:03<00:00, 155778.25 examples/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Map (num_proc=32):   0%|          | 0/558128 [00:00<?, ? examples/s]Map (num_proc=32):   0%|          | 158/558128 [00:04<3:57:07, 39.22 examples/s]Map (num_proc=32):   0%|          | 640/558128 [00:04<45:58, 202.09 examples/s] Map (num_proc=32):   0%|          | 981/558128 [00:04<26:26, 351.09 examples/s]Map (num_proc=32):   0%|          | 1416/558128 [00:04<15:57, 581.70 examples/s]Map (num_proc=32):   0%|          | 1758/558128 [00:04<11:29, 806.87 examples/s]Map (num_proc=32):   0%|          | 2194/558128 [00:04<08:20, 1111.33 examples/s]Map (num_proc=32):   0%|          | 2537/558128 [00:04<06:38, 1392.97 examples/s]Map (num_proc=32):   1%|          | 3000/558128 [00:04<05:27, 1693.80 examples/s]Map (num_proc=32):   1%|          | 3484/558128 [00:05<04:30, 2048.42 examples/s]Map (num_proc=32):   1%|          | 3996/558128 [00:05<03:49, 2414.51 examples/s]Map (num_proc=32):   1%|          | 4438/558128 [00:05<03:37, 2550.47 examples/s]Map (num_proc=32):   1%|          | 4783/558128 [00:05<03:22, 2728.20 examples/s]Map (num_proc=32):   1%|          | 5216/558128 [00:05<03:19, 2772.49 examples/s]Map (num_proc=32):   1%|          | 5561/558128 [00:05<03:09, 2921.10 examples/s]Map (num_proc=32):   1%|          | 5905/558128 [00:05<03:09, 2918.31 examples/s]Map (num_proc=32):   1%|          | 6307/558128 [00:05<03:03, 3005.91 examples/s]Map (num_proc=32):   1%|          | 6648/558128 [00:06<02:57, 3104.73 examples/s]Map (num_proc=32):   1%|▏         | 6987/558128 [00:06<02:58, 3095.40 examples/s]Map (num_proc=32):   1%|▏         | 7417/558128 [00:06<03:02, 3012.16 examples/s]Map (num_proc=32):   1%|▏         | 7753/558128 [00:06<02:57, 3096.40 examples/s]Map (num_proc=32):   1%|▏         | 8179/558128 [00:06<03:55, 2331.58 examples/s]Map (num_proc=32):   2%|▏         | 8990/558128 [00:06<02:44, 3334.69 examples/s]Map (num_proc=32):   2%|▏         | 9413/558128 [00:06<02:39, 3449.71 examples/s]Map (num_proc=32):   2%|▏         | 10095/558128 [00:07<02:09, 4231.08 examples/s]Map (num_proc=32):   2%|▏         | 10840/558128 [00:07<01:59, 4574.08 examples/s]Map (num_proc=32):   2%|▏         | 11435/558128 [00:07<01:51, 4908.58 examples/s]Map (num_proc=32):   2%|▏         | 12124/558128 [00:07<01:40, 5417.98 examples/s]Map (num_proc=32):   2%|▏         | 12812/558128 [00:07<01:35, 5725.40 examples/s]Map (num_proc=32):   2%|▏         | 13552/558128 [00:07<01:29, 6092.99 examples/s]Map (num_proc=32):   3%|▎         | 14212/558128 [00:07<01:28, 6141.08 examples/s]Map (num_proc=32):   3%|▎         | 14901/558128 [00:07<01:25, 6346.10 examples/s]Map (num_proc=32):   3%|▎         | 15570/558128 [00:07<01:32, 5853.29 examples/s]Map (num_proc=32):   3%|▎         | 16264/558128 [00:08<01:29, 6057.90 examples/s]Map (num_proc=32):   3%|▎         | 17038/558128 [00:08<01:26, 6246.85 examples/s]Map (num_proc=32):   3%|▎         | 17728/558128 [00:08<01:24, 6422.62 examples/s]Map (num_proc=32):   3%|▎         | 18585/558128 [00:08<01:24, 6371.75 examples/s]Map (num_proc=32):   3%|▎         | 19318/558128 [00:08<01:26, 6193.72 examples/s]Map (num_proc=32):   4%|▎         | 20182/558128 [00:08<01:22, 6505.84 examples/s]Map (num_proc=32):   4%|▎         | 20864/558128 [00:08<01:25, 6295.95 examples/s]Map (num_proc=32):   4%|▍         | 21516/558128 [00:08<01:33, 5766.10 examples/s]Map (num_proc=32):   4%|▍         | 22545/558128 [00:08<01:18, 6802.96 examples/s]Map (num_proc=32):   4%|▍         | 23334/558128 [00:09<01:22, 6446.19 examples/s]Map (num_proc=32):   4%|▍         | 24026/558128 [00:09<01:21, 6561.13 examples/s]Map (num_proc=32):   4%|▍         | 24709/558128 [00:09<01:20, 6624.39 examples/s]Map (num_proc=32):   5%|▍         | 25418/558128 [00:09<01:24, 6298.25 examples/s]Map (num_proc=32):   5%|▍         | 26090/558128 [00:09<01:34, 5617.16 examples/s]Map (num_proc=32):   5%|▍         | 26674/558128 [00:10<03:20, 2653.98 examples/s]Map (num_proc=32):   5%|▌         | 28185/558128 [00:10<02:07, 4145.64 examples/s]Map (num_proc=32):   5%|▌         | 28951/558128 [00:10<02:00, 4401.13 examples/s]Map (num_proc=32):   5%|▌         | 29609/558128 [00:10<01:51, 4742.91 examples/s]Map (num_proc=32):   5%|▌         | 30276/558128 [00:10<01:43, 5110.82 examples/s]Map (num_proc=32):   6%|▌         | 30958/558128 [00:10<01:39, 5290.09 examples/s]Map (num_proc=32):   6%|▌         | 31617/558128 [00:10<01:34, 5592.56 examples/s]Map (num_proc=32):   6%|▌         | 32290/558128 [00:10<01:31, 5750.86 examples/s]Map (num_proc=32):   6%|▌         | 32946/558128 [00:11<01:28, 5941.77 examples/s]Map (num_proc=32):   6%|▌         | 33614/558128 [00:11<01:27, 5990.43 examples/s]Map (num_proc=32):   6%|▌         | 34276/558128 [00:11<01:27, 5989.34 examples/s]Map (num_proc=32):   6%|▋         | 34956/558128 [00:11<01:27, 5990.76 examples/s]Map (num_proc=32):   6%|▋         | 35809/558128 [00:11<01:21, 6422.46 examples/s]Map (num_proc=32):   7%|▋         | 36461/558128 [00:11<01:21, 6403.63 examples/s]Map (num_proc=32):   7%|▋         | 37123/558128 [00:11<01:21, 6363.84 examples/s]Map (num_proc=32):   7%|▋         | 37802/558128 [00:12<02:07, 4086.01 examples/s]Map (num_proc=32):   7%|▋         | 39115/558128 [00:12<01:30, 5761.61 examples/s]Map (num_proc=32):   7%|▋         | 40351/558128 [00:12<01:12, 7185.73 examples/s]Map (num_proc=32):   7%|▋         | 41318/558128 [00:12<01:20, 6418.03 examples/s]Map (num_proc=32):   8%|▊         | 42134/558128 [00:12<01:58, 4360.53 examples/s]Map (num_proc=32):   8%|▊         | 42884/558128 [00:12<01:56, 4425.94 examples/s]Map (num_proc=32):   8%|▊         | 43545/558128 [00:13<02:03, 4172.00 examples/s]Map (num_proc=32):   8%|▊         | 44105/558128 [00:13<02:19, 3692.42 examples/s]Map (num_proc=32):   8%|▊         | 44601/558128 [00:13<02:22, 3595.79 examples/s]Map (num_proc=32):   8%|▊         | 45047/558128 [00:13<02:19, 3685.32 examples/s]Map (num_proc=32):   8%|▊         | 45725/558128 [00:13<01:58, 4327.20 examples/s]Map (num_proc=32):   8%|▊         | 46399/558128 [00:13<01:44, 4879.85 examples/s]Map (num_proc=32):   8%|▊         | 47057/558128 [00:13<01:41, 5044.40 examples/s]Map (num_proc=32):   9%|▊         | 47717/558128 [00:14<01:33, 5431.96 examples/s]Map (num_proc=32):   9%|▊         | 48389/558128 [00:14<01:28, 5770.51 examples/s]Map (num_proc=32):   9%|▉         | 49056/558128 [00:14<01:31, 5569.38 examples/s]Map (num_proc=32):   9%|▉         | 49737/558128 [00:14<01:26, 5896.04 examples/s]Map (num_proc=32):   9%|▉         | 50562/558128 [00:14<01:25, 5932.71 examples/s]Map (num_proc=32):   9%|▉         | 51220/558128 [00:14<01:23, 6101.40 examples/s]Map (num_proc=32):   9%|▉         | 51991/558128 [00:14<01:21, 6225.12 examples/s]Map (num_proc=32):   9%|▉         | 52643/558128 [00:14<01:21, 6214.37 examples/s]Map (num_proc=32):  10%|▉         | 53380/558128 [00:15<02:04, 4065.36 examples/s]Map (num_proc=32):  10%|▉         | 55005/558128 [00:15<01:18, 6428.97 examples/s]Map (num_proc=32):  10%|█         | 55980/558128 [00:15<01:13, 6820.59 examples/s]Map (num_proc=32):  10%|█         | 56911/558128 [00:15<01:18, 6383.92 examples/s]Map (num_proc=32):  10%|█         | 57915/558128 [00:15<01:12, 6858.71 examples/s]Map (num_proc=32):  11%|█         | 58733/558128 [00:15<01:12, 6905.25 examples/s]Map (num_proc=32):  11%|█         | 59558/558128 [00:15<01:19, 6305.62 examples/s]Map (num_proc=32):  11%|█         | 60308/558128 [00:16<01:23, 5956.52 examples/s]Map (num_proc=32):  11%|█         | 61014/558128 [00:16<03:21, 2461.37 examples/s]Map (num_proc=32):  11%|█         | 62485/558128 [00:16<02:08, 3860.36 examples/s]Map (num_proc=32):  11%|█▏        | 63772/558128 [00:17<01:36, 5112.44 examples/s]Map (num_proc=32):  12%|█▏        | 64840/558128 [00:17<01:24, 5867.93 examples/s]Map (num_proc=32):  12%|█▏        | 65870/558128 [00:17<01:19, 6229.11 examples/s]Map (num_proc=32):  12%|█▏        | 66835/558128 [00:17<01:21, 6056.86 examples/s]Map (num_proc=32):  12%|█▏        | 67697/558128 [00:17<01:18, 6270.55 examples/s]Map (num_proc=32):  12%|█▏        | 68496/558128 [00:17<01:19, 6141.36 examples/s]Map (num_proc=32):  12%|█▏        | 69358/558128 [00:17<01:19, 6172.46 examples/s]Map (num_proc=32):  13%|█▎        | 70179/558128 [00:18<01:16, 6353.98 examples/s]Map (num_proc=32):  13%|█▎        | 71007/558128 [00:18<02:03, 3949.73 examples/s]Map (num_proc=32):  13%|█▎        | 72496/558128 [00:18<01:25, 5708.38 examples/s]Map (num_proc=32):  13%|█▎        | 73979/558128 [00:18<01:06, 7277.45 examples/s]Map (num_proc=32):  13%|█▎        | 75102/558128 [00:18<01:13, 6586.45 examples/s]Map (num_proc=32):  14%|█▎        | 76267/558128 [00:18<01:08, 7071.73 examples/s]Map (num_proc=32):  14%|█▍        | 77212/558128 [00:19<01:10, 6847.53 examples/s]Map (num_proc=32):  14%|█▍        | 78103/558128 [00:19<01:54, 4189.07 examples/s]Map (num_proc=32):  14%|█▍        | 79103/558128 [00:19<01:40, 4754.15 examples/s]Map (num_proc=32):  14%|█▍        | 79745/558128 [00:19<01:51, 4278.76 examples/s]Map (num_proc=32):  14%|█▍        | 80332/558128 [00:20<01:58, 4042.80 examples/s]Map (num_proc=32):  15%|█▍        | 81186/558128 [00:20<01:43, 4618.42 examples/s]Map (num_proc=32):  15%|█▍        | 81803/558128 [00:20<01:36, 4915.69 examples/s]Map (num_proc=32):  15%|█▍        | 82644/558128 [00:20<01:30, 5226.08 examples/s]Map (num_proc=32):  15%|█▍        | 83304/558128 [00:20<01:25, 5526.72 examples/s]Map (num_proc=32):  15%|█▌        | 83937/558128 [00:20<01:25, 5553.54 examples/s]Map (num_proc=32):  15%|█▌        | 84628/558128 [00:20<01:20, 5885.70 examples/s]Map (num_proc=32):  15%|█▌        | 85315/558128 [00:20<01:18, 6060.28 examples/s]Map (num_proc=32):  15%|█▌        | 86110/558128 [00:21<01:16, 6196.54 examples/s]Map (num_proc=32):  16%|█▌        | 86797/558128 [00:21<01:13, 6374.31 examples/s]Map (num_proc=32):  16%|█▌        | 87457/558128 [00:21<01:14, 6281.44 examples/s]Map (num_proc=32):  16%|█▌        | 88111/558128 [00:21<01:14, 6346.08 examples/s]Map (num_proc=32):  16%|█▌        | 88797/558128 [00:21<01:14, 6268.91 examples/s]Map (num_proc=32):  16%|█▌        | 89627/558128 [00:21<02:14, 3482.93 examples/s]Map (num_proc=32):  16%|█▋        | 91235/558128 [00:21<01:24, 5542.32 examples/s]Map (num_proc=32):  17%|█▋        | 92683/558128 [00:22<01:04, 7222.76 examples/s]Map (num_proc=32):  17%|█▋        | 93683/558128 [00:22<01:05, 7060.82 examples/s]Map (num_proc=32):  17%|█▋        | 94729/558128 [00:22<01:08, 6802.62 examples/s]Map (num_proc=32):  17%|█▋        | 95600/558128 [00:22<01:33, 4941.84 examples/s]Map (num_proc=32):  17%|█▋        | 96376/558128 [00:22<01:47, 4309.05 examples/s]Map (num_proc=32):  17%|█▋        | 97045/558128 [00:23<01:53, 4062.86 examples/s]Map (num_proc=32):  17%|█▋        | 97548/558128 [00:23<02:02, 3765.55 examples/s]Map (num_proc=32):  18%|█▊        | 98389/558128 [00:23<01:42, 4468.59 examples/s]Map (num_proc=32):  18%|█▊        | 99049/558128 [00:23<01:34, 4868.41 examples/s]Map (num_proc=32):  18%|█▊        | 99651/558128 [00:23<01:33, 4926.79 examples/s]Map (num_proc=32):  18%|█▊        | 100319/558128 [00:23<01:25, 5330.42 examples/s]Map (num_proc=32):  18%|█▊        | 101002/558128 [00:23<01:20, 5697.34 examples/s]Map (num_proc=32):  18%|█▊        | 101762/558128 [00:23<01:18, 5780.14 examples/s]Map (num_proc=32):  18%|█▊        | 102449/558128 [00:24<01:15, 6059.40 examples/s]Map (num_proc=32):  18%|█▊        | 103233/558128 [00:24<01:17, 5845.96 examples/s]Map (num_proc=32):  19%|█▊        | 104070/558128 [00:24<01:11, 6339.68 examples/s]Map (num_proc=32):  19%|█▉        | 104930/558128 [00:24<01:13, 6196.79 examples/s]Map (num_proc=32):  19%|█▉        | 105719/558128 [00:24<01:11, 6297.91 examples/s]Map (num_proc=32):  19%|█▉        | 106562/558128 [00:24<01:12, 6205.83 examples/s]Map (num_proc=32):  19%|█▉        | 107206/558128 [00:25<01:51, 4046.62 examples/s]Map (num_proc=32):  20%|█▉        | 108849/558128 [00:25<01:11, 6309.67 examples/s]Map (num_proc=32):  20%|█▉        | 109946/558128 [00:25<01:02, 7145.32 examples/s]Map (num_proc=32):  20%|█▉        | 110949/558128 [00:25<01:04, 6938.03 examples/s]Map (num_proc=32):  20%|██        | 111827/558128 [00:25<01:05, 6778.66 examples/s]Map (num_proc=32):  20%|██        | 112627/558128 [00:25<01:41, 4376.68 examples/s]Map (num_proc=32):  20%|██        | 113355/558128 [00:26<01:33, 4753.18 examples/s]Map (num_proc=32):  20%|██        | 113984/558128 [00:26<01:46, 4153.33 examples/s]Map (num_proc=32):  21%|██        | 114652/558128 [00:26<01:57, 3771.38 examples/s]Map (num_proc=32):  21%|██        | 115326/558128 [00:26<01:43, 4292.26 examples/s]Map (num_proc=32):  21%|██        | 116017/558128 [00:26<01:31, 4815.14 examples/s]Map (num_proc=32):  21%|██        | 116652/558128 [00:26<01:29, 4941.39 examples/s]Map (num_proc=32):  21%|██        | 117340/558128 [00:26<01:21, 5392.63 examples/s]Map (num_proc=32):  21%|██        | 118009/558128 [00:27<01:17, 5712.44 examples/s]Map (num_proc=32):  21%|██▏       | 118652/558128 [00:27<01:17, 5671.95 examples/s]Map (num_proc=32):  21%|██▏       | 119342/558128 [00:27<01:13, 5990.21 examples/s]Map (num_proc=32):  22%|██▏       | 120208/558128 [00:27<01:07, 6482.79 examples/s]Map (num_proc=32):  22%|██▏       | 120992/558128 [00:27<01:11, 6122.99 examples/s]Map (num_proc=32):  22%|██▏       | 121685/558128 [00:27<01:09, 6243.57 examples/s]Map (num_proc=32):  22%|██▏       | 122508/558128 [00:27<01:08, 6393.07 examples/s]Map (num_proc=32):  22%|██▏       | 123167/558128 [00:27<01:07, 6435.73 examples/s]Map (num_proc=32):  22%|██▏       | 123859/558128 [00:27<01:07, 6479.03 examples/s]Map (num_proc=32):  22%|██▏       | 124652/558128 [00:28<01:10, 6146.56 examples/s]Map (num_proc=32):  22%|██▏       | 125330/558128 [00:28<01:50, 3904.78 examples/s]Map (num_proc=32):  23%|██▎       | 126652/558128 [00:28<01:16, 5621.36 examples/s]Map (num_proc=32):  23%|██▎       | 127508/558128 [00:28<01:20, 5382.20 examples/s]Map (num_proc=32):  23%|██▎       | 129329/558128 [00:28<00:54, 7849.06 examples/s]Map (num_proc=32):  23%|██▎       | 130373/558128 [00:29<01:27, 4901.41 examples/s]Map (num_proc=32):  23%|██▎       | 131125/558128 [00:29<01:22, 5166.65 examples/s]Map (num_proc=32):  24%|██▎       | 131970/558128 [00:29<01:33, 4565.01 examples/s]Map (num_proc=32):  24%|██▍       | 132598/558128 [00:29<01:43, 4093.90 examples/s]Map (num_proc=32):  24%|██▍       | 133176/558128 [00:29<01:54, 3707.02 examples/s]Map (num_proc=32):  24%|██▍       | 133850/558128 [00:30<01:42, 4153.81 examples/s]Map (num_proc=32):  24%|██▍       | 134531/558128 [00:30<01:31, 4649.01 examples/s]Map (num_proc=32):  24%|██▍       | 135117/558128 [00:30<01:29, 4730.45 examples/s]Map (num_proc=32):  24%|██▍       | 135810/558128 [00:30<01:20, 5229.69 examples/s]Map (num_proc=32):  24%|██▍       | 136673/558128 [00:30<01:13, 5747.61 examples/s]Map (num_proc=32):  25%|██▍       | 137407/558128 [00:30<01:14, 5630.73 examples/s]Map (num_proc=32):  25%|██▍       | 138267/558128 [00:30<01:08, 6100.33 examples/s]Map (num_proc=32):  25%|██▍       | 138929/558128 [00:30<01:09, 6055.65 examples/s]Map (num_proc=32):  25%|██▌       | 139601/558128 [00:31<01:07, 6217.07 examples/s]Map (num_proc=32):  25%|██▌       | 140292/558128 [00:31<01:05, 6401.49 examples/s]Map (num_proc=32):  25%|██▌       | 140960/558128 [00:31<01:06, 6256.19 examples/s]Map (num_proc=32):  25%|██▌       | 141606/558128 [00:31<01:06, 6306.63 examples/s]Map (num_proc=32):  25%|██▌       | 142295/558128 [00:31<01:04, 6466.35 examples/s]Map (num_proc=32):  26%|██▌       | 142959/558128 [00:31<01:05, 6292.34 examples/s]Map (num_proc=32):  26%|██▌       | 143779/558128 [00:31<01:06, 6203.28 examples/s]Map (num_proc=32):  26%|██▌       | 144468/558128 [00:31<01:43, 3988.87 examples/s]Map (num_proc=32):  26%|██▌       | 146392/558128 [00:32<01:02, 6569.59 examples/s]Map (num_proc=32):  26%|██▋       | 147336/558128 [00:32<01:24, 4880.08 examples/s]Map (num_proc=32):  27%|██▋       | 148105/558128 [00:32<01:37, 4208.98 examples/s]Map (num_proc=32):  27%|██▋       | 148700/558128 [00:32<01:44, 3918.75 examples/s]Map (num_proc=32):  27%|██▋       | 149177/558128 [00:33<01:49, 3746.63 examples/s]Map (num_proc=32):  27%|██▋       | 149666/558128 [00:33<01:46, 3852.46 examples/s]Map (num_proc=32):  27%|██▋       | 150445/558128 [00:33<01:34, 4326.49 examples/s]Map (num_proc=32):  27%|██▋       | 151135/558128 [00:33<01:23, 4860.53 examples/s]Map (num_proc=32):  27%|██▋       | 151915/558128 [00:33<01:18, 5154.02 examples/s]Map (num_proc=32):  27%|██▋       | 152575/558128 [00:33<01:13, 5489.48 examples/s]Map (num_proc=32):  27%|██▋       | 153257/558128 [00:33<01:12, 5579.31 examples/s]Map (num_proc=32):  28%|██▊       | 153881/558128 [00:33<01:10, 5740.17 examples/s]Map (num_proc=32):  28%|██▊       | 154558/558128 [00:33<01:07, 6012.16 examples/s]Map (num_proc=32):  28%|██▊       | 155244/558128 [00:34<01:04, 6201.93 examples/s]Map (num_proc=32):  28%|██▊       | 156047/558128 [00:34<01:03, 6333.43 examples/s]Map (num_proc=32):  28%|██▊       | 156911/558128 [00:34<01:03, 6273.76 examples/s]Map (num_proc=32):  28%|██▊       | 157564/558128 [00:34<01:04, 6225.97 examples/s]Map (num_proc=32):  28%|██▊       | 158222/558128 [00:34<01:04, 6232.02 examples/s]Map (num_proc=32):  28%|██▊       | 158909/558128 [00:34<01:02, 6403.17 examples/s]Map (num_proc=32):  29%|██▊       | 159638/558128 [00:34<01:03, 6271.63 examples/s]Map (num_proc=32):  29%|██▊       | 160326/558128 [00:34<01:07, 5905.03 examples/s]Map (num_proc=32):  29%|██▉       | 161356/558128 [00:34<00:57, 6923.02 examples/s]Map (num_proc=32):  29%|██▉       | 162107/558128 [00:35<01:00, 6561.38 examples/s]Map (num_proc=32):  29%|██▉       | 162937/558128 [00:35<01:43, 3828.41 examples/s]Map (num_proc=32):  30%|██▉       | 164663/558128 [00:35<01:04, 6068.44 examples/s]Map (num_proc=32):  30%|██▉       | 165576/558128 [00:36<02:01, 3219.94 examples/s]Map (num_proc=32):  30%|██▉       | 167352/558128 [00:36<01:19, 4891.22 examples/s]Map (num_proc=32):  30%|███       | 169005/558128 [00:36<00:59, 6501.08 examples/s]Map (num_proc=32):  31%|███       | 170247/558128 [00:36<01:00, 6365.05 examples/s]Map (num_proc=32):  31%|███       | 171274/558128 [00:36<00:59, 6527.26 examples/s]Map (num_proc=32):  31%|███       | 172177/558128 [00:37<01:00, 6337.33 examples/s]Map (num_proc=32):  31%|███       | 173042/558128 [00:37<01:00, 6417.77 examples/s]Map (num_proc=32):  31%|███       | 173838/558128 [00:37<00:59, 6458.76 examples/s]Map (num_proc=32):  31%|███▏      | 174663/558128 [00:37<01:01, 6231.55 examples/s]Map (num_proc=32):  31%|███▏      | 175350/558128 [00:37<01:00, 6370.57 examples/s]Map (num_proc=32):  32%|███▏      | 176146/558128 [00:37<01:19, 4775.46 examples/s]Map (num_proc=32):  32%|███▏      | 177681/558128 [00:37<00:55, 6806.27 examples/s]Map (num_proc=32):  32%|███▏      | 178637/558128 [00:38<00:56, 6686.08 examples/s]Map (num_proc=32):  32%|███▏      | 179494/558128 [00:38<01:00, 6244.77 examples/s]Map (num_proc=32):  32%|███▏      | 180488/558128 [00:38<00:54, 6926.55 examples/s]Map (num_proc=32):  32%|███▏      | 181339/558128 [00:38<00:58, 6441.83 examples/s]Map (num_proc=32):  33%|███▎      | 182146/558128 [00:38<00:56, 6614.70 examples/s]Map (num_proc=32):  33%|███▎      | 182935/558128 [00:38<00:58, 6383.65 examples/s]Map (num_proc=32):  33%|███▎      | 183723/558128 [00:39<02:53, 2157.13 examples/s]Map (num_proc=32):  33%|███▎      | 185306/558128 [00:39<01:48, 3444.03 examples/s]Map (num_proc=32):  33%|███▎      | 186761/558128 [00:39<01:17, 4777.67 examples/s]Map (num_proc=32):  34%|███▎      | 188135/558128 [00:40<01:01, 6064.16 examples/s]Map (num_proc=32):  34%|███▍      | 189250/558128 [00:40<01:01, 5995.55 examples/s]Map (num_proc=32):  34%|███▍      | 190277/558128 [00:40<01:01, 5946.71 examples/s]Map (num_proc=32):  34%|███▍      | 191242/558128 [00:40<01:00, 6098.45 examples/s]Map (num_proc=32):  34%|███▍      | 192101/558128 [00:40<00:56, 6511.04 examples/s]Map (num_proc=32):  35%|███▍      | 193037/558128 [00:40<01:13, 4979.25 examples/s]Map (num_proc=32):  35%|███▍      | 194648/558128 [00:41<00:54, 6658.10 examples/s]Map (num_proc=32):  35%|███▌      | 195512/558128 [00:41<00:54, 6713.51 examples/s]Map (num_proc=32):  35%|███▌      | 196335/558128 [00:41<00:56, 6421.26 examples/s]Map (num_proc=32):  35%|███▌      | 197294/558128 [00:41<00:54, 6670.11 examples/s]Map (num_proc=32):  35%|███▌      | 198105/558128 [00:41<00:53, 6747.53 examples/s]Map (num_proc=32):  36%|███▌      | 198858/558128 [00:41<00:59, 6066.22 examples/s]Map (num_proc=32):  36%|███▌      | 199719/558128 [00:41<01:00, 5947.82 examples/s]Map (num_proc=32):  36%|███▌      | 200471/558128 [00:42<01:22, 4314.35 examples/s]Map (num_proc=32):  36%|███▌      | 201080/558128 [00:42<01:24, 4201.20 examples/s]Map (num_proc=32):  36%|███▌      | 201560/558128 [00:42<01:30, 3939.44 examples/s]Map (num_proc=32):  36%|███▌      | 202183/558128 [00:42<01:21, 4343.37 examples/s]Map (num_proc=32):  36%|███▋      | 202865/558128 [00:42<01:12, 4882.57 examples/s]Map (num_proc=32):  36%|███▋      | 203550/558128 [00:42<01:07, 5220.90 examples/s]Map (num_proc=32):  37%|███▋      | 204457/558128 [00:42<01:04, 5509.25 examples/s]Map (num_proc=32):  37%|███▋      | 205146/558128 [00:43<01:00, 5839.14 examples/s]Map (num_proc=32):  37%|███▋      | 205933/558128 [00:43<00:56, 6234.87 examples/s]Map (num_proc=32):  37%|███▋      | 206746/558128 [00:43<00:58, 6047.38 examples/s]Map (num_proc=32):  37%|███▋      | 207372/558128 [00:43<00:57, 6053.78 examples/s]Map (num_proc=32):  37%|███▋      | 208044/558128 [00:43<00:56, 6225.91 examples/s]Map (num_proc=32):  37%|███▋      | 208703/558128 [00:43<00:55, 6324.34 examples/s]Map (num_proc=32):  38%|███▊      | 209380/558128 [00:43<00:56, 6164.88 examples/s]Map (num_proc=32):  38%|███▊      | 210065/558128 [00:43<00:54, 6349.04 examples/s]Map (num_proc=32):  38%|███▊      | 210876/558128 [00:44<01:17, 4457.53 examples/s]Map (num_proc=32):  38%|███▊      | 212545/558128 [00:44<00:50, 6883.79 examples/s]Map (num_proc=32):  38%|███▊      | 213533/558128 [00:44<00:49, 6946.25 examples/s]Map (num_proc=32):  38%|███▊      | 214441/558128 [00:44<00:51, 6645.11 examples/s]Map (num_proc=32):  39%|███▊      | 215296/558128 [00:44<00:50, 6757.17 examples/s]Map (num_proc=32):  39%|███▊      | 216041/558128 [00:44<00:55, 6166.66 examples/s]Map (num_proc=32):  39%|███▉      | 216898/558128 [00:44<00:51, 6581.66 examples/s]Map (num_proc=32):  39%|███▉      | 217631/558128 [00:45<01:24, 4033.46 examples/s]Map (num_proc=32):  39%|███▉      | 218469/558128 [00:45<01:15, 4473.38 examples/s]Map (num_proc=32):  39%|███▉      | 219139/558128 [00:45<01:22, 4115.36 examples/s]Map (num_proc=32):  39%|███▉      | 219639/558128 [00:45<01:29, 3765.53 examples/s]Map (num_proc=32):  39%|███▉      | 220138/558128 [00:45<01:31, 3681.18 examples/s]Map (num_proc=32):  40%|███▉      | 220803/558128 [00:46<01:23, 4055.02 examples/s]Map (num_proc=32):  40%|███▉      | 221664/558128 [00:46<01:07, 4988.55 examples/s]Map (num_proc=32):  40%|███▉      | 222328/558128 [00:46<01:05, 5117.89 examples/s]Map (num_proc=32):  40%|███▉      | 222989/558128 [00:46<01:02, 5404.46 examples/s]Map (num_proc=32):  40%|████      | 223650/558128 [00:46<00:58, 5705.18 examples/s]Map (num_proc=32):  40%|████      | 224316/558128 [00:46<00:57, 5767.76 examples/s]Map (num_proc=32):  40%|████      | 224986/558128 [00:46<00:56, 5908.74 examples/s]Map (num_proc=32):  40%|████      | 225844/558128 [00:46<00:55, 5976.43 examples/s]Map (num_proc=32):  41%|████      | 226476/558128 [00:47<01:27, 3782.15 examples/s]Map (num_proc=32):  41%|████      | 227983/558128 [00:47<00:56, 5870.29 examples/s]Map (num_proc=32):  41%|████      | 229332/558128 [00:47<00:44, 7471.46 examples/s]Map (num_proc=32):  41%|████▏     | 230323/558128 [00:47<00:44, 7365.28 examples/s]Map (num_proc=32):  41%|████▏     | 231330/558128 [00:47<00:46, 6973.12 examples/s]Map (num_proc=32):  42%|████▏     | 232151/558128 [00:47<00:51, 6385.39 examples/s]Map (num_proc=32):  42%|████▏     | 232987/558128 [00:47<00:49, 6594.84 examples/s]Map (num_proc=32):  42%|████▏     | 233746/558128 [00:48<00:53, 6076.11 examples/s]Map (num_proc=32):  42%|████▏     | 234469/558128 [00:49<02:25, 2228.81 examples/s]Map (num_proc=32):  42%|████▏     | 236246/558128 [00:49<01:24, 3797.11 examples/s]Map (num_proc=32):  43%|████▎     | 237857/558128 [00:49<00:59, 5342.46 examples/s]Map (num_proc=32):  43%|████▎     | 239299/558128 [00:49<00:47, 6691.22 examples/s]Map (num_proc=32):  43%|████▎     | 240481/558128 [00:49<00:42, 7523.68 examples/s]Map (num_proc=32):  43%|████▎     | 241782/558128 [00:49<00:44, 7134.35 examples/s]Map (num_proc=32):  43%|████▎     | 242784/558128 [00:49<00:45, 6978.36 examples/s]Map (num_proc=32):  44%|████▎     | 243780/558128 [00:49<00:45, 6847.58 examples/s]Map (num_proc=32):  44%|████▍     | 244749/558128 [00:50<01:02, 4988.55 examples/s]Map (num_proc=32):  44%|████▍     | 246263/558128 [00:50<00:47, 6622.54 examples/s]Map (num_proc=32):  44%|████▍     | 247261/558128 [00:50<00:47, 6510.79 examples/s]Map (num_proc=32):  44%|████▍     | 248253/558128 [00:50<00:44, 6957.25 examples/s]Map (num_proc=32):  45%|████▍     | 249090/558128 [00:50<00:43, 7049.91 examples/s]Map (num_proc=32):  45%|████▍     | 249948/558128 [00:50<00:45, 6754.81 examples/s]Map (num_proc=32):  45%|████▍     | 250755/558128 [00:51<00:46, 6640.81 examples/s]Map (num_proc=32):  45%|████▌     | 251595/558128 [00:51<00:47, 6443.90 examples/s]Map (num_proc=32):  45%|████▌     | 252426/558128 [00:51<00:46, 6540.16 examples/s]Map (num_proc=32):  45%|████▌     | 253188/558128 [00:51<00:48, 6224.93 examples/s]Map (num_proc=32):  45%|████▌     | 253852/558128 [00:51<01:19, 3842.71 examples/s]Map (num_proc=32):  46%|████▌     | 254896/558128 [00:51<01:01, 4952.52 examples/s]Map (num_proc=32):  46%|████▌     | 255577/558128 [00:52<00:58, 5216.21 examples/s]Map (num_proc=32):  46%|████▌     | 256357/558128 [00:52<00:57, 5282.91 examples/s]Map (num_proc=32):  46%|████▌     | 257048/558128 [00:52<00:54, 5573.55 examples/s]Map (num_proc=32):  46%|████▌     | 257713/558128 [00:52<00:51, 5823.66 examples/s]Map (num_proc=32):  46%|████▋     | 258358/558128 [00:52<00:52, 5732.33 examples/s]Map (num_proc=32):  46%|████▋     | 259222/558128 [00:52<00:47, 6268.36 examples/s]Map (num_proc=32):  47%|████▋     | 259880/558128 [00:52<00:47, 6251.59 examples/s]Map (num_proc=32):  47%|████▋     | 260534/558128 [00:52<00:47, 6236.24 examples/s]Map (num_proc=32):  47%|████▋     | 261224/558128 [00:52<00:46, 6409.06 examples/s]Map (num_proc=32):  47%|████▋     | 261878/558128 [00:53<00:47, 6288.41 examples/s]Map (num_proc=32):  47%|████▋     | 262530/558128 [00:53<00:46, 6348.38 examples/s]Map (num_proc=32):  47%|████▋     | 263215/558128 [00:53<01:06, 4462.66 examples/s]Map (num_proc=32):  47%|████▋     | 264634/558128 [00:53<00:44, 6546.14 examples/s]Map (num_proc=32):  48%|████▊     | 265669/558128 [00:53<00:40, 7159.36 examples/s]Map (num_proc=32):  48%|████▊     | 266495/558128 [00:53<00:42, 6884.76 examples/s]Map (num_proc=32):  48%|████▊     | 267281/558128 [00:53<00:43, 6652.40 examples/s]Map (num_proc=32):  48%|████▊     | 268106/558128 [00:54<00:45, 6319.39 examples/s]Map (num_proc=32):  48%|████▊     | 268807/558128 [00:54<00:45, 6417.99 examples/s]Map (num_proc=32):  48%|████▊     | 269595/558128 [00:55<02:16, 2106.13 examples/s]Map (num_proc=32):  49%|████▊     | 271296/558128 [00:55<01:19, 3598.77 examples/s]Map (num_proc=32):  49%|████▉     | 273114/558128 [00:55<00:53, 5374.28 examples/s]Map (num_proc=32):  49%|████▉     | 274625/558128 [00:55<00:41, 6790.26 examples/s]Map (num_proc=32):  49%|████▉     | 275926/558128 [00:55<00:37, 7507.22 examples/s]Map (num_proc=32):  50%|████▉     | 277182/558128 [00:55<00:39, 7044.85 examples/s]Map (num_proc=32):  50%|████▉     | 278218/558128 [00:55<00:39, 7067.75 examples/s]Map (num_proc=32):  50%|█████     | 279284/558128 [00:56<00:43, 6476.07 examples/s]Map (num_proc=32):  50%|█████     | 280146/558128 [00:56<00:41, 6750.25 examples/s]Map (num_proc=32):  50%|█████     | 280970/558128 [00:56<00:55, 5031.22 examples/s]Map (num_proc=32):  51%|█████     | 282173/558128 [00:56<00:46, 5984.70 examples/s]Map (num_proc=32):  51%|█████     | 283660/558128 [00:56<00:38, 7174.93 examples/s]Map (num_proc=32):  51%|█████     | 284630/558128 [00:56<00:41, 6653.56 examples/s]Map (num_proc=32):  51%|█████     | 285448/558128 [00:57<00:39, 6846.15 examples/s]Map (num_proc=32):  51%|█████▏    | 286294/558128 [00:57<00:40, 6683.24 examples/s]Map (num_proc=32):  51%|█████▏    | 287050/558128 [00:57<00:41, 6481.93 examples/s]Map (num_proc=32):  52%|█████▏    | 287830/558128 [00:57<00:42, 6318.56 examples/s]Map (num_proc=32):  52%|█████▏    | 288547/558128 [00:57<01:02, 4302.12 examples/s]Map (num_proc=32):  52%|█████▏    | 289384/558128 [00:57<00:53, 5034.58 examples/s]Map (num_proc=32):  52%|█████▏    | 290143/558128 [00:58<00:51, 5206.68 examples/s]Map (num_proc=32):  52%|█████▏    | 290813/558128 [00:58<00:48, 5526.84 examples/s]Map (num_proc=32):  52%|█████▏    | 291576/558128 [00:58<00:48, 5525.61 examples/s]Map (num_proc=32):  52%|█████▏    | 292267/558128 [00:58<00:45, 5846.97 examples/s]Map (num_proc=32):  52%|█████▏    | 292938/558128 [00:58<00:45, 5879.27 examples/s]Map (num_proc=32):  53%|█████▎    | 293590/558128 [00:58<00:44, 5968.52 examples/s]Map (num_proc=32):  53%|█████▎    | 294447/558128 [00:58<00:40, 6481.78 examples/s]Map (num_proc=32):  53%|█████▎    | 295276/558128 [00:58<00:42, 6119.83 examples/s]Map (num_proc=32):  53%|█████▎    | 295932/558128 [00:58<00:42, 6229.39 examples/s]Map (num_proc=32):  53%|█████▎    | 296591/558128 [00:59<00:42, 6174.51 examples/s]Map (num_proc=32):  53%|█████▎    | 297276/558128 [00:59<00:41, 6352.56 examples/s]Map (num_proc=32):  53%|█████▎    | 298074/558128 [00:59<00:40, 6486.66 examples/s]Map (num_proc=32):  54%|█████▎    | 298753/558128 [00:59<00:40, 6386.27 examples/s]Map (num_proc=32):  54%|█████▎    | 299415/558128 [00:59<00:40, 6415.25 examples/s]Map (num_proc=32):  54%|█████▍    | 300277/558128 [00:59<00:39, 6520.97 examples/s]Map (num_proc=32):  54%|█████▍    | 300931/558128 [00:59<00:40, 6335.55 examples/s]Map (num_proc=32):  54%|█████▍    | 301587/558128 [00:59<00:40, 6306.80 examples/s]Map (num_proc=32):  54%|█████▍    | 302267/558128 [01:00<00:55, 4577.41 examples/s]Map (num_proc=32):  54%|█████▍    | 303749/558128 [01:00<00:37, 6759.99 examples/s]Map (num_proc=32):  55%|█████▍    | 304678/558128 [01:00<00:41, 6090.20 examples/s]Map (num_proc=32):  55%|█████▍    | 305465/558128 [01:01<01:46, 2365.74 examples/s]Map (num_proc=32):  55%|█████▍    | 306919/558128 [01:01<01:09, 3601.93 examples/s]Map (num_proc=32):  55%|█████▌    | 308349/558128 [01:01<00:50, 4944.39 examples/s]Map (num_proc=32):  56%|█████▌    | 310200/558128 [01:01<00:35, 6934.00 examples/s]Map (num_proc=32):  56%|█████▌    | 311854/558128 [01:01<00:28, 8513.89 examples/s]Map (num_proc=32):  56%|█████▌    | 313241/558128 [01:01<00:29, 8250.58 examples/s]Map (num_proc=32):  56%|█████▋    | 314518/558128 [01:02<00:32, 7518.68 examples/s]Map (num_proc=32):  57%|█████▋    | 315513/558128 [01:02<00:40, 5954.63 examples/s]Map (num_proc=32):  57%|█████▋    | 317052/558128 [01:02<00:33, 7248.97 examples/s]Map (num_proc=32):  57%|█████▋    | 318027/558128 [01:02<00:34, 6993.89 examples/s]Map (num_proc=32):  57%|█████▋    | 318885/558128 [01:02<00:33, 7243.73 examples/s]Map (num_proc=32):  57%|█████▋    | 319852/558128 [01:02<00:34, 6850.66 examples/s]Map (num_proc=32):  57%|█████▋    | 320712/558128 [01:03<00:36, 6527.41 examples/s]Map (num_proc=32):  58%|█████▊    | 321513/558128 [01:03<00:38, 6198.06 examples/s]Map (num_proc=32):  58%|█████▊    | 322208/558128 [01:03<00:37, 6316.33 examples/s]Map (num_proc=32):  58%|█████▊    | 322883/558128 [01:03<00:56, 4194.04 examples/s]Map (num_proc=32):  58%|█████▊    | 323603/558128 [01:03<00:50, 4619.13 examples/s]Map (num_proc=32):  58%|█████▊    | 324191/558128 [01:03<00:57, 4086.55 examples/s]Map (num_proc=32):  58%|█████▊    | 324858/558128 [01:04<00:54, 4316.99 examples/s]Map (num_proc=32):  58%|█████▊    | 325545/558128 [01:04<00:48, 4841.13 examples/s]Map (num_proc=32):  58%|█████▊    | 326147/558128 [01:04<00:47, 4892.03 examples/s]Map (num_proc=32):  59%|█████▊    | 326839/558128 [01:04<00:43, 5373.39 examples/s]Map (num_proc=32):  59%|█████▊    | 327529/558128 [01:04<00:40, 5742.40 examples/s]Map (num_proc=32):  59%|█████▉    | 328159/558128 [01:04<00:39, 5796.11 examples/s]Map (num_proc=32):  59%|█████▉    | 328814/558128 [01:04<00:38, 5916.19 examples/s]Map (num_proc=32):  59%|█████▉    | 329504/558128 [01:04<00:37, 6056.46 examples/s]Map (num_proc=32):  59%|█████▉    | 330162/558128 [01:04<00:37, 6143.10 examples/s]Map (num_proc=32):  59%|█████▉    | 330811/558128 [01:05<00:36, 6236.79 examples/s]Map (num_proc=32):  59%|█████▉    | 331500/558128 [01:05<00:35, 6384.21 examples/s]Map (num_proc=32):  60%|█████▉    | 332153/558128 [01:05<00:57, 3921.13 examples/s]Map (num_proc=32):  60%|█████▉    | 333645/558128 [01:05<00:36, 6129.93 examples/s]Map (num_proc=32):  60%|█████▉    | 334812/558128 [01:05<00:30, 7349.84 examples/s]Map (num_proc=32):  60%|██████    | 335816/558128 [01:05<00:30, 7173.69 examples/s]Map (num_proc=32):  60%|██████    | 336813/558128 [01:05<00:32, 6887.62 examples/s]Map (num_proc=32):  60%|██████    | 337643/558128 [01:06<00:32, 6825.35 examples/s]Map (num_proc=32):  61%|██████    | 338395/558128 [01:06<00:33, 6553.63 examples/s]Map (num_proc=32):  61%|██████    | 339196/558128 [01:07<01:40, 2177.27 examples/s]Map (num_proc=32):  61%|██████    | 340559/558128 [01:07<01:06, 3286.61 examples/s]Map (num_proc=32):  61%|██████▏   | 342144/558128 [01:07<00:45, 4773.46 examples/s]Map (num_proc=32):  62%|██████▏   | 343611/558128 [01:07<00:34, 6201.59 examples/s]Map (num_proc=32):  62%|██████▏   | 344792/558128 [01:07<00:32, 6509.45 examples/s]Map (num_proc=32):  62%|██████▏   | 345909/558128 [01:07<00:32, 6448.73 examples/s]Map (num_proc=32):  62%|██████▏   | 346947/558128 [01:08<00:31, 6618.43 examples/s]Map (num_proc=32):  62%|██████▏   | 347909/558128 [01:08<00:32, 6529.27 examples/s]Map (num_proc=32):  62%|██████▏   | 348775/558128 [01:08<00:33, 6274.39 examples/s]Map (num_proc=32):  63%|██████▎   | 349565/558128 [01:08<00:42, 4938.34 examples/s]Map (num_proc=32):  63%|██████▎   | 351221/558128 [01:08<00:30, 6855.46 examples/s]Map (num_proc=32):  63%|██████▎   | 352179/558128 [01:08<00:30, 6808.72 examples/s]Map (num_proc=32):  63%|██████▎   | 353035/558128 [01:08<00:30, 6687.07 examples/s]Map (num_proc=32):  63%|██████▎   | 353796/558128 [01:09<00:32, 6358.30 examples/s]Map (num_proc=32):  64%|██████▎   | 354659/558128 [01:09<00:31, 6536.54 examples/s]Map (num_proc=32):  64%|██████▎   | 355482/558128 [01:09<00:31, 6364.18 examples/s]Map (num_proc=32):  64%|██████▍   | 356236/558128 [01:09<00:31, 6451.13 examples/s]Map (num_proc=32):  64%|██████▍   | 356995/558128 [01:10<01:35, 2111.92 examples/s]Map (num_proc=32):  64%|██████▍   | 358378/558128 [01:10<01:01, 3262.03 examples/s]Map (num_proc=32):  64%|██████▍   | 359824/558128 [01:10<00:42, 4628.36 examples/s]Map (num_proc=32):  65%|██████▍   | 361465/558128 [01:10<00:31, 6312.06 examples/s]Map (num_proc=32):  65%|██████▌   | 363081/558128 [01:10<00:24, 7914.88 examples/s]Map (num_proc=32):  65%|██████▌   | 364500/558128 [01:11<00:22, 8593.65 examples/s]Map (num_proc=32):  66%|██████▌   | 365691/558128 [01:11<00:23, 8146.96 examples/s]Map (num_proc=32):  66%|██████▌   | 366858/558128 [01:11<00:35, 5423.00 examples/s]Map (num_proc=32):  66%|██████▌   | 368692/558128 [01:11<00:25, 7361.44 examples/s]Map (num_proc=32):  66%|██████▋   | 369833/558128 [01:11<00:25, 7394.13 examples/s]Map (num_proc=32):  66%|██████▋   | 370961/558128 [01:12<00:25, 7365.68 examples/s]Map (num_proc=32):  67%|██████▋   | 371964/558128 [01:12<00:26, 6904.50 examples/s]Map (num_proc=32):  67%|██████▋   | 372931/558128 [01:12<00:27, 6769.31 examples/s]Map (num_proc=32):  67%|██████▋   | 373749/558128 [01:12<00:27, 6619.40 examples/s]Map (num_proc=32):  67%|██████▋   | 374508/558128 [01:12<00:28, 6349.73 examples/s]Map (num_proc=32):  67%|██████▋   | 375277/558128 [01:12<00:32, 5648.29 examples/s]Map (num_proc=32):  67%|██████▋   | 376004/558128 [01:13<01:16, 2384.77 examples/s]Map (num_proc=32):  68%|██████▊   | 377616/558128 [01:13<00:47, 3821.36 examples/s]Map (num_proc=32):  68%|██████▊   | 379054/558128 [01:13<00:34, 5211.57 examples/s]Map (num_proc=32):  68%|██████▊   | 380685/558128 [01:13<00:25, 6936.15 examples/s]Map (num_proc=32):  68%|██████▊   | 382100/558128 [01:14<00:21, 8216.47 examples/s]Map (num_proc=32):  69%|██████▊   | 383386/558128 [01:14<00:21, 8312.86 examples/s]Map (num_proc=32):  69%|██████▉   | 384560/558128 [01:14<00:23, 7422.85 examples/s]Map (num_proc=32):  69%|██████▉   | 385508/558128 [01:14<00:33, 5200.83 examples/s]Map (num_proc=32):  69%|██████▉   | 387304/558128 [01:14<00:23, 7127.16 examples/s]Map (num_proc=32):  70%|██████▉   | 388469/558128 [01:14<00:22, 7479.09 examples/s]Map (num_proc=32):  70%|██████▉   | 389476/558128 [01:15<00:23, 7086.58 examples/s]Map (num_proc=32):  70%|██████▉   | 390468/558128 [01:15<00:24, 6826.46 examples/s]Map (num_proc=32):  70%|███████   | 391290/558128 [01:15<00:24, 6763.25 examples/s]Map (num_proc=32):  70%|███████   | 392052/558128 [01:15<00:27, 6115.17 examples/s]Map (num_proc=32):  70%|███████   | 392755/558128 [01:15<00:36, 4487.82 examples/s]Map (num_proc=32):  70%|███████   | 393425/558128 [01:16<00:37, 4446.95 examples/s]Map (num_proc=32):  71%|███████   | 394223/558128 [01:16<00:32, 4975.91 examples/s]Map (num_proc=32):  71%|███████   | 394899/558128 [01:16<00:31, 5253.17 examples/s]Map (num_proc=32):  71%|███████   | 395568/558128 [01:16<00:29, 5542.43 examples/s]Map (num_proc=32):  71%|███████   | 396236/558128 [01:16<00:27, 5809.68 examples/s]Map (num_proc=32):  71%|███████   | 396912/558128 [01:16<00:27, 5899.59 examples/s]Map (num_proc=32):  71%|███████   | 397576/558128 [01:16<00:26, 5973.85 examples/s]Map (num_proc=32):  71%|███████▏  | 398230/558128 [01:16<00:26, 6115.48 examples/s]Map (num_proc=32):  71%|███████▏  | 398917/558128 [01:16<00:25, 6232.29 examples/s]Map (num_proc=32):  72%|███████▏  | 399575/558128 [01:16<00:25, 6210.69 examples/s]Map (num_proc=32):  72%|███████▏  | 400226/558128 [01:17<00:25, 6131.62 examples/s]Map (num_proc=32):  72%|███████▏  | 401083/558128 [01:17<00:26, 5961.70 examples/s]Map (num_proc=32):  72%|███████▏  | 401915/558128 [01:17<00:24, 6408.32 examples/s]Map (num_proc=32):  72%|███████▏  | 402743/558128 [01:17<00:24, 6235.47 examples/s]Map (num_proc=32):  72%|███████▏  | 403401/558128 [01:17<00:24, 6218.20 examples/s]Map (num_proc=32):  72%|███████▏  | 404088/558128 [01:17<00:35, 4341.41 examples/s]Map (num_proc=32):  73%|███████▎  | 406079/558128 [01:18<00:21, 6998.23 examples/s]Map (num_proc=32):  73%|███████▎  | 406912/558128 [01:18<00:21, 7187.18 examples/s]Map (num_proc=32):  73%|███████▎  | 407740/558128 [01:18<00:22, 6591.10 examples/s]Map (num_proc=32):  73%|███████▎  | 408565/558128 [01:18<00:22, 6601.73 examples/s]Map (num_proc=32):  73%|███████▎  | 409322/558128 [01:18<00:24, 6003.98 examples/s]Map (num_proc=32):  73%|███████▎  | 410105/558128 [01:19<00:57, 2571.88 examples/s]Map (num_proc=32):  74%|███████▎  | 411493/558128 [01:19<00:37, 3860.61 examples/s]Map (num_proc=32):  74%|███████▍  | 413002/558128 [01:19<00:26, 5419.48 examples/s]Map (num_proc=32):  74%|███████▍  | 414353/558128 [01:19<00:21, 6766.61 examples/s]Map (num_proc=32):  74%|███████▍  | 415506/558128 [01:19<00:21, 6607.59 examples/s]Map (num_proc=32):  75%|███████▍  | 416530/558128 [01:19<00:21, 6612.43 examples/s]Map (num_proc=32):  75%|███████▍  | 417503/558128 [01:20<00:21, 6413.44 examples/s]Map (num_proc=32):  75%|███████▍  | 418362/558128 [01:20<00:21, 6575.80 examples/s]Map (num_proc=32):  75%|███████▌  | 419182/558128 [01:20<00:21, 6511.68 examples/s]Map (num_proc=32):  75%|███████▌  | 419996/558128 [01:20<00:21, 6450.12 examples/s]Map (num_proc=32):  75%|███████▌  | 420841/558128 [01:20<00:21, 6407.20 examples/s]Map (num_proc=32):  76%|███████▌  | 421678/558128 [01:20<00:23, 5860.19 examples/s]Map (num_proc=32):  76%|███████▌  | 422672/558128 [01:20<00:20, 6605.11 examples/s]Map (num_proc=32):  76%|███████▌  | 423505/558128 [01:21<00:20, 6477.84 examples/s]Map (num_proc=32):  76%|███████▌  | 424186/558128 [01:21<00:20, 6395.56 examples/s]Map (num_proc=32):  76%|███████▌  | 425009/558128 [01:21<00:26, 5065.15 examples/s]Map (num_proc=32):  76%|███████▋  | 426341/558128 [01:21<00:19, 6655.11 examples/s]Map (num_proc=32):  77%|███████▋  | 427259/558128 [01:21<00:29, 4471.17 examples/s]Map (num_proc=32):  77%|███████▋  | 427993/558128 [01:22<00:27, 4781.68 examples/s]Map (num_proc=32):  77%|███████▋  | 428666/558128 [01:22<00:25, 5090.46 examples/s]Map (num_proc=32):  77%|███████▋  | 429443/558128 [01:22<00:24, 5211.98 examples/s]Map (num_proc=32):  77%|███████▋  | 430089/558128 [01:22<00:23, 5424.16 examples/s]Map (num_proc=32):  77%|███████▋  | 430778/558128 [01:22<00:22, 5763.83 examples/s]Map (num_proc=32):  77%|███████▋  | 431459/558128 [01:22<00:21, 5818.80 examples/s]Map (num_proc=32):  77%|███████▋  | 432115/558128 [01:22<00:21, 5956.28 examples/s]Map (num_proc=32):  78%|███████▊  | 432803/558128 [01:22<00:20, 6197.41 examples/s]Map (num_proc=32):  78%|███████▊  | 433453/558128 [01:22<00:20, 6056.07 examples/s]Map (num_proc=32):  78%|███████▊  | 434119/558128 [01:22<00:20, 6180.52 examples/s]Map (num_proc=32):  78%|███████▊  | 434970/558128 [01:23<00:18, 6619.30 examples/s]Map (num_proc=32):  78%|███████▊  | 435795/558128 [01:23<00:19, 6215.82 examples/s]Map (num_proc=32):  78%|███████▊  | 436455/558128 [01:23<00:19, 6310.53 examples/s]Map (num_proc=32):  78%|███████▊  | 437131/558128 [01:23<00:27, 4331.64 examples/s]Map (num_proc=32):  79%|███████▊  | 438777/558128 [01:23<00:17, 6772.15 examples/s]Map (num_proc=32):  79%|███████▉  | 439776/558128 [01:23<00:17, 6843.13 examples/s]Map (num_proc=32):  79%|███████▉  | 440629/558128 [01:24<00:17, 6708.62 examples/s]Map (num_proc=32):  79%|███████▉  | 441448/558128 [01:24<00:17, 6776.79 examples/s]Map (num_proc=32):  79%|███████▉  | 442281/558128 [01:24<00:17, 6756.51 examples/s]Map (num_proc=32):  79%|███████▉  | 443111/558128 [01:24<00:18, 6311.00 examples/s]Map (num_proc=32):  80%|███████▉  | 443930/558128 [01:24<00:17, 6590.10 examples/s]Map (num_proc=32):  80%|███████▉  | 444622/558128 [01:25<00:52, 2163.12 examples/s]Map (num_proc=32):  80%|███████▉  | 445865/558128 [01:25<00:34, 3209.36 examples/s]Map (num_proc=32):  80%|████████  | 447498/558128 [01:25<00:22, 4836.85 examples/s]Map (num_proc=32):  80%|████████  | 448900/558128 [01:25<00:17, 6212.36 examples/s]Map (num_proc=32):  81%|████████  | 450387/558128 [01:25<00:14, 7659.66 examples/s]Map (num_proc=32):  81%|████████  | 451724/558128 [01:26<00:13, 7677.07 examples/s]Map (num_proc=32):  81%|████████  | 452901/558128 [01:26<00:14, 7265.48 examples/s]Map (num_proc=32):  81%|████████▏ | 453890/558128 [01:26<00:15, 6908.29 examples/s]Map (num_proc=32):  81%|████████▏ | 454728/558128 [01:26<00:20, 4939.42 examples/s]Map (num_proc=32):  82%|████████▏ | 456235/558128 [01:26<00:15, 6563.95 examples/s]Map (num_proc=32):  82%|████████▏ | 457414/558128 [01:26<00:13, 7363.92 examples/s]Map (num_proc=32):  82%|████████▏ | 458382/558128 [01:27<00:13, 7234.89 examples/s]Map (num_proc=32):  82%|████████▏ | 459333/558128 [01:27<00:14, 6969.07 examples/s]Map (num_proc=32):  82%|████████▏ | 460160/558128 [01:27<00:15, 6473.30 examples/s]Map (num_proc=32):  83%|████████▎ | 460929/558128 [01:27<00:14, 6584.78 examples/s]Map (num_proc=32):  83%|████████▎ | 461647/558128 [01:27<00:16, 5941.25 examples/s]Map (num_proc=32):  83%|████████▎ | 462411/558128 [01:28<00:42, 2273.43 examples/s]Map (num_proc=32):  83%|████████▎ | 463983/558128 [01:28<00:25, 3643.97 examples/s]Map (num_proc=32):  83%|████████▎ | 465655/558128 [01:28<00:17, 5270.91 examples/s]Map (num_proc=32):  84%|████████▎ | 467166/558128 [01:28<00:13, 6694.55 examples/s]Map (num_proc=32):  84%|████████▍ | 468435/558128 [01:29<00:12, 7253.46 examples/s]Map (num_proc=32):  84%|████████▍ | 469569/558128 [01:29<00:12, 7197.60 examples/s]Map (num_proc=32):  84%|████████▍ | 470568/558128 [01:29<00:12, 7016.04 examples/s]Map (num_proc=32):  84%|████████▍ | 471502/558128 [01:29<00:12, 6793.94 examples/s]Map (num_proc=32):  85%|████████▍ | 472328/558128 [01:29<00:18, 4696.22 examples/s]Map (num_proc=32):  85%|████████▍ | 473996/558128 [01:29<00:12, 6631.40 examples/s]Map (num_proc=32):  85%|████████▌ | 475000/558128 [01:30<00:11, 7002.14 examples/s]Map (num_proc=32):  85%|████████▌ | 475986/558128 [01:30<00:11, 7010.32 examples/s]Map (num_proc=32):  85%|████████▌ | 476845/558128 [01:30<00:11, 7068.84 examples/s]Map (num_proc=32):  86%|████████▌ | 477678/558128 [01:30<00:12, 6520.25 examples/s]Map (num_proc=32):  86%|████████▌ | 478502/558128 [01:30<00:11, 6638.51 examples/s]Map (num_proc=32):  86%|████████▌ | 479255/558128 [01:31<00:34, 2272.51 examples/s]Map (num_proc=32):  86%|████████▌ | 480633/558128 [01:31<00:22, 3414.45 examples/s]Map (num_proc=32):  86%|████████▋ | 482088/558128 [01:31<00:15, 4786.75 examples/s]Map (num_proc=32):  87%|████████▋ | 483679/558128 [01:31<00:11, 6384.26 examples/s]Map (num_proc=32):  87%|████████▋ | 485385/558128 [01:31<00:09, 7928.80 examples/s]Map (num_proc=32):  87%|████████▋ | 486644/558128 [01:32<00:09, 7236.31 examples/s]Map (num_proc=32):  87%|████████▋ | 487776/558128 [01:32<00:09, 7184.58 examples/s]Map (num_proc=32):  88%|████████▊ | 488774/558128 [01:32<00:09, 6956.16 examples/s]Map (num_proc=32):  88%|████████▊ | 489756/558128 [01:32<00:10, 6790.08 examples/s]Map (num_proc=32):  88%|████████▊ | 490603/558128 [01:32<00:13, 5090.39 examples/s]Map (num_proc=32):  88%|████████▊ | 492297/558128 [01:33<00:09, 6927.84 examples/s]Map (num_proc=32):  88%|████████▊ | 493285/558128 [01:33<00:09, 6879.28 examples/s]Map (num_proc=32):  89%|████████▊ | 494113/558128 [01:33<00:09, 7049.12 examples/s]Map (num_proc=32):  89%|████████▊ | 494942/558128 [01:33<00:09, 6462.99 examples/s]Map (num_proc=32):  89%|████████▉ | 495778/558128 [01:33<00:09, 6636.93 examples/s]Map (num_proc=32):  89%|████████▉ | 496531/558128 [01:33<00:10, 6050.44 examples/s]Map (num_proc=32):  89%|████████▉ | 497318/558128 [01:34<00:26, 2325.42 examples/s]Map (num_proc=32):  89%|████████▉ | 498696/558128 [01:34<00:16, 3505.26 examples/s]Map (num_proc=32):  90%|████████▉ | 500364/558128 [01:34<00:11, 5150.05 examples/s]Map (num_proc=32):  90%|████████▉ | 501900/558128 [01:34<00:08, 6676.76 examples/s]Map (num_proc=32):  90%|█████████ | 503049/558128 [01:35<00:07, 6956.36 examples/s]Map (num_proc=32):  90%|█████████ | 504222/558128 [01:35<00:07, 6930.68 examples/s]Map (num_proc=32):  91%|█████████ | 505214/558128 [01:35<00:07, 6820.01 examples/s]Map (num_proc=32):  91%|█████████ | 506192/558128 [01:35<00:07, 6587.80 examples/s]Map (num_proc=32):  91%|█████████ | 506974/558128 [01:35<00:07, 6469.26 examples/s]Map (num_proc=32):  91%|█████████ | 507837/558128 [01:35<00:07, 6610.71 examples/s]Map (num_proc=32):  91%|█████████ | 508591/558128 [01:36<00:10, 4888.99 examples/s]Map (num_proc=32):  91%|█████████▏| 510442/558128 [01:36<00:06, 6911.46 examples/s]Map (num_proc=32):  92%|█████████▏| 511404/558128 [01:36<00:06, 6769.23 examples/s]Map (num_proc=32):  92%|█████████▏| 512231/558128 [01:36<00:06, 6704.57 examples/s]Map (num_proc=32):  92%|█████████▏| 513040/558128 [01:36<00:07, 6267.32 examples/s]Map (num_proc=32):  92%|█████████▏| 513805/558128 [01:37<00:10, 4157.02 examples/s]Map (num_proc=32):  92%|█████████▏| 514806/558128 [01:37<00:09, 4769.55 examples/s]Map (num_proc=32):  92%|█████████▏| 515477/558128 [01:37<00:09, 4336.60 examples/s]Map (num_proc=32):  92%|█████████▏| 516130/558128 [01:37<00:09, 4532.15 examples/s]Map (num_proc=32):  93%|█████████▎| 516808/558128 [01:37<00:08, 4971.77 examples/s]Map (num_proc=32):  93%|█████████▎| 517461/558128 [01:37<00:07, 5267.68 examples/s]Map (num_proc=32):  93%|█████████▎| 518063/558128 [01:37<00:07, 5196.12 examples/s]Map (num_proc=32):  93%|█████████▎| 518724/558128 [01:37<00:07, 5538.64 examples/s]Map (num_proc=32):  93%|█████████▎| 519415/558128 [01:38<00:06, 5886.09 examples/s]Map (num_proc=32):  93%|█████████▎| 520039/558128 [01:38<00:06, 5824.27 examples/s]Map (num_proc=32):  93%|█████████▎| 521032/558128 [01:38<00:06, 6103.28 examples/s]Map (num_proc=32):  93%|█████████▎| 521663/558128 [01:38<00:05, 6094.99 examples/s]Map (num_proc=32):  94%|█████████▎| 522320/558128 [01:38<00:05, 6204.31 examples/s]Map (num_proc=32):  94%|█████████▎| 523003/558128 [01:38<00:05, 6368.17 examples/s]Map (num_proc=32):  94%|█████████▍| 523800/558128 [01:38<00:05, 6440.41 examples/s]Map (num_proc=32):  94%|█████████▍| 524462/558128 [01:38<00:05, 6430.38 examples/s]Map (num_proc=32):  94%|█████████▍| 525321/558128 [01:39<00:05, 5766.89 examples/s]Map (num_proc=32):  94%|█████████▍| 526422/558128 [01:39<00:04, 6712.66 examples/s]Map (num_proc=32):  94%|█████████▍| 527275/558128 [01:39<00:04, 6445.28 examples/s]Map (num_proc=32):  95%|█████████▍| 528029/558128 [01:39<00:04, 6445.52 examples/s]Map (num_proc=32):  95%|█████████▍| 528714/558128 [01:39<00:04, 6544.15 examples/s]Map (num_proc=32):  95%|█████████▍| 529391/558128 [01:39<00:04, 6598.50 examples/s]Map (num_proc=32):  95%|█████████▍| 530135/558128 [01:39<00:04, 6371.36 examples/s]Map (num_proc=32):  95%|█████████▌| 530908/558128 [01:39<00:04, 6279.95 examples/s]Map (num_proc=32):  95%|█████████▌| 531571/558128 [01:40<00:06, 4254.90 examples/s]Map (num_proc=32):  95%|█████████▌| 532680/558128 [01:40<00:04, 5551.78 examples/s]Map (num_proc=32):  96%|█████████▌| 533455/558128 [01:40<00:04, 5923.02 examples/s]Map (num_proc=32):  96%|█████████▌| 534238/558128 [01:40<00:04, 5793.71 examples/s]Map (num_proc=32):  96%|█████████▌| 535022/558128 [01:40<00:03, 6151.33 examples/s]Map (num_proc=32):  96%|█████████▌| 535813/558128 [01:40<00:03, 5877.79 examples/s]Map (num_proc=32):  96%|█████████▌| 536495/558128 [01:40<00:03, 6098.36 examples/s]Map (num_proc=32):  96%|█████████▌| 537138/558128 [01:40<00:03, 6154.09 examples/s]Map (num_proc=32):  96%|█████████▋| 537925/558128 [01:41<00:03, 6449.77 examples/s]Map (num_proc=32):  97%|█████████▋| 538605/558128 [01:41<00:02, 6536.95 examples/s]Map (num_proc=32):  97%|█████████▋| 539416/558128 [01:41<00:03, 5913.88 examples/s]Map (num_proc=32):  97%|█████████▋| 540272/558128 [01:41<00:02, 6517.56 examples/s]Map (num_proc=32):  97%|█████████▋| 541093/558128 [01:41<00:02, 6201.26 examples/s]Map (num_proc=32):  97%|█████████▋| 541858/558128 [01:41<00:02, 6339.44 examples/s]Map (num_proc=32):  97%|█████████▋| 542537/558128 [01:41<00:02, 6451.47 examples/s]Map (num_proc=32):  97%|█████████▋| 543320/558128 [01:41<00:02, 6151.81 examples/s]Map (num_proc=32):  97%|█████████▋| 543984/558128 [01:42<00:02, 6272.43 examples/s]Map (num_proc=32):  98%|█████████▊| 544669/558128 [01:42<00:02, 6418.49 examples/s]Map (num_proc=32):  98%|█████████▊| 545370/558128 [01:42<00:02, 5776.72 examples/s]Map (num_proc=32):  98%|█████████▊| 546014/558128 [01:42<00:02, 4581.47 examples/s]Map (num_proc=32):  98%|█████████▊| 546589/558128 [01:42<00:02, 4834.70 examples/s]Map (num_proc=32):  98%|█████████▊| 547278/558128 [01:42<00:02, 5321.60 examples/s]Map (num_proc=32):  98%|█████████▊| 547936/558128 [01:42<00:01, 5479.17 examples/s]Map (num_proc=32):  98%|█████████▊| 548580/558128 [01:42<00:01, 5726.74 examples/s]Map (num_proc=32):  98%|█████████▊| 549203/558128 [01:43<00:01, 5556.84 examples/s]Map (num_proc=32):  99%|█████████▊| 549853/558128 [01:43<00:01, 4357.21 examples/s]Map (num_proc=32):  99%|█████████▊| 550369/558128 [01:43<00:01, 4078.02 examples/s]Map (num_proc=32):  99%|█████████▊| 550855/558128 [01:43<00:01, 3724.68 examples/s]Map (num_proc=32):  99%|█████████▉| 551374/558128 [01:43<00:01, 3645.05 examples/s]Map (num_proc=32):  99%|█████████▉| 551857/558128 [01:43<00:01, 3446.72 examples/s]Map (num_proc=32):  99%|█████████▉| 552370/558128 [01:44<00:01, 3434.94 examples/s]Map (num_proc=32):  99%|█████████▉| 552857/558128 [01:44<00:01, 3254.70 examples/s]Map (num_proc=32):  99%|█████████▉| 553205/558128 [01:44<00:01, 3300.90 examples/s]Map (num_proc=32):  99%|█████████▉| 553687/558128 [01:44<00:01, 3194.83 examples/s]Map (num_proc=32):  99%|█████████▉| 554032/558128 [01:44<00:01, 3248.62 examples/s]Map (num_proc=32):  99%|█████████▉| 554376/558128 [01:44<00:01, 3293.28 examples/s]Map (num_proc=32):  99%|█████████▉| 554858/558128 [01:44<00:01, 3156.60 examples/s]Map (num_proc=32):  99%|█████████▉| 555206/558128 [01:44<00:00, 3231.25 examples/s]Map (num_proc=32): 100%|█████████▉| 555547/558128 [01:45<00:00, 3275.19 examples/s]Map (num_proc=32): 100%|█████████▉| 556030/558128 [01:45<00:00, 3184.06 examples/s]Map (num_proc=32): 100%|█████████▉| 556365/558128 [01:45<00:00, 3222.87 examples/s]Map (num_proc=32): 100%|█████████▉| 556857/558128 [01:45<00:00, 3135.37 examples/s]Map (num_proc=32): 100%|█████████▉| 557205/558128 [01:45<00:00, 3214.79 examples/s]Map (num_proc=32): 100%|█████████▉| 557540/558128 [01:45<00:00, 3248.32 examples/s]Map (num_proc=32): 100%|█████████▉| 558031/558128 [01:45<00:00, 3163.70 examples/s]Map (num_proc=32): 100%|██████████| 558128/558128 [01:46<00:00, 5252.86 examples/s]
Filter (num_proc=32):   0%|          | 0/558128 [00:00<?, ? examples/s]Filter (num_proc=32):   0%|          | 1000/558128 [00:00<05:03, 1837.39 examples/s]Filter (num_proc=32):   1%|          | 3000/558128 [00:00<01:39, 5562.02 examples/s]Filter (num_proc=32):   1%|▏         | 7000/558128 [00:00<00:43, 12681.01 examples/s]Filter (num_proc=32):   2%|▏         | 13000/558128 [00:00<00:23, 23590.76 examples/s]Filter (num_proc=32):   4%|▍         | 23000/558128 [00:00<00:13, 40701.29 examples/s]Filter (num_proc=32):   6%|▌         | 34000/558128 [00:01<00:09, 57745.87 examples/s]Filter (num_proc=32):   8%|▊         | 47000/558128 [00:01<00:07, 70858.54 examples/s]Filter (num_proc=32):  11%|█         | 61000/558128 [00:01<00:05, 88460.56 examples/s]Filter (num_proc=32):  13%|█▎        | 71442/558128 [00:01<00:05, 90615.80 examples/s]Filter (num_proc=32):  16%|█▌        | 87884/558128 [00:01<00:04, 108609.00 examples/s]Filter (num_proc=32):  18%|█▊        | 101884/558128 [00:01<00:03, 117136.40 examples/s]Filter (num_proc=32):  21%|██        | 116768/558128 [00:01<00:03, 116754.77 examples/s]Filter (num_proc=32):  24%|██▍       | 135768/558128 [00:01<00:03, 136697.57 examples/s]Filter (num_proc=32):  27%|██▋       | 152652/558128 [00:02<00:03, 134738.85 examples/s]Filter (num_proc=32):  31%|███       | 171094/558128 [00:02<00:02, 139581.40 examples/s]Filter (num_proc=32):  33%|███▎      | 185536/558128 [00:02<00:02, 138808.02 examples/s]Filter (num_proc=32):  36%|███▌      | 200536/558128 [00:02<00:02, 130255.36 examples/s]Filter (num_proc=32):  39%|███▊      | 214978/558128 [00:02<00:02, 123824.30 examples/s]Filter (num_proc=32):  41%|████      | 229420/558128 [00:02<00:02, 119821.01 examples/s]Filter (num_proc=32):  44%|████▍     | 244862/558128 [00:02<00:02, 127813.09 examples/s]Filter (num_proc=32):  46%|████▌     | 257862/558128 [00:02<00:02, 124486.19 examples/s]Filter (num_proc=32):  50%|█████     | 279746/558128 [00:02<00:02, 138420.24 examples/s]Filter (num_proc=32):  53%|█████▎    | 294188/558128 [00:03<00:01, 138033.26 examples/s]Filter (num_proc=32):  56%|█████▌    | 311630/558128 [00:03<00:01, 136649.50 examples/s]Filter (num_proc=32):  59%|█████▉    | 328072/558128 [00:03<00:01, 137561.99 examples/s]Filter (num_proc=32):  61%|██████▏   | 342072/558128 [00:03<00:01, 135036.68 examples/s]Filter (num_proc=32):  64%|██████▍   | 357513/558128 [00:03<00:02, 91300.18 examples/s] Filter (num_proc=32):  70%|██████▉   | 388395/558128 [00:03<00:01, 133713.07 examples/s]Filter (num_proc=32):  74%|███████▍  | 412277/558128 [00:03<00:00, 156932.15 examples/s]Filter (num_proc=32):  77%|███████▋  | 431718/558128 [00:04<00:00, 137678.94 examples/s]Filter (num_proc=32):  80%|████████  | 448159/558128 [00:04<00:00, 125093.23 examples/s]Filter (num_proc=32):  83%|████████▎ | 463600/558128 [00:04<00:00, 120365.21 examples/s]Filter (num_proc=32):  85%|████████▌ | 477041/558128 [00:04<00:00, 119321.35 examples/s]Filter (num_proc=32):  88%|████████▊ | 491482/558128 [00:04<00:00, 114316.35 examples/s]Filter (num_proc=32):  90%|█████████ | 504482/558128 [00:04<00:00, 110007.16 examples/s]Filter (num_proc=32):  93%|█████████▎| 520923/558128 [00:04<00:00, 119921.46 examples/s]Filter (num_proc=32):  96%|█████████▌| 534364/558128 [00:05<00:00, 110724.64 examples/s]Filter (num_proc=32):  98%|█████████▊| 547805/558128 [00:05<00:00, 102936.93 examples/s]Filter (num_proc=32): 100%|██████████| 558128/558128 [00:05<00:00, 98931.95 examples/s] 
Map (num_proc=32):   0%|          | 0/558128 [00:00<?, ? examples/s]Map (num_proc=32):   0%|          | 600/558128 [00:05<1:28:51, 104.58 examples/s]Map (num_proc=32):   0%|          | 1605/558128 [00:05<26:49, 345.68 examples/s] Map (num_proc=32):   0%|          | 2601/558128 [00:05<13:53, 666.88 examples/s]Map (num_proc=32):   1%|          | 3596/558128 [00:06<08:30, 1086.07 examples/s]Map (num_proc=32):   1%|          | 4600/558128 [00:06<05:39, 1629.23 examples/s]Map (num_proc=32):   1%|          | 5576/558128 [00:06<04:03, 2272.20 examples/s]Map (num_proc=32):   1%|          | 6601/558128 [00:06<03:00, 3063.30 examples/s]Map (num_proc=32):   1%|▏         | 7599/558128 [00:06<02:20, 3921.53 examples/s]Map (num_proc=32):   2%|▏         | 8601/558128 [00:06<01:55, 4747.98 examples/s]Map (num_proc=32):   2%|▏         | 9601/558128 [00:06<01:37, 5641.50 examples/s]Map (num_proc=32):   2%|▏         | 11000/558128 [00:06<01:23, 6569.95 examples/s]Map (num_proc=32):   2%|▏         | 12000/558128 [00:06<01:16, 7159.64 examples/s]Map (num_proc=32):   2%|▏         | 13000/558128 [00:07<01:11, 7624.89 examples/s]Map (num_proc=32):   3%|▎         | 14597/558128 [00:07<01:02, 8673.26 examples/s]Map (num_proc=32):   3%|▎         | 15587/558128 [00:07<01:53, 4795.67 examples/s]Map (num_proc=32):   3%|▎         | 18034/558128 [00:10<06:09, 1463.37 examples/s]Map (num_proc=32):   3%|▎         | 19047/558128 [00:10<04:59, 1802.39 examples/s]Map (num_proc=32):   4%|▎         | 20040/558128 [00:10<04:00, 2232.97 examples/s]Map (num_proc=32):   4%|▍         | 21043/558128 [00:10<03:14, 2756.64 examples/s]Map (num_proc=32):   4%|▍         | 22034/558128 [00:11<02:37, 3399.81 examples/s]Map (num_proc=32):   4%|▍         | 23041/558128 [00:11<02:09, 4145.29 examples/s]Map (num_proc=32):   4%|▍         | 24048/558128 [00:11<01:48, 4940.92 examples/s]Map (num_proc=32):   4%|▍         | 25030/558128 [00:11<01:33, 5671.66 examples/s]Map (num_proc=32):   5%|▍         | 26040/558128 [00:11<01:22, 6465.28 examples/s]Map (num_proc=32):   5%|▍         | 27040/558128 [00:11<01:38, 5371.67 examples/s]Map (num_proc=32):   5%|▌         | 29442/558128 [00:11<01:00, 8675.00 examples/s]Map (num_proc=32):   6%|▌         | 31037/558128 [00:11<00:58, 9007.36 examples/s]Map (num_proc=32):   6%|▌         | 32442/558128 [00:12<00:58, 9006.15 examples/s]Map (num_proc=32):   6%|▌         | 34033/558128 [00:12<00:56, 9296.48 examples/s]Map (num_proc=32):   6%|▋         | 35455/558128 [00:15<05:49, 1494.21 examples/s]Map (num_proc=32):   7%|▋         | 36483/558128 [00:15<04:40, 1856.69 examples/s]Map (num_proc=32):   7%|▋         | 37479/558128 [00:15<04:15, 2039.06 examples/s]Map (num_proc=32):   7%|▋         | 40884/558128 [00:15<02:09, 3986.12 examples/s]Map (num_proc=32):   8%|▊         | 42476/558128 [00:15<01:47, 4797.50 examples/s]Map (num_proc=32):   8%|▊         | 43884/558128 [00:16<01:35, 5366.86 examples/s]Map (num_proc=32):   8%|▊         | 45481/558128 [00:16<01:21, 6268.78 examples/s]Map (num_proc=32):   8%|▊         | 46884/558128 [00:16<01:15, 6799.21 examples/s]Map (num_proc=32):   9%|▊         | 47884/558128 [00:16<01:12, 7032.98 examples/s]Map (num_proc=32):   9%|▉         | 48884/558128 [00:16<01:07, 7552.98 examples/s]Map (num_proc=32):   9%|▉         | 49884/558128 [00:16<01:04, 7890.90 examples/s]Map (num_proc=32):   9%|▉         | 50884/558128 [00:16<01:01, 8272.62 examples/s]Map (num_proc=32):   9%|▉         | 51884/558128 [00:16<00:58, 8680.86 examples/s]Map (num_proc=32):   9%|▉         | 52862/558128 [00:20<07:59, 1054.77 examples/s]Map (num_proc=32):  10%|▉         | 53927/558128 [00:20<05:50, 1440.29 examples/s]Map (num_proc=32):  10%|▉         | 54924/558128 [00:20<04:24, 1904.69 examples/s]Map (num_proc=32):  10%|█         | 55919/558128 [00:20<03:22, 2475.99 examples/s]Map (num_proc=32):  10%|█         | 56914/558128 [00:20<02:38, 3164.50 examples/s]Map (num_proc=32):  10%|█         | 58326/558128 [00:20<02:02, 4092.02 examples/s]Map (num_proc=32):  11%|█         | 59918/558128 [00:20<01:33, 5302.58 examples/s]Map (num_proc=32):  11%|█         | 61326/558128 [00:20<01:23, 5978.97 examples/s]Map (num_proc=32):  11%|█▏        | 62925/558128 [00:21<01:09, 7128.06 examples/s]Map (num_proc=32):  11%|█▏        | 63921/558128 [00:21<01:05, 7585.08 examples/s]Map (num_proc=32):  12%|█▏        | 64922/558128 [00:21<01:02, 7906.84 examples/s]Map (num_proc=32):  12%|█▏        | 65916/558128 [00:21<00:59, 8274.97 examples/s]Map (num_proc=32):  12%|█▏        | 67326/558128 [00:21<01:24, 5788.89 examples/s]Map (num_proc=32):  13%|█▎        | 70313/558128 [00:24<04:06, 1979.19 examples/s]Map (num_proc=32):  13%|█▎        | 71368/558128 [00:24<03:26, 2358.66 examples/s]Map (num_proc=32):  13%|█▎        | 72309/558128 [00:24<02:54, 2786.26 examples/s]Map (num_proc=32):  13%|█▎        | 73362/558128 [00:24<02:23, 3387.25 examples/s]Map (num_proc=32):  13%|█▎        | 74335/558128 [00:24<02:00, 4026.39 examples/s]Map (num_proc=32):  14%|█▎        | 75366/558128 [00:24<01:40, 4824.17 examples/s]Map (num_proc=32):  14%|█▎        | 76364/558128 [00:24<01:29, 5378.60 examples/s]Map (num_proc=32):  14%|█▍        | 77768/558128 [00:25<01:16, 6266.80 examples/s]Map (num_proc=32):  14%|█▍        | 79371/558128 [00:25<01:04, 7377.23 examples/s]Map (num_proc=32):  14%|█▍        | 80365/558128 [00:25<01:24, 5648.12 examples/s]Map (num_proc=32):  15%|█▍        | 83361/558128 [00:25<00:51, 9237.59 examples/s]Map (num_proc=32):  15%|█▌        | 84768/558128 [00:25<00:52, 9086.64 examples/s]Map (num_proc=32):  15%|█▌        | 86188/558128 [00:25<00:51, 9083.14 examples/s]Map (num_proc=32):  16%|█▌        | 87759/558128 [00:28<04:24, 1780.68 examples/s]Map (num_proc=32):  16%|█▌        | 88806/558128 [00:28<03:35, 2174.07 examples/s]Map (num_proc=32):  16%|█▌        | 89758/558128 [00:28<02:59, 2611.85 examples/s]Map (num_proc=32):  16%|█▋        | 90804/558128 [00:28<02:24, 3235.02 examples/s]Map (num_proc=32):  16%|█▋        | 91804/558128 [00:28<01:59, 3913.74 examples/s]Map (num_proc=32):  17%|█▋        | 92799/558128 [00:29<01:59, 3898.19 examples/s]Map (num_proc=32):  17%|█▋        | 95210/558128 [00:29<01:12, 6400.15 examples/s]Map (num_proc=32):  17%|█▋        | 96806/558128 [00:29<01:03, 7225.14 examples/s]Map (num_proc=32):  18%|█▊        | 98210/558128 [00:29<01:00, 7627.41 examples/s]Map (num_proc=32):  18%|█▊        | 99807/558128 [00:29<00:55, 8218.88 examples/s]Map (num_proc=32):  18%|█▊        | 101210/558128 [00:29<00:54, 8337.68 examples/s]Map (num_proc=32):  18%|█▊        | 102799/558128 [00:30<00:51, 8863.51 examples/s]Map (num_proc=32):  19%|█▊        | 104210/558128 [00:30<00:52, 8694.27 examples/s]Map (num_proc=32):  19%|█▉        | 105204/558128 [00:32<05:00, 1504.75 examples/s]Map (num_proc=32):  19%|█▉        | 106254/558128 [00:32<03:56, 1909.61 examples/s]Map (num_proc=32):  19%|█▉        | 107192/558128 [00:33<03:10, 2362.56 examples/s]Map (num_proc=32):  19%|█▉        | 108251/558128 [00:33<02:29, 3003.16 examples/s]Map (num_proc=32):  20%|█▉        | 109652/558128 [00:33<01:54, 3921.32 examples/s]Map (num_proc=32):  20%|█▉        | 110652/558128 [00:33<01:37, 4602.74 examples/s]Map (num_proc=32):  20%|██        | 111652/558128 [00:33<01:23, 5366.22 examples/s]Map (num_proc=32):  20%|██        | 112652/558128 [00:33<01:14, 5952.31 examples/s]Map (num_proc=32):  20%|██        | 114253/558128 [00:33<01:01, 7267.22 examples/s]Map (num_proc=32):  21%|██        | 115251/558128 [00:33<00:57, 7682.57 examples/s]Map (num_proc=32):  21%|██        | 116250/558128 [00:34<00:54, 8118.53 examples/s]Map (num_proc=32):  21%|██        | 117221/558128 [00:34<00:52, 8345.63 examples/s]Map (num_proc=32):  21%|██        | 118249/558128 [00:34<00:50, 8672.03 examples/s]Map (num_proc=32):  21%|██▏       | 119249/558128 [00:34<00:49, 8862.09 examples/s]Map (num_proc=32):  22%|██▏       | 120238/558128 [00:34<00:48, 9037.50 examples/s]Map (num_proc=32):  22%|██▏       | 121241/558128 [00:34<00:47, 9183.95 examples/s]Map (num_proc=32):  22%|██▏       | 122688/558128 [00:37<05:21, 1352.30 examples/s]Map (num_proc=32):  22%|██▏       | 123697/558128 [00:37<04:05, 1766.86 examples/s]Map (num_proc=32):  22%|██▏       | 124666/558128 [00:37<03:11, 2268.95 examples/s]Map (num_proc=32):  23%|██▎       | 125686/558128 [00:37<02:28, 2916.66 examples/s]Map (num_proc=32):  23%|██▎       | 127094/558128 [00:37<01:52, 3842.13 examples/s]Map (num_proc=32):  23%|██▎       | 128094/558128 [00:37<01:34, 4565.34 examples/s]Map (num_proc=32):  23%|██▎       | 129632/558128 [00:37<01:14, 5765.05 examples/s]Map (num_proc=32):  23%|██▎       | 130692/558128 [00:37<01:05, 6483.01 examples/s]Map (num_proc=32):  24%|██▎       | 131699/558128 [00:38<00:59, 7114.63 examples/s]Map (num_proc=32):  24%|██▍       | 132668/558128 [00:38<01:14, 5729.37 examples/s]Map (num_proc=32):  24%|██▍       | 135094/558128 [00:38<00:48, 8782.94 examples/s]Map (num_proc=32):  24%|██▍       | 136688/558128 [00:38<00:45, 9209.31 examples/s]Map (num_proc=32):  25%|██▍       | 138094/558128 [00:38<00:47, 8877.27 examples/s]Map (num_proc=32):  25%|██▌       | 139536/558128 [00:38<00:45, 9140.87 examples/s]Map (num_proc=32):  25%|██▌       | 141143/558128 [00:41<04:13, 1646.67 examples/s]Map (num_proc=32):  25%|██▌       | 142079/558128 [00:41<03:53, 1781.09 examples/s]Map (num_proc=32):  26%|██▌       | 145536/558128 [00:42<01:56, 3529.87 examples/s]Map (num_proc=32):  26%|██▋       | 147138/558128 [00:42<01:36, 4256.04 examples/s]Map (num_proc=32):  27%|██▋       | 148536/558128 [00:42<01:24, 4822.12 examples/s]Map (num_proc=32):  27%|██▋       | 150134/558128 [00:42<01:11, 5716.91 examples/s]Map (num_proc=32):  27%|██▋       | 151536/558128 [00:42<01:04, 6268.29 examples/s]Map (num_proc=32):  27%|██▋       | 153102/558128 [00:42<00:57, 7055.82 examples/s]Map (num_proc=32):  28%|██▊       | 154536/558128 [00:43<00:54, 7446.31 examples/s]Map (num_proc=32):  28%|██▊       | 155536/558128 [00:43<00:51, 7800.68 examples/s]Map (num_proc=32):  28%|██▊       | 156536/558128 [00:43<00:49, 8115.08 examples/s]Map (num_proc=32):  28%|██▊       | 157526/558128 [00:46<05:27, 1223.41 examples/s]Map (num_proc=32):  28%|██▊       | 158577/558128 [00:46<04:08, 1610.66 examples/s]Map (num_proc=32):  29%|██▊       | 159568/558128 [00:46<03:12, 2073.21 examples/s]Map (num_proc=32):  29%|██▉       | 160565/558128 [00:46<02:29, 2654.42 examples/s]Map (num_proc=32):  29%|██▉       | 161545/558128 [00:46<01:59, 3332.43 examples/s]Map (num_proc=32):  29%|██▉       | 162575/558128 [00:46<01:35, 4145.84 examples/s]Map (num_proc=32):  29%|██▉       | 163581/558128 [00:46<01:18, 5006.47 examples/s]Map (num_proc=32):  29%|██▉       | 164571/558128 [00:46<01:09, 5643.86 examples/s]Map (num_proc=32):  30%|██▉       | 165576/558128 [00:46<01:00, 6447.16 examples/s]Map (num_proc=32):  30%|██▉       | 166978/558128 [00:47<00:53, 7292.51 examples/s]Map (num_proc=32):  30%|███       | 167978/558128 [00:47<00:50, 7690.32 examples/s]Map (num_proc=32):  30%|███       | 168978/558128 [00:47<00:48, 8030.24 examples/s]Map (num_proc=32):  30%|███       | 169978/558128 [00:47<00:46, 8421.48 examples/s]Map (num_proc=32):  31%|███       | 170978/558128 [00:47<00:44, 8666.60 examples/s]Map (num_proc=32):  31%|███       | 171978/558128 [00:47<00:43, 8877.56 examples/s]Map (num_proc=32):  31%|███       | 172978/558128 [00:47<00:42, 9043.76 examples/s]Map (num_proc=32):  31%|███       | 173978/558128 [00:47<00:43, 8919.58 examples/s]Map (num_proc=32):  31%|███▏      | 174991/558128 [00:50<05:56, 1073.46 examples/s]Map (num_proc=32):  32%|███▏      | 176021/558128 [00:50<04:20, 1469.64 examples/s]Map (num_proc=32):  32%|███▏      | 176993/558128 [00:50<03:15, 1946.75 examples/s]Map (num_proc=32):  32%|███▏      | 178020/558128 [00:51<02:27, 2573.87 examples/s]Map (num_proc=32):  32%|███▏      | 179420/558128 [00:51<01:48, 3479.68 examples/s]Map (num_proc=32):  32%|███▏      | 181016/558128 [00:51<01:20, 4657.88 examples/s]Map (num_proc=32):  33%|███▎      | 181992/558128 [00:51<01:10, 5306.90 examples/s]Map (num_proc=32):  33%|███▎      | 183018/558128 [00:51<01:01, 6054.87 examples/s]Map (num_proc=32):  33%|███▎      | 184016/558128 [00:51<00:55, 6742.02 examples/s]Map (num_proc=32):  33%|███▎      | 185015/558128 [00:51<00:50, 7359.87 examples/s]Map (num_proc=32):  33%|███▎      | 186004/558128 [00:52<01:03, 5888.63 examples/s]Map (num_proc=32):  34%|███▍      | 188420/558128 [00:52<00:41, 8917.43 examples/s]Map (num_proc=32):  34%|███▍      | 190007/558128 [00:52<00:40, 9169.27 examples/s]Map (num_proc=32):  34%|███▍      | 191420/558128 [00:52<00:40, 8986.75 examples/s]Map (num_proc=32):  34%|███▍      | 192454/558128 [00:55<04:03, 1501.82 examples/s]Map (num_proc=32):  35%|███▍      | 193462/558128 [00:55<03:13, 1884.83 examples/s]Map (num_proc=32):  35%|███▍      | 194436/558128 [00:55<02:34, 2359.39 examples/s]Map (num_proc=32):  35%|███▌      | 195455/558128 [00:55<02:01, 2973.73 examples/s]Map (num_proc=32):  35%|███▌      | 196465/558128 [00:55<01:38, 3689.76 examples/s]Map (num_proc=32):  35%|███▌      | 197458/558128 [00:55<01:21, 4443.87 examples/s]Map (num_proc=32):  36%|███▌      | 198459/558128 [00:55<01:25, 4207.94 examples/s]Map (num_proc=32):  36%|███▌      | 200862/558128 [00:56<00:50, 7030.82 examples/s]Map (num_proc=32):  36%|███▋      | 202459/558128 [00:56<00:45, 7839.40 examples/s]Map (num_proc=32):  37%|███▋      | 203862/558128 [00:56<00:43, 8091.41 examples/s]Map (num_proc=32):  37%|███▋      | 205451/558128 [00:56<00:41, 8585.59 examples/s]Map (num_proc=32):  37%|███▋      | 206862/558128 [00:56<00:40, 8695.53 examples/s]Map (num_proc=32):  37%|███▋      | 207862/558128 [00:56<00:40, 8747.06 examples/s]Map (num_proc=32):  38%|███▊      | 209304/558128 [00:56<00:38, 8996.59 examples/s]Map (num_proc=32):  38%|███▊      | 210304/558128 [00:59<04:16, 1358.68 examples/s]Map (num_proc=32):  38%|███▊      | 212304/558128 [00:59<02:39, 2164.36 examples/s]Map (num_proc=32):  38%|███▊      | 213304/558128 [00:59<02:11, 2620.11 examples/s]Map (num_proc=32):  38%|███▊      | 214304/558128 [00:59<01:48, 3177.03 examples/s]Map (num_proc=32):  39%|███▊      | 215304/558128 [01:00<01:29, 3830.94 examples/s]Map (num_proc=32):  39%|███▉      | 216304/558128 [01:00<01:15, 4505.56 examples/s]Map (num_proc=32):  39%|███▉      | 217304/558128 [01:00<01:04, 5289.61 examples/s]Map (num_proc=32):  39%|███▉      | 218304/558128 [01:00<00:56, 6062.43 examples/s]Map (num_proc=32):  39%|███▉      | 219304/558128 [01:00<00:49, 6784.89 examples/s]Map (num_proc=32):  40%|███▉      | 220869/558128 [01:00<00:43, 7802.41 examples/s]Map (num_proc=32):  40%|███▉      | 221900/558128 [01:00<00:41, 8155.40 examples/s]Map (num_proc=32):  40%|███▉      | 222898/558128 [01:00<00:39, 8449.92 examples/s]Map (num_proc=32):  40%|████      | 223891/558128 [01:00<00:38, 8712.56 examples/s]Map (num_proc=32):  40%|████      | 224890/558128 [01:01<00:38, 8662.09 examples/s]Map (num_proc=32):  40%|████      | 225893/558128 [01:01<00:37, 8900.31 examples/s]Map (num_proc=32):  41%|████      | 227332/558128 [01:03<04:20, 1270.69 examples/s]Map (num_proc=32):  41%|████      | 228344/558128 [01:04<03:18, 1663.11 examples/s]Map (num_proc=32):  41%|████      | 229337/558128 [01:04<02:32, 2150.84 examples/s]Map (num_proc=32):  41%|████▏     | 230301/558128 [01:04<02:00, 2731.39 examples/s]Map (num_proc=32):  41%|████▏     | 231335/558128 [01:04<01:33, 3476.98 examples/s]Map (num_proc=32):  42%|████▏     | 232282/558128 [01:04<01:17, 4205.89 examples/s]Map (num_proc=32):  42%|████▏     | 233337/558128 [01:04<01:03, 5079.42 examples/s]Map (num_proc=32):  42%|████▏     | 234295/558128 [01:04<00:55, 5824.27 examples/s]Map (num_proc=32):  42%|████▏     | 235336/558128 [01:04<00:48, 6629.81 examples/s]Map (num_proc=32):  42%|████▏     | 236343/558128 [01:04<00:44, 7276.88 examples/s]Map (num_proc=32):  43%|████▎     | 237323/558128 [01:05<01:00, 5278.75 examples/s]Map (num_proc=32):  43%|████▎     | 240335/558128 [01:05<00:33, 9515.77 examples/s]Map (num_proc=32):  43%|████▎     | 241741/558128 [01:05<00:33, 9388.81 examples/s]Map (num_proc=32):  44%|████▎     | 243050/558128 [01:05<00:34, 9042.31 examples/s]Map (num_proc=32):  44%|████▍     | 244188/558128 [01:05<00:34, 9221.73 examples/s]Map (num_proc=32):  44%|████▍     | 245789/558128 [01:08<03:28, 1495.81 examples/s]Map (num_proc=32):  44%|████▍     | 247188/558128 [01:08<02:35, 1999.48 examples/s]Map (num_proc=32):  44%|████▍     | 248188/558128 [01:09<02:22, 2170.91 examples/s]Map (num_proc=32):  45%|████▌     | 251781/558128 [01:09<01:11, 4304.24 examples/s]Map (num_proc=32):  45%|████▌     | 253188/558128 [01:09<01:02, 4887.96 examples/s]Map (num_proc=32):  46%|████▌     | 254782/558128 [01:09<00:53, 5711.96 examples/s]Map (num_proc=32):  46%|████▌     | 256188/558128 [01:09<00:48, 6250.89 examples/s]Map (num_proc=32):  46%|████▌     | 257781/558128 [01:09<00:43, 6962.76 examples/s]Map (num_proc=32):  46%|████▋     | 259188/558128 [01:10<00:40, 7383.00 examples/s]Map (num_proc=32):  47%|████▋     | 260775/558128 [01:10<00:36, 8134.48 examples/s]Map (num_proc=32):  47%|████▋     | 262221/558128 [01:12<03:02, 1619.08 examples/s]Map (num_proc=32):  47%|████▋     | 263230/558128 [01:13<02:29, 1974.19 examples/s]Map (num_proc=32):  47%|████▋     | 264630/558128 [01:13<01:54, 2567.51 examples/s]Map (num_proc=32):  48%|████▊     | 266231/558128 [01:13<01:24, 3455.32 examples/s]Map (num_proc=32):  48%|████▊     | 267226/558128 [01:13<01:12, 4022.61 examples/s]Map (num_proc=32):  48%|████▊     | 268229/558128 [01:13<01:02, 4675.35 examples/s]Map (num_proc=32):  48%|████▊     | 269630/558128 [01:13<00:51, 5602.50 examples/s]Map (num_proc=32):  48%|████▊     | 270630/558128 [01:13<00:46, 6121.46 examples/s]Map (num_proc=32):  49%|████▉     | 272227/558128 [01:13<00:39, 7304.42 examples/s]Map (num_proc=32):  49%|████▉     | 273226/558128 [01:14<00:37, 7649.54 examples/s]Map (num_proc=32):  49%|████▉     | 274226/558128 [01:14<00:35, 7982.53 examples/s]Map (num_proc=32):  49%|████▉     | 275217/558128 [01:14<00:33, 8362.26 examples/s]Map (num_proc=32):  49%|████▉     | 276222/558128 [01:14<00:32, 8653.36 examples/s]Map (num_proc=32):  50%|████▉     | 277630/558128 [01:14<00:32, 8558.09 examples/s]Map (num_proc=32):  50%|████▉     | 278630/558128 [01:14<00:33, 8439.43 examples/s]Map (num_proc=32):  50%|█████     | 279663/558128 [01:17<04:20, 1068.27 examples/s]Map (num_proc=32):  50%|█████     | 280676/558128 [01:17<03:14, 1426.42 examples/s]Map (num_proc=32):  50%|█████     | 281669/558128 [01:18<02:26, 1882.52 examples/s]Map (num_proc=32):  51%|█████     | 282666/558128 [01:18<01:52, 2454.32 examples/s]Map (num_proc=32):  51%|█████     | 283668/558128 [01:18<01:27, 3139.49 examples/s]Map (num_proc=32):  51%|█████     | 284668/558128 [01:18<01:09, 3922.82 examples/s]Map (num_proc=32):  51%|█████     | 285646/558128 [01:18<00:57, 4718.61 examples/s]Map (num_proc=32):  51%|█████▏    | 286665/558128 [01:18<00:48, 5586.59 examples/s]Map (num_proc=32):  52%|█████▏    | 287623/558128 [01:18<00:42, 6313.81 examples/s]Map (num_proc=32):  52%|█████▏    | 288667/558128 [01:18<00:37, 7146.23 examples/s]Map (num_proc=32):  52%|█████▏    | 289660/558128 [01:18<00:35, 7636.83 examples/s]Map (num_proc=32):  52%|█████▏    | 290616/558128 [01:18<00:33, 8002.40 examples/s]Map (num_proc=32):  52%|█████▏    | 291661/558128 [01:19<00:31, 8330.40 examples/s]Map (num_proc=32):  52%|█████▏    | 292653/558128 [01:19<00:30, 8618.67 examples/s]Map (num_proc=32):  53%|█████▎    | 293655/558128 [01:19<00:30, 8723.43 examples/s]Map (num_proc=32):  53%|█████▎    | 294658/558128 [01:19<00:29, 8928.90 examples/s]Map (num_proc=32):  53%|█████▎    | 295624/558128 [01:19<00:43, 6104.06 examples/s]Map (num_proc=32):  53%|█████▎    | 297107/558128 [01:22<03:35, 1211.13 examples/s]Map (num_proc=32):  53%|█████▎    | 298114/558128 [01:22<02:43, 1593.54 examples/s]Map (num_proc=32):  54%|█████▎    | 299099/558128 [01:22<02:05, 2064.09 examples/s]Map (num_proc=32):  54%|█████▍    | 300109/558128 [01:22<01:36, 2678.76 examples/s]Map (num_proc=32):  54%|█████▍    | 301106/558128 [01:22<01:16, 3373.41 examples/s]Map (num_proc=32):  54%|█████▍    | 302114/558128 [01:22<01:01, 4154.90 examples/s]Map (num_proc=32):  54%|█████▍    | 303106/558128 [01:23<00:51, 4965.38 examples/s]Map (num_proc=32):  54%|█████▍    | 304104/558128 [01:23<00:43, 5798.60 examples/s]Map (num_proc=32):  55%|█████▍    | 305084/558128 [01:23<00:58, 4350.59 examples/s]Map (num_proc=32):  55%|█████▌    | 308513/558128 [01:23<00:28, 8851.92 examples/s]Map (num_proc=32):  56%|█████▌    | 310107/558128 [01:23<00:27, 9059.25 examples/s]Map (num_proc=32):  56%|█████▌    | 311513/558128 [01:23<00:27, 9080.17 examples/s]Map (num_proc=32):  56%|█████▌    | 313101/558128 [01:24<00:26, 9413.09 examples/s]Map (num_proc=32):  56%|█████▋    | 314548/558128 [01:26<02:28, 1641.48 examples/s]Map (num_proc=32):  57%|█████▋    | 315548/558128 [01:27<02:08, 1885.03 examples/s]Map (num_proc=32):  57%|█████▋    | 317954/558128 [01:27<01:19, 3034.51 examples/s]Map (num_proc=32):  57%|█████▋    | 318954/558128 [01:27<01:08, 3504.67 examples/s]Map (num_proc=32):  57%|█████▋    | 319954/558128 [01:27<00:58, 4088.75 examples/s]Map (num_proc=32):  58%|█████▊    | 320954/558128 [01:27<00:50, 4714.70 examples/s]Map (num_proc=32):  58%|█████▊    | 321954/558128 [01:27<00:43, 5399.70 examples/s]Map (num_proc=32):  58%|█████▊    | 322954/558128 [01:27<00:38, 6150.16 examples/s]Map (num_proc=32):  58%|█████▊    | 323954/558128 [01:27<00:34, 6748.50 examples/s]Map (num_proc=32):  58%|█████▊    | 324954/558128 [01:28<00:31, 7356.04 examples/s]Map (num_proc=32):  58%|█████▊    | 325954/558128 [01:28<00:29, 7890.63 examples/s]Map (num_proc=32):  59%|█████▊    | 326954/558128 [01:28<00:28, 8205.89 examples/s]Map (num_proc=32):  59%|█████▉    | 327954/558128 [01:28<00:26, 8545.30 examples/s]Map (num_proc=32):  59%|█████▉    | 328954/558128 [01:28<00:26, 8572.58 examples/s]Map (num_proc=32):  59%|█████▉    | 330541/558128 [01:28<00:24, 9472.81 examples/s]Map (num_proc=32):  59%|█████▉    | 331970/558128 [01:31<02:45, 1369.12 examples/s]Map (num_proc=32):  60%|█████▉    | 332922/558128 [01:31<02:10, 1724.26 examples/s]Map (num_proc=32):  60%|█████▉    | 333989/558128 [01:31<01:40, 2237.69 examples/s]Map (num_proc=32):  60%|██████    | 334988/558128 [01:31<01:18, 2831.07 examples/s]Map (num_proc=32):  60%|██████    | 335986/558128 [01:31<01:02, 3539.62 examples/s]Map (num_proc=32):  60%|██████    | 336988/558128 [01:31<00:51, 4303.76 examples/s]Map (num_proc=32):  61%|██████    | 337989/558128 [01:32<00:43, 5114.88 examples/s]Map (num_proc=32):  61%|██████    | 338978/558128 [01:32<00:36, 5928.23 examples/s]Map (num_proc=32):  61%|██████    | 339989/558128 [01:32<00:32, 6672.02 examples/s]Map (num_proc=32):  61%|██████    | 340990/558128 [01:32<00:29, 7342.28 examples/s]Map (num_proc=32):  61%|██████▏   | 341988/558128 [01:32<00:27, 7886.45 examples/s]Map (num_proc=32):  62%|██████▏   | 343395/558128 [01:32<00:27, 7938.71 examples/s]Map (num_proc=32):  62%|██████▏   | 344395/558128 [01:32<00:25, 8344.22 examples/s]Map (num_proc=32):  62%|██████▏   | 345986/558128 [01:33<00:33, 6368.19 examples/s]Map (num_proc=32):  63%|██████▎   | 348836/558128 [01:33<00:20, 9966.67 examples/s]Map (num_proc=32):  63%|██████▎   | 350410/558128 [01:36<02:00, 1725.33 examples/s]Map (num_proc=32):  63%|██████▎   | 351432/558128 [01:36<01:39, 2082.87 examples/s]Map (num_proc=32):  63%|██████▎   | 352429/558128 [01:36<01:21, 2514.08 examples/s]Map (num_proc=32):  63%|██████▎   | 353836/558128 [01:36<01:03, 3221.71 examples/s]Map (num_proc=32):  64%|██████▎   | 354836/558128 [01:36<00:52, 3840.47 examples/s]Map (num_proc=32):  64%|██████▍   | 355836/558128 [01:36<00:55, 3661.13 examples/s]Map (num_proc=32):  64%|██████▍   | 358836/558128 [01:36<00:30, 6560.43 examples/s]Map (num_proc=32):  65%|██████▍   | 360428/558128 [01:37<00:27, 7313.01 examples/s]Map (num_proc=32):  65%|██████▍   | 361836/558128 [01:37<00:25, 7584.42 examples/s]Map (num_proc=32):  65%|██████▌   | 363426/558128 [01:37<00:23, 8223.81 examples/s]Map (num_proc=32):  65%|██████▌   | 364836/558128 [01:37<00:23, 8344.14 examples/s]Map (num_proc=32):  66%|██████▌   | 366277/558128 [01:37<00:22, 8639.46 examples/s]Map (num_proc=32):  66%|██████▌   | 367277/558128 [01:40<02:11, 1456.34 examples/s]Map (num_proc=32):  66%|██████▋   | 369870/558128 [01:40<01:15, 2488.31 examples/s]Map (num_proc=32):  67%|██████▋   | 371277/558128 [01:40<01:01, 3058.39 examples/s]Map (num_proc=32):  67%|██████▋   | 372872/558128 [01:40<00:48, 3838.05 examples/s]Map (num_proc=32):  67%|██████▋   | 373868/558128 [01:40<00:42, 4367.55 examples/s]Map (num_proc=32):  67%|██████▋   | 374872/558128 [01:41<00:36, 5001.19 examples/s]Map (num_proc=32):  67%|██████▋   | 376277/558128 [01:41<00:31, 5803.47 examples/s]Map (num_proc=32):  68%|██████▊   | 377277/558128 [01:41<00:28, 6306.87 examples/s]Map (num_proc=32):  68%|██████▊   | 378277/558128 [01:41<00:25, 6923.95 examples/s]Map (num_proc=32):  68%|██████▊   | 379871/558128 [01:41<00:22, 7984.11 examples/s]Map (num_proc=32):  68%|██████▊   | 380861/558128 [01:41<00:21, 8302.87 examples/s]Map (num_proc=32):  68%|██████▊   | 381863/558128 [01:41<00:20, 8468.73 examples/s]Map (num_proc=32):  69%|██████▊   | 382863/558128 [01:41<00:20, 8726.66 examples/s]Map (num_proc=32):  69%|██████▉   | 384312/558128 [01:44<02:13, 1297.49 examples/s]Map (num_proc=32):  69%|██████▉   | 385251/558128 [01:44<01:44, 1650.84 examples/s]Map (num_proc=32):  69%|██████▉   | 386314/558128 [01:44<01:18, 2176.13 examples/s]Map (num_proc=32):  69%|██████▉   | 387310/558128 [01:45<01:01, 2761.99 examples/s]Map (num_proc=32):  70%|██████▉   | 388718/558128 [01:45<00:46, 3653.65 examples/s]Map (num_proc=32):  70%|██████▉   | 390309/558128 [01:45<00:34, 4811.91 examples/s]Map (num_proc=32):  70%|███████   | 391302/558128 [01:45<00:30, 5482.69 examples/s]Map (num_proc=32):  70%|███████   | 392718/558128 [01:45<00:26, 6346.67 examples/s]Map (num_proc=32):  71%|███████   | 393718/558128 [01:45<00:23, 6940.03 examples/s]Map (num_proc=32):  71%|███████   | 394718/558128 [01:45<00:21, 7549.21 examples/s]Map (num_proc=32):  71%|███████   | 395718/558128 [01:45<00:21, 7589.84 examples/s]Map (num_proc=32):  71%|███████   | 397312/558128 [01:46<00:18, 8547.45 examples/s]Map (num_proc=32):  71%|███████▏  | 398308/558128 [01:46<00:18, 8769.96 examples/s]Map (num_proc=32):  72%|███████▏  | 399305/558128 [01:46<00:17, 8842.49 examples/s]Map (num_proc=32):  72%|███████▏  | 400305/558128 [01:46<00:24, 6470.03 examples/s]Map (num_proc=32):  72%|███████▏  | 401740/558128 [01:49<02:06, 1235.34 examples/s]Map (num_proc=32):  72%|███████▏  | 403159/558128 [01:49<01:28, 1749.20 examples/s]Map (num_proc=32):  72%|███████▏  | 404159/558128 [01:49<01:09, 2204.40 examples/s]Map (num_proc=32):  73%|███████▎  | 405159/558128 [01:49<00:55, 2774.46 examples/s]Map (num_proc=32):  73%|███████▎  | 406763/558128 [01:49<00:39, 3845.24 examples/s]Map (num_proc=32):  73%|███████▎  | 407760/558128 [01:50<00:33, 4506.31 examples/s]Map (num_proc=32):  73%|███████▎  | 408750/558128 [01:50<00:35, 4202.71 examples/s]Map (num_proc=32):  74%|███████▍  | 411761/558128 [01:50<00:20, 7282.19 examples/s]Map (num_proc=32):  74%|███████▍  | 413159/558128 [01:50<00:19, 7549.03 examples/s]Map (num_proc=32):  74%|███████▍  | 414752/558128 [01:50<00:17, 8189.87 examples/s]Map (num_proc=32):  75%|███████▍  | 416159/558128 [01:50<00:17, 8350.67 examples/s]Map (num_proc=32):  75%|███████▍  | 417159/558128 [01:51<00:16, 8660.48 examples/s]Map (num_proc=32):  75%|███████▍  | 418159/558128 [01:51<00:15, 8751.31 examples/s]Map (num_proc=32):  75%|███████▌  | 419195/558128 [01:53<01:37, 1424.59 examples/s]Map (num_proc=32):  75%|███████▌  | 420194/558128 [01:53<01:19, 1732.29 examples/s]Map (num_proc=32):  76%|███████▌  | 422600/558128 [01:53<00:45, 3008.72 examples/s]Map (num_proc=32):  76%|███████▌  | 424195/558128 [01:54<00:34, 3858.29 examples/s]Map (num_proc=32):  76%|███████▌  | 425197/558128 [01:54<00:29, 4441.57 examples/s]Map (num_proc=32):  76%|███████▋  | 426192/558128 [01:54<00:26, 5057.13 examples/s]Map (num_proc=32):  77%|███████▋  | 427195/558128 [01:54<00:22, 5730.31 examples/s]Map (num_proc=32):  77%|███████▋  | 428198/558128 [01:54<00:20, 6424.81 examples/s]Map (num_proc=32):  77%|███████▋  | 429167/558128 [01:54<00:18, 7003.85 examples/s]Map (num_proc=32):  77%|███████▋  | 430194/558128 [01:54<00:16, 7688.58 examples/s]Map (num_proc=32):  77%|███████▋  | 431181/558128 [01:54<00:15, 8118.24 examples/s]Map (num_proc=32):  77%|███████▋  | 432193/558128 [01:54<00:15, 8313.55 examples/s]Map (num_proc=32):  78%|███████▊  | 433194/558128 [01:55<00:14, 8647.49 examples/s]Map (num_proc=32):  78%|███████▊  | 434188/558128 [01:55<00:13, 8887.55 examples/s]Map (num_proc=32):  78%|███████▊  | 435187/558128 [01:55<00:13, 8799.46 examples/s]Map (num_proc=32):  78%|███████▊  | 436631/558128 [01:58<01:38, 1239.01 examples/s]Map (num_proc=32):  78%|███████▊  | 437640/558128 [01:58<01:14, 1627.61 examples/s]Map (num_proc=32):  79%|███████▊  | 438639/558128 [01:58<00:56, 2114.52 examples/s]Map (num_proc=32):  79%|███████▉  | 439638/558128 [01:58<00:43, 2717.86 examples/s]Map (num_proc=32):  79%|███████▉  | 440634/558128 [01:58<00:34, 3417.41 examples/s]Map (num_proc=32):  79%|███████▉  | 441634/558128 [01:58<00:27, 4212.94 examples/s]Map (num_proc=32):  79%|███████▉  | 442635/558128 [01:58<00:23, 5018.24 examples/s]Map (num_proc=32):  79%|███████▉  | 443603/558128 [01:58<00:19, 5796.67 examples/s]Map (num_proc=32):  80%|███████▉  | 444636/558128 [01:58<00:17, 6617.45 examples/s]Map (num_proc=32):  80%|███████▉  | 446041/558128 [01:59<00:15, 7158.06 examples/s]Map (num_proc=32):  80%|████████  | 447635/558128 [01:59<00:13, 8221.13 examples/s]Map (num_proc=32):  80%|████████  | 448604/558128 [01:59<00:13, 8392.23 examples/s]Map (num_proc=32):  81%|████████  | 449635/558128 [01:59<00:12, 8620.04 examples/s]Map (num_proc=32):  81%|████████  | 450634/558128 [01:59<00:16, 6668.99 examples/s]Map (num_proc=32):  81%|████████  | 453041/558128 [01:59<00:10, 9630.93 examples/s]Map (num_proc=32):  81%|████████▏ | 454482/558128 [02:02<01:06, 1567.00 examples/s]Map (num_proc=32):  82%|████████▏ | 455482/558128 [02:02<00:53, 1930.58 examples/s]Map (num_proc=32):  82%|████████▏ | 456482/558128 [02:02<00:42, 2391.11 examples/s]Map (num_proc=32):  82%|████████▏ | 458074/558128 [02:03<00:30, 3293.80 examples/s]Map (num_proc=32):  82%|████████▏ | 459046/558128 [02:03<00:25, 3891.53 examples/s]Map (num_proc=32):  82%|████████▏ | 460074/558128 [02:03<00:26, 3697.89 examples/s]Map (num_proc=32):  83%|████████▎ | 463073/558128 [02:03<00:14, 6700.18 examples/s]Map (num_proc=32):  83%|████████▎ | 464482/558128 [02:03<00:13, 7122.45 examples/s]Map (num_proc=32):  84%|████████▎ | 466074/558128 [02:03<00:11, 7788.01 examples/s]Map (num_proc=32):  84%|████████▍ | 467482/558128 [02:04<00:11, 8019.11 examples/s]Map (num_proc=32):  84%|████████▍ | 469067/558128 [02:04<00:10, 8575.61 examples/s]Map (num_proc=32):  84%|████████▍ | 470482/558128 [02:04<00:10, 8700.22 examples/s]Map (num_proc=32):  84%|████████▍ | 471519/558128 [02:06<00:56, 1546.09 examples/s]Map (num_proc=32):  85%|████████▍ | 472523/558128 [02:07<00:49, 1735.51 examples/s]Map (num_proc=32):  85%|████████▌ | 475923/558128 [02:07<00:23, 3465.88 examples/s]Map (num_proc=32):  86%|████████▌ | 477376/558128 [02:07<00:19, 4099.85 examples/s]Map (num_proc=32):  86%|████████▌ | 478513/558128 [02:07<00:16, 4689.69 examples/s]Map (num_proc=32):  86%|████████▌ | 479923/558128 [02:07<00:14, 5388.07 examples/s]Map (num_proc=32):  86%|████████▋ | 481488/558128 [02:07<00:12, 6245.42 examples/s]Map (num_proc=32):  86%|████████▋ | 482515/558128 [02:08<00:11, 6793.99 examples/s]Map (num_proc=32):  87%|████████▋ | 483519/558128 [02:08<00:10, 7224.25 examples/s]Map (num_proc=32):  87%|████████▋ | 484510/558128 [02:08<00:09, 7679.90 examples/s]Map (num_proc=32):  87%|████████▋ | 485923/558128 [02:08<00:09, 7789.74 examples/s]Map (num_proc=32):  87%|████████▋ | 486923/558128 [02:08<00:08, 8234.41 examples/s]Map (num_proc=32):  87%|████████▋ | 487923/558128 [02:08<00:08, 8535.45 examples/s]Map (num_proc=32):  88%|████████▊ | 488959/558128 [02:11<00:56, 1223.68 examples/s]Map (num_proc=32):  88%|████████▊ | 489967/558128 [02:11<00:41, 1623.55 examples/s]Map (num_proc=32):  88%|████████▊ | 490962/558128 [02:11<00:31, 2114.55 examples/s]Map (num_proc=32):  88%|████████▊ | 491957/558128 [02:11<00:24, 2730.13 examples/s]Map (num_proc=32):  88%|████████▊ | 492962/558128 [02:11<00:18, 3460.29 examples/s]Map (num_proc=32):  89%|████████▊ | 493958/558128 [02:11<00:15, 4267.60 examples/s]Map (num_proc=32):  89%|████████▉ | 495364/558128 [02:12<00:11, 5273.45 examples/s]Map (num_proc=32):  89%|████████▉ | 496364/558128 [02:12<00:10, 5995.58 examples/s]Map (num_proc=32):  89%|████████▉ | 497364/558128 [02:12<00:09, 6651.93 examples/s]Map (num_proc=32):  89%|████████▉ | 498364/558128 [02:12<00:08, 7306.15 examples/s]Map (num_proc=32):  89%|████████▉ | 499364/558128 [02:12<00:07, 7813.91 examples/s]Map (num_proc=32):  90%|████████▉ | 500364/558128 [02:12<00:07, 8148.83 examples/s]Map (num_proc=32):  90%|████████▉ | 501364/558128 [02:12<00:06, 8518.28 examples/s]Map (num_proc=32):  90%|█████████ | 502364/558128 [02:12<00:06, 8443.68 examples/s]Map (num_proc=32):  90%|█████████ | 503364/558128 [02:13<00:10, 5423.84 examples/s]Map (num_proc=32):  91%|█████████ | 506400/558128 [02:15<00:30, 1678.27 examples/s]Map (num_proc=32):  91%|█████████ | 507405/558128 [02:16<00:25, 2026.00 examples/s]Map (num_proc=32):  91%|█████████ | 508403/558128 [02:16<00:20, 2475.06 examples/s]Map (num_proc=32):  91%|█████████▏| 509398/558128 [02:16<00:16, 3033.95 examples/s]Map (num_proc=32):  91%|█████████▏| 510400/558128 [02:16<00:12, 3699.09 examples/s]Map (num_proc=32):  92%|█████████▏| 511408/558128 [02:16<00:10, 4428.73 examples/s]Map (num_proc=32):  92%|█████████▏| 512401/558128 [02:16<00:08, 5171.42 examples/s]Map (num_proc=32):  92%|█████████▏| 513392/558128 [02:16<00:09, 4804.63 examples/s]Map (num_proc=32):  92%|█████████▏| 515805/558128 [02:16<00:05, 7573.15 examples/s]Map (num_proc=32):  93%|█████████▎| 517400/558128 [02:17<00:04, 8242.58 examples/s]Map (num_proc=32):  93%|█████████▎| 518805/558128 [02:17<00:04, 8215.78 examples/s]Map (num_proc=32):  93%|█████████▎| 520399/558128 [02:17<00:04, 9007.04 examples/s]Map (num_proc=32):  93%|█████████▎| 521805/558128 [02:17<00:04, 8878.92 examples/s]Map (num_proc=32):  94%|█████████▎| 522805/558128 [02:17<00:03, 8889.19 examples/s]Map (num_proc=32):  94%|█████████▍| 523837/558128 [02:20<00:25, 1321.08 examples/s]Map (num_proc=32):  94%|█████████▍| 526246/558128 [02:20<00:13, 2310.55 examples/s]Map (num_proc=32):  95%|█████████▍| 527837/558128 [02:20<00:10, 3020.64 examples/s]Map (num_proc=32):  95%|█████████▍| 529246/558128 [02:20<00:07, 3676.85 examples/s]Map (num_proc=32):  95%|█████████▌| 530837/558128 [02:21<00:05, 4583.48 examples/s]Map (num_proc=32):  95%|█████████▌| 532246/558128 [02:21<00:04, 5285.78 examples/s]Map (num_proc=32):  96%|█████████▌| 533246/558128 [02:21<00:04, 5838.92 examples/s]Map (num_proc=32):  96%|█████████▌| 534246/558128 [02:21<00:03, 6420.40 examples/s]Map (num_proc=32):  96%|█████████▌| 535246/558128 [02:21<00:03, 6955.83 examples/s]Map (num_proc=32):  96%|█████████▌| 536246/558128 [02:21<00:02, 7565.73 examples/s]Map (num_proc=32):  96%|█████████▋| 537246/558128 [02:21<00:02, 8015.76 examples/s]Map (num_proc=32):  96%|█████████▋| 538246/558128 [02:21<00:02, 8148.22 examples/s]Map (num_proc=32):  97%|█████████▋| 539246/558128 [02:21<00:02, 8394.54 examples/s]Map (num_proc=32):  97%|█████████▋| 540246/558128 [02:22<00:02, 8693.41 examples/s]Map (num_proc=32):  97%|█████████▋| 541279/558128 [02:24<00:14, 1132.32 examples/s]Map (num_proc=32):  97%|█████████▋| 542288/558128 [02:24<00:10, 1528.69 examples/s]Map (num_proc=32):  97%|█████████▋| 543281/558128 [02:25<00:07, 2029.61 examples/s]Map (num_proc=32):  98%|█████████▊| 544280/558128 [02:25<00:05, 2643.72 examples/s]Map (num_proc=32):  98%|█████████▊| 545272/558128 [02:25<00:03, 3365.27 examples/s]Map (num_proc=32):  98%|█████████▊| 546235/558128 [02:25<00:02, 4129.24 examples/s]Map (num_proc=32):  98%|█████████▊| 547281/558128 [02:25<00:02, 5016.06 examples/s]Map (num_proc=32):  98%|█████████▊| 548257/558128 [02:25<00:01, 5809.44 examples/s]Map (num_proc=32):  98%|█████████▊| 549281/558128 [02:25<00:01, 6624.21 examples/s]Map (num_proc=32):  99%|█████████▊| 550283/558128 [02:25<00:01, 7165.09 examples/s]Map (num_proc=32):  99%|█████████▉| 551282/558128 [02:25<00:00, 7751.15 examples/s]Map (num_proc=32):  99%|█████████▉| 552277/558128 [02:26<00:00, 8211.41 examples/s]Map (num_proc=32):  99%|█████████▉| 553257/558128 [02:26<00:00, 8431.16 examples/s]Map (num_proc=32):  99%|█████████▉| 554282/558128 [02:26<00:00, 8764.45 examples/s]Map (num_proc=32):  99%|█████████▉| 555279/558128 [02:26<00:00, 8970.89 examples/s]Map (num_proc=32): 100%|█████████▉| 556269/558128 [02:26<00:00, 9105.88 examples/s]Map (num_proc=32): 100%|█████████▉| 557274/558128 [02:26<00:00, 9224.29 examples/s]Map (num_proc=32): 100%|██████████| 558128/558128 [02:27<00:00, 3786.74 examples/s]
06/30 11:26:31 - mmengine - WARNING - Dataset LLaVADataset has no metainfo. ``dataset_meta`` in visualizer will be None.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.70s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.00s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.10s/it]
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.57s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.65s/it]06/30 11:26:36 - mmengine - INFO - Dispatch Phi3FlashAttention2 forward. Due to the implementation of the PyTorch version of flash attention, even when the `output_attentions` flag is set to True, it is not possible to return the `attn_weights`.
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.67s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.51s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.58s/it]You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.62s/it]Loading checkpoint shards:  50%|█████     | 1/2 [00:02<00:02,  2.60s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.86s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.97s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.89s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.00s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.82s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.93s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  1.90s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:04<00:00,  2.01s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.98s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.99s/it]
Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.87s/it]Loading checkpoint shards: 100%|██████████| 2/2 [00:03<00:00,  1.98s/it]
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-30 11:26:38,801] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-30 11:26:48,154] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-06-30 11:26:48,157] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-06-30 11:26:48,157] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-06-30 11:26:48,215] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-06-30 11:26:48,215] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-06-30 11:26:48,216] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-06-30 11:26:48,216] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 500,000,000
[2024-06-30 11:26:48,216] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500,000,000
[2024-06-30 11:26:48,216] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-06-30 11:26:48,216] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2024-06-30 11:26:48,749] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-06-30 11:26:48,750] [INFO] [utils.py:782:see_memory_usage] MA 7.71 GB         Max_MA 7.72 GB         CA 7.72 GB         Max_CA 8 GB 
[2024-06-30 11:26:48,750] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.84 GB, percent = 4.9%
[2024-06-30 11:26:48,917] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-06-30 11:26:48,918] [INFO] [utils.py:782:see_memory_usage] MA 7.71 GB         Max_MA 7.72 GB         CA 7.74 GB         Max_CA 8 GB 
[2024-06-30 11:26:48,918] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.85 GB, percent = 4.9%
[2024-06-30 11:26:48,918] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-06-30 11:26:49,071] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-06-30 11:26:49,072] [INFO] [utils.py:782:see_memory_usage] MA 7.71 GB         Max_MA 7.71 GB         CA 7.74 GB         Max_CA 8 GB 
[2024-06-30 11:26:49,072] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.87 GB, percent = 4.9%
[2024-06-30 11:26:49,072] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-06-30 11:26:49,073] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-06-30 11:26:49,073] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-06-30 11:26:49,073] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[(0.9, 0.999)]
[2024-06-30 11:26:49,074] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-06-30 11:26:49,074] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-06-30 11:26:49,074] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-06-30 11:26:49,074] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   bfloat16_enabled ............. True
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5f080be380>
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   fp16_auto_cast ............... None
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   fp16_enabled ................. False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 1
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   gradient_clipping ............ 1
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 1
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   loss_scale ................... 1.0
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   steps_per_print .............. 10000000000000
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   train_batch_size ............. 256
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  32
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   world_size ................... 8
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. False
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 2
[2024-06-30 11:26:49,161] [INFO] [config.py:987:print_user_config]   json = {
    "gradient_accumulation_steps": 1, 
    "train_micro_batch_size_per_gpu": 32, 
    "gradient_clipping": 1, 
    "zero_allow_untested_optimizer": true, 
    "zero_force_ds_cpu_optimizer": false, 
    "zero_optimization": {
        "stage": 2, 
        "overlap_comm": true
    }, 
    "fp16": {
        "enabled": false, 
        "initial_scale_power": 16
    }, 
    "bf16": {
        "enabled": true
    }, 
    "steps_per_print": 1.000000e+13
}
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 11:26:50 - mmengine - INFO - Num train samples 558128
06/30 11:26:50 - mmengine - INFO - train example:
06/30 11:26:50 - mmengine - INFO - <s><|user|><image>
Render a clear and concise summary of the photo.<|end|><|assistant|> select luxury furniture 3 - inch gel memory foam mattress topper<|end|> 

06/30 11:26:50 - mmengine - INFO - before_train in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 11:26:53 - mmengine - INFO - Sample output:
<|user|>
<image>
请描述一下这张照片<|end|>
<|assistant|>
下，这个人的意思，这个人的意思，这个人的意思，这个人的意思，这个人的意思，这个人的意思，这个人的意思

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 11:26:54 - mmengine - INFO - Sample output:
<|user|>
<image>
Please describe this picture<|end|>
<|assistant|>
The year was 1999, and the world was a bustling place. The economy was booming, and the stock market was soaring. The world was a place of opportunity and prosperity. But amidst all this prosper

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 11:26:54 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
06/30 11:26:54 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
06/30 11:26:54 - mmengine - INFO - Checkpoints will be saved to /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/30 11:27:45 - mmengine - INFO - Iter(train) [  10/2181]  lr: 1.4063e-04  eta: 3:05:25  time: 5.1248  data_time: 0.0035  memory: 22328  loss: 5.4189
06/30 11:28:31 - mmengine - INFO - Iter(train) [  20/2181]  lr: 2.9688e-04  eta: 2:54:18  time: 4.5549  data_time: 0.0036  memory: 22328  loss: 4.1209
06/30 11:29:14 - mmengine - INFO - Iter(train) [  30/2181]  lr: 4.5313e-04  eta: 2:47:14  time: 4.3152  data_time: 0.0035  memory: 22383  loss: 3.6412
06/30 11:29:55 - mmengine - INFO - Iter(train) [  40/2181]  lr: 6.0938e-04  eta: 2:41:21  time: 4.0932  data_time: 0.0034  memory: 22217  loss: 3.5165
06/30 11:30:35 - mmengine - INFO - Iter(train) [  50/2181]  lr: 7.6563e-04  eta: 2:36:44  time: 3.9775  data_time: 0.0034  memory: 22290  loss: 3.2703
06/30 11:31:12 - mmengine - INFO - Iter(train) [  60/2181]  lr: 9.2188e-04  eta: 2:32:00  time: 3.7364  data_time: 0.0034  memory: 22290  loss: 3.2619
06/30 11:31:53 - mmengine - INFO - Iter(train) [  70/2181]  lr: 9.9999e-04  eta: 2:30:10  time: 4.0752  data_time: 0.0035  memory: 22290  loss: 3.2020
06/30 11:32:30 - mmengine - INFO - Iter(train) [  80/2181]  lr: 9.9989e-04  eta: 2:27:14  time: 3.7634  data_time: 0.0034  memory: 22290  loss: 3.0848
06/30 11:33:08 - mmengine - INFO - Iter(train) [  90/2181]  lr: 9.9968e-04  eta: 2:24:45  time: 3.7427  data_time: 0.0034  memory: 22364  loss: 3.0193
06/30 11:33:45 - mmengine - INFO - Iter(train) [ 100/2181]  lr: 9.9936e-04  eta: 2:22:42  time: 3.7642  data_time: 0.0034  memory: 22328  loss: 2.9876
06/30 11:34:25 - mmengine - INFO - Iter(train) [ 110/2181]  lr: 9.9893e-04  eta: 2:21:24  time: 3.9176  data_time: 0.0034  memory: 22420  loss: 2.8497
06/30 11:35:02 - mmengine - INFO - Iter(train) [ 120/2181]  lr: 9.9839e-04  eta: 2:19:44  time: 3.7506  data_time: 0.0034  memory: 22290  loss: 2.8580
06/30 11:35:42 - mmengine - INFO - Iter(train) [ 130/2181]  lr: 9.9774e-04  eta: 2:18:54  time: 4.0134  data_time: 0.0034  memory: 22290  loss: 2.7027
06/30 11:36:22 - mmengine - INFO - Iter(train) [ 140/2181]  lr: 9.9699e-04  eta: 2:17:56  time: 3.9404  data_time: 0.0033  memory: 22346  loss: 2.6753
06/30 11:37:01 - mmengine - INFO - Iter(train) [ 150/2181]  lr: 9.9612e-04  eta: 2:16:54  time: 3.8962  data_time: 0.0034  memory: 22364  loss: 2.6431
06/30 11:37:38 - mmengine - INFO - Iter(train) [ 160/2181]  lr: 9.9514e-04  eta: 2:15:39  time: 3.7721  data_time: 0.0033  memory: 22346  loss: 2.5772
06/30 11:38:18 - mmengine - INFO - Iter(train) [ 170/2181]  lr: 9.9405e-04  eta: 2:14:50  time: 3.9586  data_time: 0.0034  memory: 22309  loss: 2.4770
06/30 11:38:56 - mmengine - INFO - Iter(train) [ 180/2181]  lr: 9.9286e-04  eta: 2:13:46  time: 3.8061  data_time: 0.0034  memory: 22309  loss: 2.5037
06/30 11:39:33 - mmengine - INFO - Iter(train) [ 190/2181]  lr: 9.9155e-04  eta: 2:12:38  time: 3.7439  data_time: 0.0034  memory: 22290  loss: 2.5734
06/30 11:40:11 - mmengine - INFO - Iter(train) [ 200/2181]  lr: 9.9014e-04  eta: 2:11:31  time: 3.7252  data_time: 0.0033  memory: 22309  loss: 2.4938
06/30 11:40:48 - mmengine - INFO - Iter(train) [ 210/2181]  lr: 9.8862e-04  eta: 2:10:28  time: 3.7339  data_time: 0.0033  memory: 22328  loss: 2.5438
06/30 11:41:25 - mmengine - INFO - Iter(train) [ 220/2181]  lr: 9.8699e-04  eta: 2:09:27  time: 3.7310  data_time: 0.0033  memory: 22346  loss: 2.4178
06/30 11:42:03 - mmengine - INFO - Iter(train) [ 230/2181]  lr: 9.8525e-04  eta: 2:08:28  time: 3.7328  data_time: 0.0033  memory: 22217  loss: 2.5027
06/30 11:42:40 - mmengine - INFO - Iter(train) [ 240/2181]  lr: 9.8341e-04  eta: 2:07:30  time: 3.7246  data_time: 0.0033  memory: 22328  loss: 2.4832
06/30 11:43:17 - mmengine - INFO - Iter(train) [ 250/2181]  lr: 9.8146e-04  eta: 2:06:34  time: 3.7244  data_time: 0.0034  memory: 22272  loss: 2.3665
06/30 11:43:56 - mmengine - INFO - Iter(train) [ 260/2181]  lr: 9.7940e-04  eta: 2:05:47  time: 3.8384  data_time: 0.0033  memory: 22290  loss: 2.5354
06/30 11:44:33 - mmengine - INFO - Iter(train) [ 270/2181]  lr: 9.7724e-04  eta: 2:04:54  time: 3.7332  data_time: 0.0033  memory: 22346  loss: 2.5168
06/30 11:45:10 - mmengine - INFO - Iter(train) [ 280/2181]  lr: 9.7497e-04  eta: 2:04:02  time: 3.7325  data_time: 0.0034  memory: 22254  loss: 2.4012
06/30 11:45:48 - mmengine - INFO - Iter(train) [ 290/2181]  lr: 9.7260e-04  eta: 2:03:12  time: 3.7405  data_time: 0.0034  memory: 22272  loss: 2.4464
06/30 11:46:25 - mmengine - INFO - Iter(train) [ 300/2181]  lr: 9.7013e-04  eta: 2:02:23  time: 3.7616  data_time: 0.0034  memory: 22236  loss: 2.4710
06/30 11:47:05 - mmengine - INFO - Iter(train) [ 310/2181]  lr: 9.6755e-04  eta: 2:01:49  time: 3.9817  data_time: 0.0034  memory: 22328  loss: 2.4591
06/30 11:47:43 - mmengine - INFO - Iter(train) [ 320/2181]  lr: 9.6487e-04  eta: 2:01:01  time: 3.7549  data_time: 0.0033  memory: 22364  loss: 2.3807
06/30 11:48:20 - mmengine - INFO - Iter(train) [ 330/2181]  lr: 9.6208e-04  eta: 2:00:14  time: 3.7561  data_time: 0.0034  memory: 22272  loss: 2.4158
06/30 11:48:58 - mmengine - INFO - Iter(train) [ 340/2181]  lr: 9.5920e-04  eta: 1:59:27  time: 3.7555  data_time: 0.0034  memory: 22309  loss: 2.3892
06/30 11:49:35 - mmengine - INFO - Iter(train) [ 350/2181]  lr: 9.5621e-04  eta: 1:58:41  time: 3.7550  data_time: 0.0034  memory: 22420  loss: 2.3951
06/30 11:50:13 - mmengine - INFO - Iter(train) [ 360/2181]  lr: 9.5312e-04  eta: 1:57:54  time: 3.7391  data_time: 0.0034  memory: 22236  loss: 2.4522
06/30 11:50:50 - mmengine - INFO - Iter(train) [ 370/2181]  lr: 9.4993e-04  eta: 1:57:09  time: 3.7449  data_time: 0.0034  memory: 22346  loss: 2.4308
06/30 11:51:29 - mmengine - INFO - Iter(train) [ 380/2181]  lr: 9.4664e-04  eta: 1:56:28  time: 3.8489  data_time: 0.0035  memory: 22272  loss: 2.4329
06/30 11:52:08 - mmengine - INFO - Iter(train) [ 390/2181]  lr: 9.4326e-04  eta: 1:55:54  time: 3.9707  data_time: 0.0034  memory: 22272  loss: 2.4149
06/30 11:52:46 - mmengine - INFO - Iter(train) [ 400/2181]  lr: 9.3977e-04  eta: 1:55:08  time: 3.7315  data_time: 0.0034  memory: 22290  loss: 2.3032
06/30 11:53:23 - mmengine - INFO - Iter(train) [ 410/2181]  lr: 9.3619e-04  eta: 1:54:23  time: 3.7381  data_time: 0.0034  memory: 22328  loss: 2.4403
06/30 11:54:03 - mmengine - INFO - Iter(train) [ 420/2181]  lr: 9.3252e-04  eta: 1:53:48  time: 3.9668  data_time: 0.0034  memory: 22290  loss: 2.4526
06/30 11:54:40 - mmengine - INFO - Iter(train) [ 430/2181]  lr: 9.2874e-04  eta: 1:53:04  time: 3.7314  data_time: 0.0034  memory: 22254  loss: 2.4691
06/30 11:55:17 - mmengine - INFO - Iter(train) [ 440/2181]  lr: 9.2488e-04  eta: 1:52:19  time: 3.7248  data_time: 0.0034  memory: 22236  loss: 2.3144
06/30 11:55:55 - mmengine - INFO - Iter(train) [ 450/2181]  lr: 9.2092e-04  eta: 1:51:35  time: 3.7288  data_time: 0.0034  memory: 22309  loss: 2.3953
06/30 11:56:32 - mmengine - INFO - Iter(train) [ 460/2181]  lr: 9.1687e-04  eta: 1:50:51  time: 3.7414  data_time: 0.0033  memory: 22272  loss: 2.3094
06/30 11:57:09 - mmengine - INFO - Iter(train) [ 470/2181]  lr: 9.1272e-04  eta: 1:50:08  time: 3.7337  data_time: 0.0034  memory: 22364  loss: 2.3849
06/30 11:57:47 - mmengine - INFO - Iter(train) [ 480/2181]  lr: 9.0848e-04  eta: 1:49:25  time: 3.7362  data_time: 0.0034  memory: 22290  loss: 2.3559
06/30 11:58:26 - mmengine - INFO - Iter(train) [ 490/2181]  lr: 9.0416e-04  eta: 1:48:48  time: 3.9165  data_time: 0.0034  memory: 22457  loss: 2.3983
06/30 11:59:03 - mmengine - INFO - Iter(train) [ 500/2181]  lr: 8.9974e-04  eta: 1:48:05  time: 3.7370  data_time: 0.0034  memory: 22272  loss: 2.3084
06/30 11:59:03 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 11:59:04 - mmengine - INFO - Sample output:
<|user|>
<image>
请描述一下这张照片<|end|>
<|assistant|>
a wooden boardwalk over a lake with mountains in the background<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 11:59:04 - mmengine - INFO - Sample output:
<|user|>
<image>
Please describe this picture<|end|>
<|assistant|>
a wooden boardwalk overlooking a lake in the mountains<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 11:59:04 - mmengine - INFO - Saving checkpoint at 500 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-30 11:59:05,710] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_500.pth is about to be saved!
[2024-06-30 11:59:05,721] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/mp_rank_00_model_states.pt
[2024-06-30 11:59:05,722] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/mp_rank_00_model_states.pt...
[2024-06-30 11:59:05,749] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/mp_rank_00_model_states.pt.
[2024-06-30 11:59:05,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-30 11:59:05,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-30 11:59:05,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-30 11:59:05,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-30 11:59:05,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-30 11:59:05,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-30 11:59:05,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-30 11:59:05,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-30 11:59:05,766] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-30 11:59:05,766] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-30 11:59:05,766] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-30 11:59:05,766] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-30 11:59:05,766] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-30 11:59:05,766] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-30 11:59:05,767] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-30 11:59:05,767] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-30 11:59:05,767] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-30 11:59:05,767] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-30 11:59:05,767] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-30 11:59:05,768] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-30 11:59:05,769] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-30 11:59:05,769] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/30 11:59:43 - mmengine - INFO - Iter(train) [ 510/2181]  lr: 8.9524e-04  eta: 1:47:30  time: 3.9500  data_time: 0.2142  memory: 22401  loss: 2.3000
06/30 12:00:19 - mmengine - INFO - Iter(train) [ 520/2181]  lr: 8.9065e-04  eta: 1:46:45  time: 3.6643  data_time: 0.0034  memory: 22309  loss: 2.2710
06/30 12:00:57 - mmengine - INFO - Iter(train) [ 530/2181]  lr: 8.8597e-04  eta: 1:46:03  time: 3.7600  data_time: 0.0035  memory: 22290  loss: 2.3523
06/30 12:01:35 - mmengine - INFO - Iter(train) [ 540/2181]  lr: 8.8121e-04  eta: 1:45:24  time: 3.8259  data_time: 0.0035  memory: 22254  loss: 2.3184
06/30 12:02:13 - mmengine - INFO - Iter(train) [ 550/2181]  lr: 8.7637e-04  eta: 1:44:43  time: 3.7801  data_time: 0.0034  memory: 22309  loss: 2.3358
06/30 12:02:52 - mmengine - INFO - Iter(train) [ 560/2181]  lr: 8.7144e-04  eta: 1:44:06  time: 3.8837  data_time: 0.0034  memory: 22290  loss: 2.3142
06/30 12:03:33 - mmengine - INFO - Iter(train) [ 570/2181]  lr: 8.6643e-04  eta: 1:43:34  time: 4.0929  data_time: 0.0035  memory: 22767  loss: 2.3243
06/30 12:04:10 - mmengine - INFO - Iter(train) [ 580/2181]  lr: 8.6134e-04  eta: 1:42:52  time: 3.7409  data_time: 0.0034  memory: 22383  loss: 2.3293
06/30 12:04:48 - mmengine - INFO - Iter(train) [ 590/2181]  lr: 8.5617e-04  eta: 1:42:10  time: 3.7368  data_time: 0.0035  memory: 22346  loss: 2.3013
06/30 12:05:25 - mmengine - INFO - Iter(train) [ 600/2181]  lr: 8.5092e-04  eta: 1:41:29  time: 3.7334  data_time: 0.0034  memory: 22309  loss: 2.4125
06/30 12:06:02 - mmengine - INFO - Iter(train) [ 610/2181]  lr: 8.4559e-04  eta: 1:40:47  time: 3.7222  data_time: 0.0034  memory: 22328  loss: 2.3031
06/30 12:06:39 - mmengine - INFO - Iter(train) [ 620/2181]  lr: 8.4019e-04  eta: 1:40:05  time: 3.7310  data_time: 0.0034  memory: 22254  loss: 2.2368
06/30 12:07:17 - mmengine - INFO - Iter(train) [ 630/2181]  lr: 8.3471e-04  eta: 1:39:24  time: 3.7331  data_time: 0.0035  memory: 22217  loss: 2.3121
06/30 12:07:54 - mmengine - INFO - Iter(train) [ 640/2181]  lr: 8.2916e-04  eta: 1:38:43  time: 3.7288  data_time: 0.0034  memory: 22254  loss: 2.2650
06/30 12:08:31 - mmengine - INFO - Iter(train) [ 650/2181]  lr: 8.2354e-04  eta: 1:38:01  time: 3.7251  data_time: 0.0034  memory: 22217  loss: 2.3090
06/30 12:09:09 - mmengine - INFO - Iter(train) [ 660/2181]  lr: 8.1784e-04  eta: 1:37:21  time: 3.7350  data_time: 0.0034  memory: 22383  loss: 2.3298
06/30 12:09:47 - mmengine - INFO - Iter(train) [ 670/2181]  lr: 8.1208e-04  eta: 1:36:42  time: 3.8246  data_time: 0.0034  memory: 22309  loss: 2.3201
06/30 12:10:24 - mmengine - INFO - Iter(train) [ 680/2181]  lr: 8.0624e-04  eta: 1:36:01  time: 3.7269  data_time: 0.0034  memory: 22272  loss: 2.2888
06/30 12:11:01 - mmengine - INFO - Iter(train) [ 690/2181]  lr: 8.0034e-04  eta: 1:35:20  time: 3.7288  data_time: 0.0034  memory: 22328  loss: 2.3736
06/30 12:11:39 - mmengine - INFO - Iter(train) [ 700/2181]  lr: 7.9437e-04  eta: 1:34:40  time: 3.7454  data_time: 0.0034  memory: 22290  loss: 2.3332
06/30 12:12:16 - mmengine - INFO - Iter(train) [ 710/2181]  lr: 7.8834e-04  eta: 1:33:59  time: 3.7232  data_time: 0.0034  memory: 22290  loss: 2.3028
06/30 12:12:53 - mmengine - INFO - Iter(train) [ 720/2181]  lr: 7.8224e-04  eta: 1:33:19  time: 3.7372  data_time: 0.0034  memory: 22254  loss: 2.2757
06/30 12:13:31 - mmengine - INFO - Iter(train) [ 730/2181]  lr: 7.7609e-04  eta: 1:32:39  time: 3.7437  data_time: 0.0034  memory: 22346  loss: 2.3211
06/30 12:14:08 - mmengine - INFO - Iter(train) [ 740/2181]  lr: 7.6987e-04  eta: 1:31:59  time: 3.7440  data_time: 0.0034  memory: 22290  loss: 2.2881
06/30 12:14:46 - mmengine - INFO - Iter(train) [ 750/2181]  lr: 7.6359e-04  eta: 1:31:19  time: 3.7494  data_time: 0.0034  memory: 22272  loss: 2.2319
06/30 12:15:23 - mmengine - INFO - Iter(train) [ 760/2181]  lr: 7.5725e-04  eta: 1:30:39  time: 3.7534  data_time: 0.0035  memory: 22309  loss: 2.2759
06/30 12:16:01 - mmengine - INFO - Iter(train) [ 770/2181]  lr: 7.5086e-04  eta: 1:30:00  time: 3.7732  data_time: 0.0034  memory: 22328  loss: 2.3518
06/30 12:16:39 - mmengine - INFO - Iter(train) [ 780/2181]  lr: 7.4441e-04  eta: 1:29:21  time: 3.7636  data_time: 0.0034  memory: 22346  loss: 2.3366
06/30 12:17:16 - mmengine - INFO - Iter(train) [ 790/2181]  lr: 7.3790e-04  eta: 1:28:41  time: 3.7574  data_time: 0.0035  memory: 22290  loss: 2.2962
06/30 12:17:54 - mmengine - INFO - Iter(train) [ 800/2181]  lr: 7.3135e-04  eta: 1:28:02  time: 3.7504  data_time: 0.0034  memory: 22309  loss: 2.2350
06/30 12:18:31 - mmengine - INFO - Iter(train) [ 810/2181]  lr: 7.2474e-04  eta: 1:27:22  time: 3.7300  data_time: 0.0034  memory: 22290  loss: 2.2661
06/30 12:19:08 - mmengine - INFO - Iter(train) [ 820/2181]  lr: 7.1809e-04  eta: 1:26:42  time: 3.7300  data_time: 0.0034  memory: 22181  loss: 2.3263
06/30 12:19:46 - mmengine - INFO - Iter(train) [ 830/2181]  lr: 7.1138e-04  eta: 1:26:02  time: 3.7281  data_time: 0.0034  memory: 22401  loss: 2.2632
06/30 12:20:23 - mmengine - INFO - Iter(train) [ 840/2181]  lr: 7.0463e-04  eta: 1:25:23  time: 3.7344  data_time: 0.0034  memory: 22474  loss: 2.2490
06/30 12:21:00 - mmengine - INFO - Iter(train) [ 850/2181]  lr: 6.9784e-04  eta: 1:24:43  time: 3.7415  data_time: 0.0034  memory: 22346  loss: 2.2583
06/30 12:21:38 - mmengine - INFO - Iter(train) [ 860/2181]  lr: 6.9100e-04  eta: 1:24:04  time: 3.7518  data_time: 0.0034  memory: 22364  loss: 2.2959
06/30 12:22:15 - mmengine - INFO - Iter(train) [ 870/2181]  lr: 6.8412e-04  eta: 1:23:24  time: 3.7336  data_time: 0.0034  memory: 22420  loss: 2.2623
06/30 12:22:53 - mmengine - INFO - Iter(train) [ 880/2181]  lr: 6.7720e-04  eta: 1:22:45  time: 3.7306  data_time: 0.0034  memory: 22383  loss: 2.2306
06/30 12:23:30 - mmengine - INFO - Iter(train) [ 890/2181]  lr: 6.7023e-04  eta: 1:22:06  time: 3.7372  data_time: 0.0034  memory: 22272  loss: 2.3479
06/30 12:24:07 - mmengine - INFO - Iter(train) [ 900/2181]  lr: 6.6324e-04  eta: 1:21:26  time: 3.7393  data_time: 0.0034  memory: 22383  loss: 2.2349
06/30 12:24:45 - mmengine - INFO - Iter(train) [ 910/2181]  lr: 6.5620e-04  eta: 1:20:47  time: 3.7296  data_time: 0.0034  memory: 22420  loss: 2.1865
06/30 12:25:22 - mmengine - INFO - Iter(train) [ 920/2181]  lr: 6.4913e-04  eta: 1:20:08  time: 3.7381  data_time: 0.0034  memory: 22346  loss: 2.3290
06/30 12:26:00 - mmengine - INFO - Iter(train) [ 930/2181]  lr: 6.4203e-04  eta: 1:19:29  time: 3.7463  data_time: 0.0034  memory: 22272  loss: 2.2768
06/30 12:26:37 - mmengine - INFO - Iter(train) [ 940/2181]  lr: 6.3490e-04  eta: 1:18:50  time: 3.7454  data_time: 0.0034  memory: 22364  loss: 2.2136
06/30 12:27:14 - mmengine - INFO - Iter(train) [ 950/2181]  lr: 6.2774e-04  eta: 1:18:11  time: 3.7406  data_time: 0.0035  memory: 22290  loss: 2.2745
06/30 12:27:52 - mmengine - INFO - Iter(train) [ 960/2181]  lr: 6.2054e-04  eta: 1:17:32  time: 3.7427  data_time: 0.0034  memory: 22199  loss: 2.3281
06/30 12:28:29 - mmengine - INFO - Iter(train) [ 970/2181]  lr: 6.1333e-04  eta: 1:16:53  time: 3.7526  data_time: 0.0034  memory: 22364  loss: 2.2899
06/30 12:29:10 - mmengine - INFO - Iter(train) [ 980/2181]  lr: 6.0608e-04  eta: 1:16:18  time: 4.0323  data_time: 0.0034  memory: 22309  loss: 2.3621
06/30 12:29:47 - mmengine - INFO - Iter(train) [ 990/2181]  lr: 5.9882e-04  eta: 1:15:39  time: 3.7500  data_time: 0.0034  memory: 22328  loss: 2.2988
06/30 12:30:25 - mmengine - INFO - Exp name: llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy_20240630_112119
06/30 12:30:25 - mmengine - INFO - Iter(train) [1000/2181]  lr: 5.9153e-04  eta: 1:15:00  time: 3.7572  data_time: 0.0034  memory: 22346  loss: 2.3662
06/30 12:30:25 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 12:30:25 - mmengine - INFO - Sample output:
<|user|>
<image>
请描述一下这张照片<|end|>
<|assistant|>
a wooden dock with a boat on it<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 12:30:25 - mmengine - INFO - Sample output:
<|user|>
<image>
Please describe this picture<|end|>
<|assistant|>
a wooden dock with a boat on it in the water<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 12:30:25 - mmengine - INFO - Saving checkpoint at 1000 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-30 12:30:26,969] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_1000.pth is about to be saved!
[2024-06-30 12:30:26,979] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/mp_rank_00_model_states.pt
[2024-06-30 12:30:26,980] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/mp_rank_00_model_states.pt...
[2024-06-30 12:30:27,007] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/mp_rank_00_model_states.pt.
[2024-06-30 12:30:27,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-30 12:30:27,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-30 12:30:27,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-30 12:30:27,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-30 12:30:27,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-30 12:30:27,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-30 12:30:27,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-30 12:30:27,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-30 12:30:27,024] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-30 12:30:27,024] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-30 12:30:27,024] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-30 12:30:27,024] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-30 12:30:27,024] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-30 12:30:27,024] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-30 12:30:27,024] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-30 12:30:27,025] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-30 12:30:27,024] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-30 12:30:27,025] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-30 12:30:27,025] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-30 12:30:27,025] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-30 12:30:27,025] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-30 12:30:27,025] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-30 12:30:27,025] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-30 12:30:27,025] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-30 12:30:27,025] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-30 12:30:27,025] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-30 12:30:27,025] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-30 12:30:27,025] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-30 12:30:27,025] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-30 12:30:27,027] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-30 12:30:27,028] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-30 12:30:27,028] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/30 12:31:04 - mmengine - INFO - Iter(train) [1010/2181]  lr: 5.8422e-04  eta: 1:14:23  time: 3.9344  data_time: 0.1841  memory: 22328  loss: 2.3502
06/30 12:31:42 - mmengine - INFO - Iter(train) [1020/2181]  lr: 5.7690e-04  eta: 1:13:44  time: 3.7451  data_time: 0.0034  memory: 22290  loss: 2.2754
06/30 12:32:20 - mmengine - INFO - Iter(train) [1030/2181]  lr: 5.6955e-04  eta: 1:13:07  time: 3.8719  data_time: 0.0034  memory: 22346  loss: 2.3220
06/30 12:32:58 - mmengine - INFO - Iter(train) [1040/2181]  lr: 5.6220e-04  eta: 1:12:28  time: 3.7347  data_time: 0.0034  memory: 22217  loss: 2.2762
06/30 12:33:35 - mmengine - INFO - Iter(train) [1050/2181]  lr: 5.5482e-04  eta: 1:11:49  time: 3.7464  data_time: 0.0033  memory: 22290  loss: 2.2137
06/30 12:34:12 - mmengine - INFO - Iter(train) [1060/2181]  lr: 5.4744e-04  eta: 1:11:10  time: 3.7346  data_time: 0.0034  memory: 22328  loss: 2.2861
06/30 12:34:50 - mmengine - INFO - Iter(train) [1070/2181]  lr: 5.4004e-04  eta: 1:10:31  time: 3.7325  data_time: 0.0034  memory: 22309  loss: 2.1892
06/30 12:35:27 - mmengine - INFO - Iter(train) [1080/2181]  lr: 5.3264e-04  eta: 1:09:53  time: 3.7326  data_time: 0.0034  memory: 22309  loss: 2.2229
06/30 12:36:04 - mmengine - INFO - Iter(train) [1090/2181]  lr: 5.2523e-04  eta: 1:09:14  time: 3.7245  data_time: 0.0034  memory: 22328  loss: 2.2364
06/30 12:36:42 - mmengine - INFO - Iter(train) [1100/2181]  lr: 5.1781e-04  eta: 1:08:35  time: 3.7281  data_time: 0.0034  memory: 22309  loss: 2.2602
06/30 12:37:19 - mmengine - INFO - Iter(train) [1110/2181]  lr: 5.1039e-04  eta: 1:07:56  time: 3.7292  data_time: 0.0034  memory: 22290  loss: 2.2704
06/30 12:37:56 - mmengine - INFO - Iter(train) [1120/2181]  lr: 5.0297e-04  eta: 1:07:17  time: 3.7339  data_time: 0.0034  memory: 22492  loss: 2.2377
06/30 12:38:33 - mmengine - INFO - Iter(train) [1130/2181]  lr: 4.9555e-04  eta: 1:06:38  time: 3.7289  data_time: 0.0034  memory: 22217  loss: 2.2733
06/30 12:39:11 - mmengine - INFO - Iter(train) [1140/2181]  lr: 4.8812e-04  eta: 1:06:00  time: 3.7350  data_time: 0.0034  memory: 22290  loss: 2.2478
06/30 12:39:48 - mmengine - INFO - Iter(train) [1150/2181]  lr: 4.8070e-04  eta: 1:05:21  time: 3.7376  data_time: 0.0034  memory: 22328  loss: 2.3062
06/30 12:40:26 - mmengine - INFO - Iter(train) [1160/2181]  lr: 4.7329e-04  eta: 1:04:42  time: 3.7398  data_time: 0.0034  memory: 22328  loss: 2.1635
06/30 12:41:03 - mmengine - INFO - Iter(train) [1170/2181]  lr: 4.6588e-04  eta: 1:04:04  time: 3.7547  data_time: 0.0034  memory: 22364  loss: 2.2957
06/30 12:41:41 - mmengine - INFO - Iter(train) [1180/2181]  lr: 4.5848e-04  eta: 1:03:26  time: 3.7631  data_time: 0.0034  memory: 22236  loss: 2.2871
06/30 12:42:18 - mmengine - INFO - Iter(train) [1190/2181]  lr: 4.5108e-04  eta: 1:02:47  time: 3.7636  data_time: 0.0034  memory: 22272  loss: 2.2905
06/30 12:42:56 - mmengine - INFO - Iter(train) [1200/2181]  lr: 4.4370e-04  eta: 1:02:09  time: 3.7543  data_time: 0.0034  memory: 22401  loss: 2.2898
06/30 12:43:34 - mmengine - INFO - Iter(train) [1210/2181]  lr: 4.3633e-04  eta: 1:01:31  time: 3.7588  data_time: 0.0034  memory: 22236  loss: 2.2326
06/30 12:44:11 - mmengine - INFO - Iter(train) [1220/2181]  lr: 4.2898e-04  eta: 1:00:52  time: 3.7595  data_time: 0.0033  memory: 22328  loss: 2.2563
06/30 12:44:51 - mmengine - INFO - Iter(train) [1230/2181]  lr: 4.2164e-04  eta: 1:00:15  time: 3.9410  data_time: 0.0034  memory: 22290  loss: 2.3032
06/30 12:45:28 - mmengine - INFO - Iter(train) [1240/2181]  lr: 4.1431e-04  eta: 0:59:37  time: 3.7659  data_time: 0.0034  memory: 22364  loss: 2.2466
06/30 12:46:06 - mmengine - INFO - Iter(train) [1250/2181]  lr: 4.0701e-04  eta: 0:58:58  time: 3.7366  data_time: 0.0033  memory: 22217  loss: 2.2488
06/30 12:46:43 - mmengine - INFO - Iter(train) [1260/2181]  lr: 3.9973e-04  eta: 0:58:20  time: 3.7443  data_time: 0.0034  memory: 22364  loss: 2.2601
06/30 12:47:20 - mmengine - INFO - Iter(train) [1270/2181]  lr: 3.9246e-04  eta: 0:57:42  time: 3.7404  data_time: 0.0034  memory: 22364  loss: 2.2467
06/30 12:47:58 - mmengine - INFO - Iter(train) [1280/2181]  lr: 3.8523e-04  eta: 0:57:03  time: 3.7411  data_time: 0.0033  memory: 22309  loss: 2.1962
06/30 12:48:35 - mmengine - INFO - Iter(train) [1290/2181]  lr: 3.7801e-04  eta: 0:56:25  time: 3.7263  data_time: 0.0034  memory: 22236  loss: 2.2576
06/30 12:49:12 - mmengine - INFO - Iter(train) [1300/2181]  lr: 3.7083e-04  eta: 0:55:46  time: 3.7226  data_time: 0.0034  memory: 22401  loss: 2.1759
06/30 12:49:50 - mmengine - INFO - Iter(train) [1310/2181]  lr: 3.6367e-04  eta: 0:55:08  time: 3.7326  data_time: 0.0034  memory: 22254  loss: 2.2035
06/30 12:50:27 - mmengine - INFO - Iter(train) [1320/2181]  lr: 3.5655e-04  eta: 0:54:29  time: 3.7362  data_time: 0.0034  memory: 22309  loss: 2.2827
06/30 12:51:04 - mmengine - INFO - Iter(train) [1330/2181]  lr: 3.4945e-04  eta: 0:53:51  time: 3.7307  data_time: 0.0034  memory: 22346  loss: 2.2061
06/30 12:51:42 - mmengine - INFO - Iter(train) [1340/2181]  lr: 3.4239e-04  eta: 0:53:13  time: 3.7360  data_time: 0.0034  memory: 22272  loss: 2.1921
06/30 12:52:19 - mmengine - INFO - Iter(train) [1350/2181]  lr: 3.3536e-04  eta: 0:52:34  time: 3.7364  data_time: 0.0034  memory: 22290  loss: 2.2107
06/30 12:52:56 - mmengine - INFO - Iter(train) [1360/2181]  lr: 3.2837e-04  eta: 0:51:56  time: 3.7315  data_time: 0.0033  memory: 22364  loss: 2.2199
06/30 12:53:34 - mmengine - INFO - Iter(train) [1370/2181]  lr: 3.2142e-04  eta: 0:51:18  time: 3.7401  data_time: 0.0033  memory: 22328  loss: 2.2026
06/30 12:54:11 - mmengine - INFO - Iter(train) [1380/2181]  lr: 3.1450e-04  eta: 0:50:39  time: 3.7435  data_time: 0.0034  memory: 22328  loss: 2.2467
06/30 12:54:49 - mmengine - INFO - Iter(train) [1390/2181]  lr: 3.0763e-04  eta: 0:50:01  time: 3.7405  data_time: 0.0034  memory: 22309  loss: 2.2717
06/30 12:55:26 - mmengine - INFO - Iter(train) [1400/2181]  lr: 3.0080e-04  eta: 0:49:23  time: 3.7576  data_time: 0.0034  memory: 22438  loss: 2.1880
06/30 12:56:04 - mmengine - INFO - Iter(train) [1410/2181]  lr: 2.9401e-04  eta: 0:48:45  time: 3.7548  data_time: 0.0034  memory: 22309  loss: 2.2205
06/30 12:56:41 - mmengine - INFO - Iter(train) [1420/2181]  lr: 2.8727e-04  eta: 0:48:07  time: 3.7692  data_time: 0.0034  memory: 22364  loss: 2.2834
06/30 12:57:19 - mmengine - INFO - Iter(train) [1430/2181]  lr: 2.8058e-04  eta: 0:47:29  time: 3.7576  data_time: 0.0035  memory: 22383  loss: 2.2878
06/30 12:57:57 - mmengine - INFO - Iter(train) [1440/2181]  lr: 2.7393e-04  eta: 0:46:50  time: 3.7538  data_time: 0.0034  memory: 22309  loss: 2.2943
06/30 12:58:34 - mmengine - INFO - Iter(train) [1450/2181]  lr: 2.6734e-04  eta: 0:46:12  time: 3.7498  data_time: 0.0034  memory: 22364  loss: 2.2019
06/30 12:59:11 - mmengine - INFO - Iter(train) [1460/2181]  lr: 2.6079e-04  eta: 0:45:34  time: 3.7443  data_time: 0.0034  memory: 22346  loss: 2.1839
06/30 12:59:49 - mmengine - INFO - Iter(train) [1470/2181]  lr: 2.5430e-04  eta: 0:44:56  time: 3.7397  data_time: 0.0035  memory: 22309  loss: 2.2517
06/30 13:00:26 - mmengine - INFO - Iter(train) [1480/2181]  lr: 2.4786e-04  eta: 0:44:17  time: 3.6692  data_time: 0.0034  memory: 22272  loss: 2.3358
06/30 13:01:03 - mmengine - INFO - Iter(train) [1490/2181]  lr: 2.4148e-04  eta: 0:43:39  time: 3.7565  data_time: 0.0035  memory: 22383  loss: 2.1782
06/30 13:01:41 - mmengine - INFO - Iter(train) [1500/2181]  lr: 2.3515e-04  eta: 0:43:01  time: 3.7554  data_time: 0.0034  memory: 22236  loss: 2.2282
06/30 13:01:41 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:01:41 - mmengine - INFO - Sample output:
<|user|>
<image>
请描述一下这张照片<|end|>
<|assistant|>
a wooden dock with a boat in the water<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:01:42 - mmengine - INFO - Sample output:
<|user|>
<image>
Please describe this picture<|end|>
<|assistant|>
a wooden dock with a boat in the water and a mountain in the background<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:01:42 - mmengine - INFO - Saving checkpoint at 1500 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-30 13:01:43,295] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_1500.pth is about to be saved!
[2024-06-30 13:01:43,306] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/mp_rank_00_model_states.pt
[2024-06-30 13:01:43,306] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/mp_rank_00_model_states.pt...
[2024-06-30 13:01:43,334] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/mp_rank_00_model_states.pt.
[2024-06-30 13:01:43,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-30 13:01:43,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-30 13:01:43,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-30 13:01:43,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-30 13:01:43,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-30 13:01:43,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-30 13:01:43,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-30 13:01:43,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-30 13:01:43,352] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-30 13:01:43,352] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-30 13:01:43,352] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-30 13:01:43,352] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-30 13:01:43,352] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-30 13:01:43,353] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-30 13:01:43,353] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-30 13:01:43,353] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-30 13:01:43,353] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-30 13:01:43,353] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-30 13:01:43,355] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-30 13:01:43,356] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-30 13:01:43,356] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/30 13:02:20 - mmengine - INFO - Iter(train) [1510/2181]  lr: 2.2889e-04  eta: 0:42:24  time: 3.9713  data_time: 0.2195  memory: 22328  loss: 2.3119
06/30 13:02:58 - mmengine - INFO - Iter(train) [1520/2181]  lr: 2.2268e-04  eta: 0:41:46  time: 3.7465  data_time: 0.0034  memory: 22254  loss: 2.3203
06/30 13:03:35 - mmengine - INFO - Iter(train) [1530/2181]  lr: 2.1653e-04  eta: 0:41:08  time: 3.7434  data_time: 0.0034  memory: 22254  loss: 2.2701
06/30 13:04:13 - mmengine - INFO - Iter(train) [1540/2181]  lr: 2.1045e-04  eta: 0:40:30  time: 3.7400  data_time: 0.0034  memory: 22236  loss: 2.2331
06/30 13:04:50 - mmengine - INFO - Iter(train) [1550/2181]  lr: 2.0443e-04  eta: 0:39:52  time: 3.7489  data_time: 0.0034  memory: 22254  loss: 2.1601
06/30 13:05:28 - mmengine - INFO - Iter(train) [1560/2181]  lr: 1.9847e-04  eta: 0:39:14  time: 3.7444  data_time: 0.0034  memory: 22420  loss: 2.1488
06/30 13:06:05 - mmengine - INFO - Iter(train) [1570/2181]  lr: 1.9259e-04  eta: 0:38:35  time: 3.7164  data_time: 0.0034  memory: 22383  loss: 2.1855
06/30 13:06:42 - mmengine - INFO - Iter(train) [1580/2181]  lr: 1.8676e-04  eta: 0:37:57  time: 3.7292  data_time: 0.0034  memory: 22254  loss: 2.1865
06/30 13:07:19 - mmengine - INFO - Iter(train) [1590/2181]  lr: 1.8101e-04  eta: 0:37:19  time: 3.7279  data_time: 0.0033  memory: 22272  loss: 2.2061
06/30 13:07:57 - mmengine - INFO - Iter(train) [1600/2181]  lr: 1.7533e-04  eta: 0:36:41  time: 3.7574  data_time: 0.0034  memory: 22272  loss: 2.2565
06/30 13:08:38 - mmengine - INFO - Iter(train) [1610/2181]  lr: 1.6972e-04  eta: 0:36:04  time: 4.0829  data_time: 0.0034  memory: 22878  loss: 2.1653
06/30 13:09:15 - mmengine - INFO - Iter(train) [1620/2181]  lr: 1.6419e-04  eta: 0:35:26  time: 3.7374  data_time: 0.0034  memory: 22217  loss: 2.2625
06/30 13:09:53 - mmengine - INFO - Iter(train) [1630/2181]  lr: 1.5872e-04  eta: 0:34:48  time: 3.7533  data_time: 0.0034  memory: 22345  loss: 2.1852
06/30 13:10:30 - mmengine - INFO - Iter(train) [1640/2181]  lr: 1.5334e-04  eta: 0:34:10  time: 3.7625  data_time: 0.0036  memory: 22309  loss: 2.2018
06/30 13:11:08 - mmengine - INFO - Iter(train) [1650/2181]  lr: 1.4802e-04  eta: 0:33:32  time: 3.7522  data_time: 0.0034  memory: 22254  loss: 2.1891
06/30 13:11:45 - mmengine - INFO - Iter(train) [1660/2181]  lr: 1.4279e-04  eta: 0:32:54  time: 3.7573  data_time: 0.0033  memory: 22419  loss: 2.1903
06/30 13:12:23 - mmengine - INFO - Iter(train) [1670/2181]  lr: 1.3764e-04  eta: 0:32:16  time: 3.7506  data_time: 0.0034  memory: 22364  loss: 2.2373
06/30 13:13:00 - mmengine - INFO - Iter(train) [1680/2181]  lr: 1.3256e-04  eta: 0:31:38  time: 3.7555  data_time: 0.0034  memory: 22345  loss: 2.2000
06/30 13:13:38 - mmengine - INFO - Iter(train) [1690/2181]  lr: 1.2757e-04  eta: 0:31:00  time: 3.7373  data_time: 0.0034  memory: 22364  loss: 2.2011
06/30 13:14:15 - mmengine - INFO - Iter(train) [1700/2181]  lr: 1.2266e-04  eta: 0:30:22  time: 3.7400  data_time: 0.0034  memory: 22254  loss: 2.1362
06/30 13:14:53 - mmengine - INFO - Iter(train) [1710/2181]  lr: 1.1783e-04  eta: 0:29:44  time: 3.7945  data_time: 0.0034  memory: 22290  loss: 2.2297
06/30 13:15:31 - mmengine - INFO - Iter(train) [1720/2181]  lr: 1.1308e-04  eta: 0:29:06  time: 3.7322  data_time: 0.0034  memory: 22272  loss: 2.1294
06/30 13:16:08 - mmengine - INFO - Iter(train) [1730/2181]  lr: 1.0842e-04  eta: 0:28:28  time: 3.7375  data_time: 0.0034  memory: 22327  loss: 2.2123
06/30 13:16:45 - mmengine - INFO - Iter(train) [1740/2181]  lr: 1.0385e-04  eta: 0:27:50  time: 3.7428  data_time: 0.0034  memory: 22437  loss: 2.1067
06/30 13:17:23 - mmengine - INFO - Iter(train) [1750/2181]  lr: 9.9366e-05  eta: 0:27:12  time: 3.7370  data_time: 0.0034  memory: 22327  loss: 2.2489
06/30 13:18:00 - mmengine - INFO - Iter(train) [1760/2181]  lr: 9.4969e-05  eta: 0:26:34  time: 3.7388  data_time: 0.0034  memory: 22272  loss: 2.0724
06/30 13:18:37 - mmengine - INFO - Iter(train) [1770/2181]  lr: 9.0661e-05  eta: 0:25:56  time: 3.7313  data_time: 0.0034  memory: 22290  loss: 2.1771
06/30 13:19:15 - mmengine - INFO - Iter(train) [1780/2181]  lr: 8.6443e-05  eta: 0:25:18  time: 3.7316  data_time: 0.0034  memory: 22290  loss: 2.2135
06/30 13:19:52 - mmengine - INFO - Iter(train) [1790/2181]  lr: 8.2317e-05  eta: 0:24:40  time: 3.7472  data_time: 0.0034  memory: 22345  loss: 2.2265
06/30 13:20:30 - mmengine - INFO - Iter(train) [1800/2181]  lr: 7.8282e-05  eta: 0:24:02  time: 3.7351  data_time: 0.0034  memory: 22290  loss: 2.1568
06/30 13:21:07 - mmengine - INFO - Iter(train) [1810/2181]  lr: 7.4341e-05  eta: 0:23:24  time: 3.7304  data_time: 0.0034  memory: 22272  loss: 2.2128
06/30 13:21:44 - mmengine - INFO - Iter(train) [1820/2181]  lr: 7.0493e-05  eta: 0:22:46  time: 3.7340  data_time: 0.0034  memory: 22327  loss: 2.2643
06/30 13:22:22 - mmengine - INFO - Iter(train) [1830/2181]  lr: 6.6740e-05  eta: 0:22:08  time: 3.7408  data_time: 0.0034  memory: 22309  loss: 2.2133
06/30 13:22:59 - mmengine - INFO - Iter(train) [1840/2181]  lr: 6.3083e-05  eta: 0:21:30  time: 3.7462  data_time: 0.0034  memory: 22364  loss: 2.1937
06/30 13:23:37 - mmengine - INFO - Iter(train) [1850/2181]  lr: 5.9521e-05  eta: 0:20:52  time: 3.7502  data_time: 0.0034  memory: 22382  loss: 2.1538
06/30 13:24:14 - mmengine - INFO - Iter(train) [1860/2181]  lr: 5.6057e-05  eta: 0:20:14  time: 3.7496  data_time: 0.0034  memory: 22290  loss: 2.2268
06/30 13:24:52 - mmengine - INFO - Iter(train) [1870/2181]  lr: 5.2691e-05  eta: 0:19:37  time: 3.7546  data_time: 0.0034  memory: 22290  loss: 2.1864
06/30 13:25:29 - mmengine - INFO - Iter(train) [1880/2181]  lr: 4.9424e-05  eta: 0:18:59  time: 3.7556  data_time: 0.0034  memory: 22290  loss: 2.2450
06/30 13:26:07 - mmengine - INFO - Iter(train) [1890/2181]  lr: 4.6255e-05  eta: 0:18:21  time: 3.7528  data_time: 0.0034  memory: 22290  loss: 2.1061
06/30 13:26:44 - mmengine - INFO - Iter(train) [1900/2181]  lr: 4.3187e-05  eta: 0:17:43  time: 3.7489  data_time: 0.0034  memory: 22199  loss: 2.1542
06/30 13:27:22 - mmengine - INFO - Iter(train) [1910/2181]  lr: 4.0219e-05  eta: 0:17:05  time: 3.7582  data_time: 0.0034  memory: 22345  loss: 2.1790
06/30 13:27:59 - mmengine - INFO - Iter(train) [1920/2181]  lr: 3.7353e-05  eta: 0:16:27  time: 3.7405  data_time: 0.0034  memory: 22236  loss: 2.2560
06/30 13:28:37 - mmengine - INFO - Iter(train) [1930/2181]  lr: 3.4589e-05  eta: 0:15:49  time: 3.7397  data_time: 0.0034  memory: 22364  loss: 2.2401
06/30 13:29:14 - mmengine - INFO - Iter(train) [1940/2181]  lr: 3.1927e-05  eta: 0:15:11  time: 3.7379  data_time: 0.0034  memory: 22272  loss: 2.2077
06/30 13:29:51 - mmengine - INFO - Iter(train) [1950/2181]  lr: 2.9369e-05  eta: 0:14:33  time: 3.7340  data_time: 0.0034  memory: 22217  loss: 2.1186
06/30 13:30:29 - mmengine - INFO - Iter(train) [1960/2181]  lr: 2.6914e-05  eta: 0:13:56  time: 3.7364  data_time: 0.0034  memory: 22272  loss: 2.2360
06/30 13:31:06 - mmengine - INFO - Iter(train) [1970/2181]  lr: 2.4564e-05  eta: 0:13:18  time: 3.7346  data_time: 0.0034  memory: 22290  loss: 2.1644
06/30 13:31:44 - mmengine - INFO - Iter(train) [1980/2181]  lr: 2.2318e-05  eta: 0:12:40  time: 3.8223  data_time: 0.0034  memory: 22455  loss: 2.1820
06/30 13:32:22 - mmengine - INFO - Iter(train) [1990/2181]  lr: 2.0178e-05  eta: 0:12:02  time: 3.7306  data_time: 0.0034  memory: 22290  loss: 2.1353
06/30 13:32:59 - mmengine - INFO - Exp name: llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy_20240630_112119
06/30 13:32:59 - mmengine - INFO - Iter(train) [2000/2181]  lr: 1.8143e-05  eta: 0:11:24  time: 3.7400  data_time: 0.0034  memory: 22345  loss: 2.1626
06/30 13:32:59 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:32:59 - mmengine - INFO - Sample output:
<|user|>
<image>
请描述一下这张照片<|end|>
<|assistant|>
a wooden dock in the middle of a lake<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:33:00 - mmengine - INFO - Sample output:
<|user|>
<image>
Please describe this picture<|end|>
<|assistant|>
a wooden dock in the middle of a lake with a mountain in the background<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:33:00 - mmengine - INFO - Saving checkpoint at 2000 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-30 13:33:01,484] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_2000.pth is about to be saved!
[2024-06-30 13:33:01,495] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/mp_rank_00_model_states.pt
[2024-06-30 13:33:01,495] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/mp_rank_00_model_states.pt...
[2024-06-30 13:33:01,524] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/mp_rank_00_model_states.pt.
[2024-06-30 13:33:01,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-30 13:33:01,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-30 13:33:01,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-30 13:33:01,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-30 13:33:01,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-30 13:33:01,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-30 13:33:01,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-30 13:33:01,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-30 13:33:01,540] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-30 13:33:01,540] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-30 13:33:01,540] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-30 13:33:01,540] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-30 13:33:01,541] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-30 13:33:01,541] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-30 13:33:01,541] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-30 13:33:01,541] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-30 13:33:01,541] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-30 13:33:01,541] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-30 13:33:01,542] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-30 13:33:01,542] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-30 13:33:01,542] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-30 13:33:01,542] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/30 13:33:38 - mmengine - INFO - Iter(train) [2010/2181]  lr: 1.6215e-05  eta: 0:10:46  time: 3.9512  data_time: 0.2174  memory: 22364  loss: 2.1187
06/30 13:34:16 - mmengine - INFO - Iter(train) [2020/2181]  lr: 1.4393e-05  eta: 0:10:09  time: 3.7315  data_time: 0.0034  memory: 22327  loss: 2.1123
06/30 13:34:53 - mmengine - INFO - Iter(train) [2030/2181]  lr: 1.2678e-05  eta: 0:09:31  time: 3.7305  data_time: 0.0034  memory: 22290  loss: 2.1649
06/30 13:35:30 - mmengine - INFO - Iter(train) [2040/2181]  lr: 1.1071e-05  eta: 0:08:53  time: 3.7331  data_time: 0.0034  memory: 22272  loss: 2.1061
06/30 13:36:08 - mmengine - INFO - Iter(train) [2050/2181]  lr: 9.5712e-06  eta: 0:08:15  time: 3.7391  data_time: 0.0034  memory: 22272  loss: 2.1824
06/30 13:36:45 - mmengine - INFO - Iter(train) [2060/2181]  lr: 8.1798e-06  eta: 0:07:37  time: 3.7492  data_time: 0.0034  memory: 22309  loss: 2.2707
06/30 13:37:24 - mmengine - INFO - Iter(train) [2070/2181]  lr: 6.8967e-06  eta: 0:06:59  time: 3.9159  data_time: 0.0034  memory: 22290  loss: 2.1893
06/30 13:38:02 - mmengine - INFO - Iter(train) [2080/2181]  lr: 5.7224e-06  eta: 0:06:22  time: 3.7529  data_time: 0.0034  memory: 22345  loss: 2.1276
06/30 13:38:40 - mmengine - INFO - Iter(train) [2090/2181]  lr: 4.6570e-06  eta: 0:05:44  time: 3.7567  data_time: 0.0033  memory: 22382  loss: 2.2513
06/30 13:39:20 - mmengine - INFO - Iter(train) [2100/2181]  lr: 3.7008e-06  eta: 0:05:06  time: 4.0665  data_time: 0.0034  memory: 22382  loss: 2.1354
06/30 13:39:58 - mmengine - INFO - Iter(train) [2110/2181]  lr: 2.8540e-06  eta: 0:04:28  time: 3.7489  data_time: 0.0034  memory: 22272  loss: 2.1474
06/30 13:40:35 - mmengine - INFO - Iter(train) [2120/2181]  lr: 2.1168e-06  eta: 0:03:50  time: 3.7644  data_time: 0.0034  memory: 22327  loss: 2.1634
06/30 13:41:13 - mmengine - INFO - Iter(train) [2130/2181]  lr: 1.4894e-06  eta: 0:03:12  time: 3.7523  data_time: 0.0034  memory: 22327  loss: 2.1567
06/30 13:41:50 - mmengine - INFO - Iter(train) [2140/2181]  lr: 9.7178e-07  eta: 0:02:35  time: 3.7423  data_time: 0.0034  memory: 22364  loss: 2.2191
06/30 13:42:28 - mmengine - INFO - Iter(train) [2150/2181]  lr: 5.6419e-07  eta: 0:01:57  time: 3.8197  data_time: 0.0034  memory: 22254  loss: 2.2741
06/30 13:43:06 - mmengine - INFO - Iter(train) [2160/2181]  lr: 2.6670e-07  eta: 0:01:19  time: 3.7342  data_time: 0.0034  memory: 22327  loss: 2.1979
06/30 13:43:43 - mmengine - INFO - Iter(train) [2170/2181]  lr: 7.9352e-08  eta: 0:00:41  time: 3.7372  data_time: 0.0034  memory: 22345  loss: 2.2571
06/30 13:44:21 - mmengine - INFO - Iter(train) [2180/2181]  lr: 2.2043e-09  eta: 0:00:03  time: 3.7354  data_time: 0.0037  memory: 22364  loss: 2.0672
06/30 13:44:21 - mmengine - INFO - Exp name: llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy_20240630_112119
06/30 13:44:21 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:44:22 - mmengine - INFO - Sample output:
<|user|>
<image>
请描述一下这张照片<|end|>
<|assistant|>
a wooden dock in the middle of a lake<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:44:22 - mmengine - INFO - Sample output:
<|user|>
<image>
Please describe this picture<|end|>
<|assistant|>
a wooden dock in the middle of a lake with a mountain in the background<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:44:22 - mmengine - INFO - Saving checkpoint at 2181 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-30 13:44:23,669] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_2181.pth is about to be saved!
[2024-06-30 13:44:23,680] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/mp_rank_00_model_states.pt
[2024-06-30 13:44:23,680] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/mp_rank_00_model_states.pt...
[2024-06-30 13:44:23,709] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/mp_rank_00_model_states.pt.
[2024-06-30 13:44:23,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-30 13:44:23,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-30 13:44:23,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-30 13:44:23,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-30 13:44:23,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-30 13:44:23,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-30 13:44:23,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-30 13:44:23,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-30 13:44:23,726] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-30 13:44:23,726] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-30 13:44:23,726] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2181.pth is ready now!
[2024-06-30 13:44:23,726] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-30 13:44:23,726] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-30 13:44:23,726] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-30 13:44:23,726] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2181.pth is ready now!
[2024-06-30 13:44:23,726] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-30 13:44:23,726] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-30 13:44:23,726] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-30 13:44:23,727] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-30 13:44:23,727] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2181.pth is ready now!
[2024-06-30 13:44:23,727] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-30 13:44:23,727] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2181.pth is ready now!
[2024-06-30 13:44:23,727] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2181.pth is ready now!
[2024-06-30 13:44:23,727] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-30 13:44:23,727] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-30 13:44:23,727] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2181.pth is ready now!
[2024-06-30 13:44:23,727] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-30 13:44:23,727] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-30 13:44:23,727] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2181.pth is ready now!
[2024-06-30 13:44:23,727] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-30 13:44:23,728] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-30 13:44:23,728] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2181.pth is ready now!
06/30 13:44:23 - mmengine - INFO - after_train in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:44:24 - mmengine - INFO - Sample output:
<|user|>
<image>
请描述一下这张照片<|end|>
<|assistant|>
a wooden dock in the lake with a boat in the background<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:44:24 - mmengine - INFO - Sample output:
<|user|>
<image>
Please describe this picture<|end|>
<|assistant|>
a wooden dock in the middle of a lake with a mountain in the background<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
wandb: - 4.930 MB of 4.930 MB uploaded (4.903 MB deduped)wandb: \ 4.930 MB of 4.930 MB uploaded (4.903 MB deduped)wandb: | 4.930 MB of 4.930 MB uploaded (4.903 MB deduped)wandb: / 4.930 MB of 4.930 MB uploaded (4.903 MB deduped)wandb: - 4.930 MB of 4.930 MB uploaded (4.903 MB deduped)wandb: \ 4.930 MB of 4.930 MB uploaded (4.903 MB deduped)wandb: | 4.930 MB of 4.930 MB uploaded (4.903 MB deduped)wandb: / 4.930 MB of 4.930 MB uploaded (4.903 MB deduped)wandb: - 4.965 MB of 4.965 MB uploaded (4.903 MB deduped)wandb: \ 4.973 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: | 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: / 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: - 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: \ 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: | 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: / 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: - 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: \ 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: | 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: / 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: - 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: \ 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: | 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: / 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: - 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: \ 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: | 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: / 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: - 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: \ 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: | 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: / 5.054 MB of 5.054 MB uploaded (4.903 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 97.0%             
wandb: 
wandb: Run history:
wandb: data_time ▇▃▂▂▁▂▃▂▁▂▃▃▂▃▃▃▃▂▃▂▂▂▂▁▃▃▂▃▃▂▂▃▃▃▃▃▂▂▃█
wandb:      iter ▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇███
wandb:      loss █▅▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▁▁▁▂▁▂▁▁▁▁▁▁▁▂▁▁
wandb:        lr ▃▇██████▇▇▇▇▇▆▆▆▆▅▅▅▅▄▄▄▃▃▃▃▂▂▂▂▂▁▁▁▁▁▁▁
wandb:    memory ▄▃▃▄▄▂▄▃▃▃▃▂▃▃▅█▅▁▅▄█▂▄▄▃▇▅▂▆▁▄▃▃▅▁▃▄▄▄▅
wandb:      time █▁▁▂▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁
wandb: 
wandb: Run summary:
wandb: data_time 0.00368
wandb:      iter 2180
wandb:      loss 2.0672
wandb:        lr 0.0
wandb:    memory 22364
wandb:      time 3.73535
wandb: 
wandb: 🚀 View run charmed-durian-8 at: https://wandb.ai/jzyztzn/lava-clipL32-Phi-3-mini/runs/r4hb3aac
wandb: ⭐️ View project at: https://wandb.ai/jzyztzn/lava-clipL32-Phi-3-mini
wandb: Synced 6 W&B file(s), 0 media file(s), 736 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/20240630_112119/vis_data/wandb/run-20240630_112126-r4hb3aac/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
