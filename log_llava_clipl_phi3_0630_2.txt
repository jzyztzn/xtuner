nohup: ignoring input
06/30 11:21:08 - mmengine - WARNING - Urun_llavaphi3.sh: 4: xtuner: not found
:09,605] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 11:21:17,615] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 11:21:17,701] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 11:21:17,706] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 11:21:17,753] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 11:21:17,755] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 11:21:17,756] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 11:21:17,793] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[2024-06-30 11:21:17,798] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt[93m [WARNING] [0m async_io: please install the libaio-dev package with apt

[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.

[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH

[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[2024-06-30 11:21:19,784] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 11:21:19,784] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-06-30 11:21:20,497] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 11:21:20,696] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 11:21:20,699] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 11:21:20,746] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 11:21:20,750] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 11:21:20,754] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 11:21:20,757] [INFO] [comm.py:637:init_distributed] cdb=None
06/30 11:21:24 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 576542663
    GPU 0,1,2,3,4,5,6,7: NVIDIA A800 80GB PCIe
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 12.1, V12.1.105
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.3.1+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.18.1+cu121
    OpenCV: 4.10.0
    MMEngine: 0.10.4

Runtime environment:
    launcher: pytorch
    randomness: {'seed': None, 'deterministic': False}
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: None
    deterministic: False
    Distributed launcher: pytorch
    Distributed training: True
    GPU number: 8
------------------------------------------------------------

06/30 11:21:25 - mmengine - INFO - Config:
SYSTEM = ''
accumulative_counts = 1
batch_size = 32
betas = (
    0.9,
    0.999,
)
custom_hooks = [
    dict(
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.engine.hooks.DatasetInfoHook'),
    dict(
        evaluation_images='view.jpg',
        evaluation_inputs=[
            'è¯·æè¿°ä¸€ä¸‹è¿™å¼ ç…§ç‰‡',
            'Please describe this picture',
        ],
        every_n_iters=500,
        image_processor=dict(
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
            trust_remote_code=True,
            type='transformers.CLIPImageProcessor.from_pretrained'),
        prompt_template='xtuner.utils.PROMPT_TEMPLATE.phi3_chat',
        system='',
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.engine.hooks.EvaluateChatHook'),
]
data_path = './data/llava_data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json'
data_root = './data/llava_data/'
dataloader_num_workers = 4
default_hooks = dict(
    checkpoint=dict(
        by_epoch=False,
        interval=500,
        max_keep_ckpts=2,
        type='mmengine.hooks.CheckpointHook'),
    logger=dict(
        interval=10,
        log_metric_by_epoch=False,
        type='mmengine.hooks.LoggerHook'),
    param_scheduler=dict(type='mmengine.hooks.ParamSchedulerHook'),
    sampler_seed=dict(type='mmengine.hooks.DistSamplerSeedHook'),
    timer=dict(type='mmengine.hooks.IterTimerHook'))
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
evaluation_freq = 500
evaluation_images = 'view.jpg'
evaluation_inputs = [
    'è¯·æè¿°ä¸€ä¸‹è¿™å¼ ç…§ç‰‡',
    'Please describe this picture',
]
image_folder = './data/llava_data/LLaVA-Pretrain/images'
image_processor = dict(
    pretrained_model_name_or_path=
    '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
    trust_remote_code=True,
    type='transformers.CLIPImageProcessor.from_pretrained')
launcher = 'pytorch'
llava_dataset = dict(
    data_path='./data/llava_data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json',
    dataset_map_fn='xtuner.dataset.map_fns.llava_map_fn',
    image_folder='./data/llava_data/LLaVA-Pretrain/images',
    image_processor=dict(
        pretrained_model_name_or_path=
        '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
        trust_remote_code=True,
        type='transformers.CLIPImageProcessor.from_pretrained'),
    max_length=1472,
    pad_image_to_square=False,
    template_map_fn=dict(
        template='xtuner.utils.PROMPT_TEMPLATE.phi3_chat',
        type='xtuner.dataset.map_fns.template_map_fn_factory'),
    tokenizer=dict(
        padding_side='right',
        pretrained_model_name_or_path=
        '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
        trust_remote_code=True,
        type='transformers.AutoTokenizer.from_pretrained'),
    type='xtuner.dataset.LLaVADataset')
llm_name_or_path = '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct'
load_from = None
log_level = 'INFO'
log_processor = dict(by_epoch=False)
lr = 0.001
max_epochs = 1
max_length = 1472
max_norm = 1
model = dict(
    freeze_llm=True,
    freeze_visual_encoder=True,
    llm=dict(
        pretrained_model_name_or_path=
        '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
        trust_remote_code=True,
        type='transformers.AutoModelForCausalLM.from_pretrained'),
    type='xtuner.model.LLaVAModel',
    visual_encoder=dict(
        pretrained_model_name_or_path=
        '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
        type='transformers.CLIPVisionModel.from_pretrained'))
optim_type = 'torch.optim.AdamW'
optim_wrapper = dict(
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        lr=0.001,
        type='torch.optim.AdamW',
        weight_decay=0),
    type='DeepSpeedOptimWrapper')
param_scheduler = [
    dict(
        begin=0,
        by_epoch=True,
        convert_to_iter_based=True,
        end=0.03,
        start_factor=1e-05,
        type='mmengine.optim.LinearLR'),
    dict(
        begin=0.03,
        by_epoch=True,
        convert_to_iter_based=True,
        end=1,
        eta_min=0.0,
        type='mmengine.optim.CosineAnnealingLR'),
]
prompt_template = 'xtuner.utils.PROMPT_TEMPLATE.phi3_chat'
randomness = dict(deterministic=False, seed=None)
resume = False
runner_type = 'FlexibleRunner'
save_steps = 500
save_total_limit = 2
strategy = dict(
    config=dict(
        bf16=dict(enabled=True),
        fp16=dict(enabled=False, initial_scale_power=16),
        gradient_accumulation_steps='auto',
        gradient_clipping='auto',
        train_micro_batch_size_per_gpu='auto',
        zero_allow_untested_optimizer=True,
        zero_force_ds_cpu_optimizer=False,
        zero_optimization=dict(overlap_comm=True, stage=2)),
    exclude_frozen_parameters=True,
    gradient_accumulation_steps=1,
    gradient_clipping=1,
    sequence_parallel_size=1,
    train_micro_batch_size_per_gpu=32,
    type='xtuner.engine.DeepSpeedStrategy')
tokenizer = dict(
    padding_side='right',
    pretrained_model_name_or_path=
    '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
    trust_remote_code=True,
    type='transformers.AutoTokenizer.from_pretrained')
train_cfg = dict(max_epochs=1, type='xtuner.engine.runner.TrainLoop')
train_dataloader = dict(
    batch_size=32,
    collate_fn=dict(type='xtuner.dataset.collate_fns.default_collate_fn'),
    dataset=dict(
        data_path=
        './data/llava_data/LLaVA-Pretrain/blip_laion_cc_sbu_558k.json',
        dataset_map_fn='xtuner.dataset.map_fns.llava_map_fn',
        image_folder='./data/llava_data/LLaVA-Pretrain/images',
        image_processor=dict(
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
            trust_remote_code=True,
            type='transformers.CLIPImageProcessor.from_pretrained'),
        max_length=1472,
        pad_image_to_square=False,
        template_map_fn=dict(
            template='xtuner.utils.PROMPT_TEMPLATE.phi3_chat',
            type='xtuner.dataset.map_fns.template_map_fn_factory'),
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.dataset.LLaVADataset'),
    num_workers=4,
    pin_memory=True,
    sampler=dict(shuffle=True, type='mmengine.dataset.DefaultSampler'))
use_wandb = True
visual_encoder_name_or_path = '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336'
visualizer = dict(
    type='mmengine.visualization.Visualizer',
    vis_backends=[
        dict(
            init_kwargs=dict(project='lava-clipL32-Phi-3-mini'),
            type='mmengine.visualization.WandbVisBackend'),
    ])
wandb_name = 'lava-clipL32-Phi-3-mini'
warmup_ratio = 0.03
weight_decay = 0
work_dir = './work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy'

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
wandb: Currently logged in as: jzyztzn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/20240630_112119/vis_data/wandb/run-20240630_112126-r4hb3aac
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run charmed-durian-8
wandb: â­ï¸ View project at https://wandb.ai/jzyztzn/lava-clipL32-Phi-3-mini
wandb: ğŸš€ View run at https://wandb.ai/jzyztzn/lava-clipL32-Phi-3-mini/runs/r4hb3aac
06/30 11:21:32 - mmengine - WARNING - Failed to search registry with scope "mmengine" in the "builder" registry tree. As a workaround, the current "builder" registry in "xtuner" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether "mmengine" is a correct scope, or whether the registry is initialized.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
06/30 11:21:33 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DatasetInfoHook                    
(LOW         ) EvaluateChatHook                   
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(LOW         ) EvaluateChatHook                   
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) DatasetInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
(LOW         ) EvaluateChatHook                   
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(LOW         ) EvaluateChatHook                   
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) DatasetInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
06/30 11:21:38 - mmengine - INFO - xtuner_dataset_timeout = 1:00:00
Map (num_proc=32):   0%|          | 0/558128 [00:00<?, ? examples/s]Map (num_proc=32):   0%|          | 987/558128 [00:00<07:16, 1275.14 examples/s]Map (num_proc=32):   0%|          | 2620/558128 [00:00<02:46, 3335.15 examples/s]Map (num_proc=32):   1%|          | 4341/558128 [00:01<01:39, 5569.19 examples/s]Map (num_proc=32):   1%|â–         | 7977/558128 [00:01<00:49, 11155.42 examples/s]Map (num_proc=32):   2%|â–         | 10757/558128 [00:01<00:41, 13111.69 examples/s]Map (num_proc=32):   2%|â–         | 13404/558128 [00:01<00:35, 15174.29 examples/s]Map (num_proc=32):   4%|â–         | 22943/558128 [00:01<00:17, 30302.91 examples/s]Map (num_proc=32):   5%|â–         | 27353/558128 [00:01<00:18, 28819.21 examples/s]Map (num_proc=32):   6%|â–‹         | 35549/558128 [00:01<00:15, 33962.00 examples/s]Map (num_proc=32):   8%|â–Š         | 43362/558128 [00:02<00:12, 42358.49 examples/s]Map (num_proc=32):   9%|â–Š         | 48307/558128 [00:02<00:13, 38633.77 examples/s]Map (num_proc=32):  10%|â–‰         | 53884/558128 [00:02<00:12, 40810.77 examples/s]Map (num_proc=32):  11%|â–ˆ         | 59401/558128 [00:02<00:12, 40772.61 examples/s]Map (num_proc=32):  11%|â–ˆâ–        | 63904/558128 [00:02<00:12, 39469.98 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 71343/558128 [00:02<00:10, 47156.13 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 77522/558128 [00:02<00:10, 45290.38 examples/s]Map (num_proc=32):  15%|â–ˆâ–        | 82935/558128 [00:02<00:10, 44274.95 examples/s]Map (num_proc=32):  17%|â–ˆâ–‹        | 92138/558128 [00:03<00:08, 54928.34 examples/s]Map (num_proc=32):  18%|â–ˆâ–Š        | 98708/558128 [00:03<00:11, 40554.07 examples/s]Map (num_proc=32):  20%|â–ˆâ–‰        | 108998/558128 [00:03<00:08, 52166.62 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆ        | 115256/558128 [00:03<00:11, 39886.67 examples/s]Map (num_proc=32):  22%|â–ˆâ–ˆâ–       | 123429/558128 [00:03<00:09, 46013.22 examples/s]Map (num_proc=32):  23%|â–ˆâ–ˆâ–       | 129702/558128 [00:03<00:09, 44991.36 examples/s]Map (num_proc=32):  25%|â–ˆâ–ˆâ–       | 138403/558128 [00:04<00:07, 53233.08 examples/s]Map (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 145254/558128 [00:04<00:12, 34212.31 examples/s]Map (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 158536/558128 [00:04<00:08, 48660.59 examples/s]Map (num_proc=32):  30%|â–ˆâ–ˆâ–‰       | 165180/558128 [00:04<00:08, 47887.58 examples/s]Map (num_proc=32):  31%|â–ˆâ–ˆâ–ˆ       | 171173/558128 [00:04<00:07, 49470.90 examples/s]Map (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 177583/558128 [00:04<00:07, 50495.75 examples/s]Map (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–      | 183875/558128 [00:05<00:09, 39441.93 examples/s]Map (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–      | 193420/558128 [00:05<00:08, 42616.14 examples/s]Map (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 198521/558128 [00:05<00:09, 38322.38 examples/s]Map (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 210862/558128 [00:05<00:09, 35270.39 examples/s]Map (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 228262/558128 [00:06<00:06, 47612.55 examples/s]Map (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 233598/558128 [00:06<00:07, 44988.89 examples/s]Map (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 245545/558128 [00:06<00:07, 39156.02 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 265171/558128 [00:07<00:06, 41982.40 examples/s]Map (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 281630/558128 [00:07<00:04, 56853.32 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 290367/558128 [00:07<00:05, 50001.38 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 299078/558128 [00:07<00:04, 53948.09 examples/s]Map (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 306229/558128 [00:07<00:05, 48029.47 examples/s]Map (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 313646/558128 [00:07<00:04, 50993.50 examples/s]Map (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 320091/558128 [00:08<00:05, 45970.20 examples/s]Map (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 326101/558128 [00:08<00:04, 48303.73 examples/s]Map (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 332973/558128 [00:08<00:06, 33168.07 examples/s]Map (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 349206/558128 [00:09<00:05, 37471.84 examples/s]Map (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 367836/558128 [00:09<00:04, 47137.57 examples/s]Map (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 373596/558128 [00:09<00:03, 48435.47 examples/s]Map (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 386738/558128 [00:09<00:04, 42166.24 examples/s]Map (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 402718/558128 [00:10<00:03, 42689.58 examples/s]Map (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 421159/558128 [00:10<00:03, 44785.57 examples/s]Map (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 439352/558128 [00:10<00:01, 59798.26 examples/s]Map (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 447601/558128 [00:10<00:02, 51971.92 examples/s]Map (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 455041/558128 [00:11<00:02, 39284.96 examples/s]Map (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 474373/558128 [00:11<00:01, 58488.57 examples/s]Map (num_proc=32):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 484068/558128 [00:11<00:01, 51327.51 examples/s]Map (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 492001/558128 [00:11<00:01, 39749.74 examples/s]Map (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 509977/558128 [00:12<00:00, 57601.31 examples/s]Map (num_proc=32):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 519856/558128 [00:12<00:00, 55370.53 examples/s]Map (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 528657/558128 [00:12<00:00, 49167.88 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 535612/558128 [00:12<00:00, 50534.00 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 542246/558128 [00:12<00:00, 50952.96 examples/s]Map (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 548775/558128 [00:12<00:00, 44591.76 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 554771/558128 [00:13<00:00, 39945.91 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 558128/558128 [00:13<00:00, 40866.48 examples/s]
Map (num_proc=32):   0%|          | 0/558128 [00:00<?, ? examples/s]Map (num_proc=32):   0%|          | 886/558128 [00:00<01:55, 4832.88 examples/s]Map (num_proc=32):   1%|          | 3686/558128 [00:00<00:40, 13774.32 examples/s]Map (num_proc=32):   2%|â–         | 9294/558128 [00:00<00:19, 28611.93 examples/s]Map (num_proc=32):   3%|â–         | 16580/558128 [00:00<00:12, 42868.54 examples/s]Map (num_proc=32):   5%|â–         | 25939/558128 [00:00<00:09, 57418.93 examples/s]Map (num_proc=32):   6%|â–‹         | 35731/558128 [00:00<00:07, 68556.75 examples/s]Map (num_proc=32):  10%|â–‰         | 53895/558128 [00:00<00:05, 100627.12 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 66513/558128 [00:00<00:04, 105678.19 examples/s]Map (num_proc=32):  16%|â–ˆâ–Œ        | 89029/558128 [00:01<00:03, 137893.91 examples/s]Map (num_proc=32):  19%|â–ˆâ–‰        | 105267/558128 [00:01<00:03, 140402.07 examples/s]Map (num_proc=32):  24%|â–ˆâ–ˆâ–       | 131606/558128 [00:01<00:02, 174866.22 examples/s]Map (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 150332/558128 [00:01<00:02, 176573.12 examples/s]Map (num_proc=32):  31%|â–ˆâ–ˆâ–ˆâ–      | 175670/558128 [00:01<00:02, 137691.38 examples/s]Map (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 208300/558128 [00:01<00:02, 134272.90 examples/s]Map (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 229958/558128 [00:02<00:02, 119723.98 examples/s]Map (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 243355/558128 [00:02<00:03, 93161.06 examples/s] Map (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 254562/558128 [00:02<00:03, 87947.59 examples/s]Map (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 276166/558128 [00:02<00:02, 106355.27 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 288207/558128 [00:02<00:02, 104987.64 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 302766/558128 [00:02<00:02, 112704.52 examples/s]Map (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 315060/558128 [00:02<00:02, 111870.19 examples/s]Map (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 343870/558128 [00:03<00:01, 138359.05 examples/s]Map (num_proc=32):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 362653/558128 [00:03<00:01, 137067.77 examples/s]Map (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 377252/558128 [00:03<00:01, 136828.88 examples/s]Map (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 406315/558128 [00:03<00:00, 171502.71 examples/s]Map (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 424402/558128 [00:03<00:00, 173086.51 examples/s]Map (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 442275/558128 [00:03<00:00, 136492.66 examples/s]Map (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 482600/558128 [00:03<00:00, 198033.27 examples/s]Map (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 505612/558128 [00:04<00:00, 203640.73 examples/s]Map (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 528312/558128 [00:04<00:00, 123132.65 examples/s]Map (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 545864/558128 [00:04<00:00, 101187.57 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 558128/558128 [00:05<00:00, 103277.01 examples/s]
Filter (num_proc=32):   0%|          | 0/558128 [00:00<?, ? examples/s]Filter (num_proc=32):   1%|          | 3000/558128 [00:00<00:43, 12802.89 examples/s]Filter (num_proc=32):   2%|â–         | 12000/558128 [00:00<00:14, 38241.92 examples/s]Filter (num_proc=32):   5%|â–         | 27000/558128 [00:00<00:07, 72830.53 examples/s]Filter (num_proc=32):   9%|â–‰         | 50442/558128 [00:00<00:04, 122562.81 examples/s]Filter (num_proc=32):  13%|â–ˆâ–        | 70884/558128 [00:00<00:03, 142019.66 examples/s]Filter (num_proc=32):  16%|â–ˆâ–‹        | 91326/558128 [00:00<00:03, 154684.00 examples/s]Filter (num_proc=32):  20%|â–ˆâ–‰        | 111210/558128 [00:00<00:02, 164101.64 examples/s]Filter (num_proc=32):  25%|â–ˆâ–ˆâ–       | 137652/558128 [00:01<00:02, 187185.37 examples/s]Filter (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 158094/558128 [00:01<00:02, 189717.93 examples/s]Filter (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 178536/558128 [00:01<00:02, 187941.27 examples/s]Filter (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 198420/558128 [00:01<00:01, 185679.53 examples/s]Filter (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 218862/558128 [00:01<00:02, 167290.81 examples/s]Filter (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 241304/558128 [00:01<00:01, 179704.23 examples/s]Filter (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 261746/558128 [00:01<00:02, 138094.36 examples/s]Filter (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 294072/558128 [00:01<00:01, 169134.97 examples/s]Filter (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 314513/558128 [00:02<00:01, 157352.13 examples/s]Filter (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 335513/558128 [00:02<00:01, 159467.43 examples/s]Filter (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 355395/558128 [00:02<00:01, 164677.46 examples/s]Filter (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 381836/558128 [00:02<00:00, 184248.68 examples/s]Filter (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 402277/558128 [00:02<00:00, 185462.07 examples/s]Filter (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 421718/558128 [00:02<00:00, 167180.22 examples/s]Filter (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 447600/558128 [00:02<00:00, 184552.40 examples/s]Filter (num_proc=32):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 467600/558128 [00:02<00:00, 184091.44 examples/s]Filter (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 491923/558128 [00:03<00:00, 190237.87 examples/s]Filter (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 515364/558128 [00:03<00:00, 200730.82 examples/s]Filter (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 538805/558128 [00:03<00:00, 195409.23 examples/s]Filter (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 558128/558128 [00:03<00:00, 155778.25 examples/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Map (num_proc=32):   0%|          | 0/558128 [00:00<?, ? examples/s]Map (num_proc=32):   0%|          | 158/558128 [00:04<3:57:07, 39.22 examples/s]Map (num_proc=32):   0%|          | 640/558128 [00:04<45:58, 202.09 examples/s] Map (num_proc=32):   0%|          | 981/558128 [00:04<26:26, 351.09 examples/s]Map (num_proc=32):   0%|          | 1416/558128 [00:04<15:57, 581.70 examples/s]Map (num_proc=32):   0%|          | 1758/558128 [00:04<11:29, 806.87 examples/s]Map (num_proc=32):   0%|          | 2194/558128 [00:04<08:20, 1111.33 examples/s]Map (num_proc=32):   0%|          | 2537/558128 [00:04<06:38, 1392.97 examples/s]Map (num_proc=32):   1%|          | 3000/558128 [00:04<05:27, 1693.80 examples/s]Map (num_proc=32):   1%|          | 3484/558128 [00:05<04:30, 2048.42 examples/s]Map (num_proc=32):   1%|          | 3996/558128 [00:05<03:49, 2414.51 examples/s]Map (num_proc=32):   1%|          | 4438/558128 [00:05<03:37, 2550.47 examples/s]Map (num_proc=32):   1%|          | 4783/558128 [00:05<03:22, 2728.20 examples/s]Map (num_proc=32):   1%|          | 5216/558128 [00:05<03:19, 2772.49 examples/s]Map (num_proc=32):   1%|          | 5561/558128 [00:05<03:09, 2921.10 examples/s]Map (num_proc=32):   1%|          | 5905/558128 [00:05<03:09, 2918.31 examples/s]Map (num_proc=32):   1%|          | 6307/558128 [00:05<03:03, 3005.91 examples/s]Map (num_proc=32):   1%|          | 6648/558128 [00:06<02:57, 3104.73 examples/s]Map (num_proc=32):   1%|â–         | 6987/558128 [00:06<02:58, 3095.40 examples/s]Map (num_proc=32):   1%|â–         | 7417/558128 [00:06<03:02, 3012.16 examples/s]Map (num_proc=32):   1%|â–         | 7753/558128 [00:06<02:57, 3096.40 examples/s]Map (num_proc=32):   1%|â–         | 8179/558128 [00:06<03:55, 2331.58 examples/s]Map (num_proc=32):   2%|â–         | 8990/558128 [00:06<02:44, 3334.69 examples/s]Map (num_proc=32):   2%|â–         | 9413/558128 [00:06<02:39, 3449.71 examples/s]Map (num_proc=32):   2%|â–         | 10095/558128 [00:07<02:09, 4231.08 examples/s]Map (num_proc=32):   2%|â–         | 10840/558128 [00:07<01:59, 4574.08 examples/s]Map (num_proc=32):   2%|â–         | 11435/558128 [00:07<01:51, 4908.58 examples/s]Map (num_proc=32):   2%|â–         | 12124/558128 [00:07<01:40, 5417.98 examples/s]Map (num_proc=32):   2%|â–         | 12812/558128 [00:07<01:35, 5725.40 examples/s]Map (num_proc=32):   2%|â–         | 13552/558128 [00:07<01:29, 6092.99 examples/s]Map (num_proc=32):   3%|â–         | 14212/558128 [00:07<01:28, 6141.08 examples/s]Map (num_proc=32):   3%|â–         | 14901/558128 [00:07<01:25, 6346.10 examples/s]Map (num_proc=32):   3%|â–         | 15570/558128 [00:07<01:32, 5853.29 examples/s]Map (num_proc=32):   3%|â–         | 16264/558128 [00:08<01:29, 6057.90 examples/s]Map (num_proc=32):   3%|â–         | 17038/558128 [00:08<01:26, 6246.85 examples/s]Map (num_proc=32):   3%|â–         | 17728/558128 [00:08<01:24, 6422.62 examples/s]Map (num_proc=32):   3%|â–         | 18585/558128 [00:08<01:24, 6371.75 examples/s]Map (num_proc=32):   3%|â–         | 19318/558128 [00:08<01:26, 6193.72 examples/s]Map (num_proc=32):   4%|â–         | 20182/558128 [00:08<01:22, 6505.84 examples/s]Map (num_proc=32):   4%|â–         | 20864/558128 [00:08<01:25, 6295.95 examples/s]Map (num_proc=32):   4%|â–         | 21516/558128 [00:08<01:33, 5766.10 examples/s]Map (num_proc=32):   4%|â–         | 22545/558128 [00:08<01:18, 6802.96 examples/s]Map (num_proc=32):   4%|â–         | 23334/558128 [00:09<01:22, 6446.19 examples/s]Map (num_proc=32):   4%|â–         | 24026/558128 [00:09<01:21, 6561.13 examples/s]Map (num_proc=32):   4%|â–         | 24709/558128 [00:09<01:20, 6624.39 examples/s]Map (num_proc=32):   5%|â–         | 25418/558128 [00:09<01:24, 6298.25 examples/s]Map (num_proc=32):   5%|â–         | 26090/558128 [00:09<01:34, 5617.16 examples/s]Map (num_proc=32):   5%|â–         | 26674/558128 [00:10<03:20, 2653.98 examples/s]Map (num_proc=32):   5%|â–Œ         | 28185/558128 [00:10<02:07, 4145.64 examples/s]Map (num_proc=32):   5%|â–Œ         | 28951/558128 [00:10<02:00, 4401.13 examples/s]Map (num_proc=32):   5%|â–Œ         | 29609/558128 [00:10<01:51, 4742.91 examples/s]Map (num_proc=32):   5%|â–Œ         | 30276/558128 [00:10<01:43, 5110.82 examples/s]Map (num_proc=32):   6%|â–Œ         | 30958/558128 [00:10<01:39, 5290.09 examples/s]Map (num_proc=32):   6%|â–Œ         | 31617/558128 [00:10<01:34, 5592.56 examples/s]Map (num_proc=32):   6%|â–Œ         | 32290/558128 [00:10<01:31, 5750.86 examples/s]Map (num_proc=32):   6%|â–Œ         | 32946/558128 [00:11<01:28, 5941.77 examples/s]Map (num_proc=32):   6%|â–Œ         | 33614/558128 [00:11<01:27, 5990.43 examples/s]Map (num_proc=32):   6%|â–Œ         | 34276/558128 [00:11<01:27, 5989.34 examples/s]Map (num_proc=32):   6%|â–‹         | 34956/558128 [00:11<01:27, 5990.76 examples/s]Map (num_proc=32):   6%|â–‹         | 35809/558128 [00:11<01:21, 6422.46 examples/s]Map (num_proc=32):   7%|â–‹         | 36461/558128 [00:11<01:21, 6403.63 examples/s]Map (num_proc=32):   7%|â–‹         | 37123/558128 [00:11<01:21, 6363.84 examples/s]Map (num_proc=32):   7%|â–‹         | 37802/558128 [00:12<02:07, 4086.01 examples/s]Map (num_proc=32):   7%|â–‹         | 39115/558128 [00:12<01:30, 5761.61 examples/s]Map (num_proc=32):   7%|â–‹         | 40351/558128 [00:12<01:12, 7185.73 examples/s]Map (num_proc=32):   7%|â–‹         | 41318/558128 [00:12<01:20, 6418.03 examples/s]Map (num_proc=32):   8%|â–Š         | 42134/558128 [00:12<01:58, 4360.53 examples/s]Map (num_proc=32):   8%|â–Š         | 42884/558128 [00:12<01:56, 4425.94 examples/s]Map (num_proc=32):   8%|â–Š         | 43545/558128 [00:13<02:03, 4172.00 examples/s]Map (num_proc=32):   8%|â–Š         | 44105/558128 [00:13<02:19, 3692.42 examples/s]Map (num_proc=32):   8%|â–Š         | 44601/558128 [00:13<02:22, 3595.79 examples/s]Map (num_proc=32):   8%|â–Š         | 45047/558128 [00:13<02:19, 3685.32 examples/s]Map (num_proc=32):   8%|â–Š         | 45725/558128 [00:13<01:58, 4327.20 examples/s]Map (num_proc=32):   8%|â–Š         | 46399/558128 [00:13<01:44, 4879.85 examples/s]Map (num_proc=32):   8%|â–Š         | 47057/558128 [00:13<01:41, 5044.40 examples/s]Map (num_proc=32):   9%|â–Š         | 47717/558128 [00:14<01:33, 5431.96 examples/s]Map (num_proc=32):   9%|â–Š         | 48389/558128 [00:14<01:28, 5770.51 examples/s]Map (num_proc=32):   9%|â–‰         | 49056/558128 [00:14<01:31, 5569.38 examples/s]Map (num_proc=32):   9%|â–‰         | 49737/558128 [00:14<01:26, 5896.04 examples/s]Map (num_proc=32):   9%|â–‰         | 50562/558128 [00:14<01:25, 5932.71 examples/s]Map (num_proc=32):   9%|â–‰         | 51220/558128 [00:14<01:23, 6101.40 examples/s]Map (num_proc=32):   9%|â–‰         | 51991/558128 [00:14<01:21, 6225.12 examples/s]Map (num_proc=32):   9%|â–‰         | 52643/558128 [00:14<01:21, 6214.37 examples/s]Map (num_proc=32):  10%|â–‰         | 53380/558128 [00:15<02:04, 4065.36 examples/s]Map (num_proc=32):  10%|â–‰         | 55005/558128 [00:15<01:18, 6428.97 examples/s]Map (num_proc=32):  10%|â–ˆ         | 55980/558128 [00:15<01:13, 6820.59 examples/s]Map (num_proc=32):  10%|â–ˆ         | 56911/558128 [00:15<01:18, 6383.92 examples/s]Map (num_proc=32):  10%|â–ˆ         | 57915/558128 [00:15<01:12, 6858.71 examples/s]Map (num_proc=32):  11%|â–ˆ         | 58733/558128 [00:15<01:12, 6905.25 examples/s]Map (num_proc=32):  11%|â–ˆ         | 59558/558128 [00:15<01:19, 6305.62 examples/s]Map (num_proc=32):  11%|â–ˆ         | 60308/558128 [00:16<01:23, 5956.52 examples/s]Map (num_proc=32):  11%|â–ˆ         | 61014/558128 [00:16<03:21, 2461.37 examples/s]Map (num_proc=32):  11%|â–ˆ         | 62485/558128 [00:16<02:08, 3860.36 examples/s]Map (num_proc=32):  11%|â–ˆâ–        | 63772/558128 [00:17<01:36, 5112.44 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 64840/558128 [00:17<01:24, 5867.93 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 65870/558128 [00:17<01:19, 6229.11 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 66835/558128 [00:17<01:21, 6056.86 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 67697/558128 [00:17<01:18, 6270.55 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 68496/558128 [00:17<01:19, 6141.36 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 69358/558128 [00:17<01:19, 6172.46 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 70179/558128 [00:18<01:16, 6353.98 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 71007/558128 [00:18<02:03, 3949.73 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 72496/558128 [00:18<01:25, 5708.38 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 73979/558128 [00:18<01:06, 7277.45 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 75102/558128 [00:18<01:13, 6586.45 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 76267/558128 [00:18<01:08, 7071.73 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 77212/558128 [00:19<01:10, 6847.53 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 78103/558128 [00:19<01:54, 4189.07 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 79103/558128 [00:19<01:40, 4754.15 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 79745/558128 [00:19<01:51, 4278.76 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 80332/558128 [00:20<01:58, 4042.80 examples/s]Map (num_proc=32):  15%|â–ˆâ–        | 81186/558128 [00:20<01:43, 4618.42 examples/s]Map (num_proc=32):  15%|â–ˆâ–        | 81803/558128 [00:20<01:36, 4915.69 examples/s]Map (num_proc=32):  15%|â–ˆâ–        | 82644/558128 [00:20<01:30, 5226.08 examples/s]Map (num_proc=32):  15%|â–ˆâ–        | 83304/558128 [00:20<01:25, 5526.72 examples/s]Map (num_proc=32):  15%|â–ˆâ–Œ        | 83937/558128 [00:20<01:25, 5553.54 examples/s]Map (num_proc=32):  15%|â–ˆâ–Œ        | 84628/558128 [00:20<01:20, 5885.70 examples/s]Map (num_proc=32):  15%|â–ˆâ–Œ        | 85315/558128 [00:20<01:18, 6060.28 examples/s]Map (num_proc=32):  15%|â–ˆâ–Œ        | 86110/558128 [00:21<01:16, 6196.54 examples/s]Map (num_proc=32):  16%|â–ˆâ–Œ        | 86797/558128 [00:21<01:13, 6374.31 examples/s]Map (num_proc=32):  16%|â–ˆâ–Œ        | 87457/558128 [00:21<01:14, 6281.44 examples/s]Map (num_proc=32):  16%|â–ˆâ–Œ        | 88111/558128 [00:21<01:14, 6346.08 examples/s]Map (num_proc=32):  16%|â–ˆâ–Œ        | 88797/558128 [00:21<01:14, 6268.91 examples/s]Map (num_proc=32):  16%|â–ˆâ–Œ        | 89627/558128 [00:21<02:14, 3482.93 examples/s]Map (num_proc=32):  16%|â–ˆâ–‹        | 91235/558128 [00:21<01:24, 5542.32 examples/s]Map (num_proc=32):  17%|â–ˆâ–‹        | 92683/558128 [00:22<01:04, 7222.76 examples/s]Map (num_proc=32):  17%|â–ˆâ–‹        | 93683/558128 [00:22<01:05, 7060.82 examples/s]Map (num_proc=32):  17%|â–ˆâ–‹        | 94729/558128 [00:22<01:08, 6802.62 examples/s]Map (num_proc=32):  17%|â–ˆâ–‹        | 95600/558128 [00:22<01:33, 4941.84 examples/s]Map (num_proc=32):  17%|â–ˆâ–‹        | 96376/558128 [00:22<01:47, 4309.05 examples/s]Map (num_proc=32):  17%|â–ˆâ–‹        | 97045/558128 [00:23<01:53, 4062.86 examples/s]Map (num_proc=32):  17%|â–ˆâ–‹        | 97548/558128 [00:23<02:02, 3765.55 examples/s]Map (num_proc=32):  18%|â–ˆâ–Š        | 98389/558128 [00:23<01:42, 4468.59 examples/s]Map (num_proc=32):  18%|â–ˆâ–Š        | 99049/558128 [00:23<01:34, 4868.41 examples/s]Map (num_proc=32):  18%|â–ˆâ–Š        | 99651/558128 [00:23<01:33, 4926.79 examples/s]Map (num_proc=32):  18%|â–ˆâ–Š        | 100319/558128 [00:23<01:25, 5330.42 examples/s]Map (num_proc=32):  18%|â–ˆâ–Š        | 101002/558128 [00:23<01:20, 5697.34 examples/s]Map (num_proc=32):  18%|â–ˆâ–Š        | 101762/558128 [00:23<01:18, 5780.14 examples/s]Map (num_proc=32):  18%|â–ˆâ–Š        | 102449/558128 [00:24<01:15, 6059.40 examples/s]Map (num_proc=32):  18%|â–ˆâ–Š        | 103233/558128 [00:24<01:17, 5845.96 examples/s]Map (num_proc=32):  19%|â–ˆâ–Š        | 104070/558128 [00:24<01:11, 6339.68 examples/s]Map (num_proc=32):  19%|â–ˆâ–‰        | 104930/558128 [00:24<01:13, 6196.79 examples/s]Map (num_proc=32):  19%|â–ˆâ–‰        | 105719/558128 [00:24<01:11, 6297.91 examples/s]Map (num_proc=32):  19%|â–ˆâ–‰        | 106562/558128 [00:24<01:12, 6205.83 examples/s]Map (num_proc=32):  19%|â–ˆâ–‰        | 107206/558128 [00:25<01:51, 4046.62 examples/s]Map (num_proc=32):  20%|â–ˆâ–‰        | 108849/558128 [00:25<01:11, 6309.67 examples/s]Map (num_proc=32):  20%|â–ˆâ–‰        | 109946/558128 [00:25<01:02, 7145.32 examples/s]Map (num_proc=32):  20%|â–ˆâ–‰        | 110949/558128 [00:25<01:04, 6938.03 examples/s]Map (num_proc=32):  20%|â–ˆâ–ˆ        | 111827/558128 [00:25<01:05, 6778.66 examples/s]Map (num_proc=32):  20%|â–ˆâ–ˆ        | 112627/558128 [00:25<01:41, 4376.68 examples/s]Map (num_proc=32):  20%|â–ˆâ–ˆ        | 113355/558128 [00:26<01:33, 4753.18 examples/s]Map (num_proc=32):  20%|â–ˆâ–ˆ        | 113984/558128 [00:26<01:46, 4153.33 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆ        | 114652/558128 [00:26<01:57, 3771.38 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆ        | 115326/558128 [00:26<01:43, 4292.26 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆ        | 116017/558128 [00:26<01:31, 4815.14 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆ        | 116652/558128 [00:26<01:29, 4941.39 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆ        | 117340/558128 [00:26<01:21, 5392.63 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆ        | 118009/558128 [00:27<01:17, 5712.44 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆâ–       | 118652/558128 [00:27<01:17, 5671.95 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆâ–       | 119342/558128 [00:27<01:13, 5990.21 examples/s]Map (num_proc=32):  22%|â–ˆâ–ˆâ–       | 120208/558128 [00:27<01:07, 6482.79 examples/s]Map (num_proc=32):  22%|â–ˆâ–ˆâ–       | 120992/558128 [00:27<01:11, 6122.99 examples/s]Map (num_proc=32):  22%|â–ˆâ–ˆâ–       | 121685/558128 [00:27<01:09, 6243.57 examples/s]Map (num_proc=32):  22%|â–ˆâ–ˆâ–       | 122508/558128 [00:27<01:08, 6393.07 examples/s]Map (num_proc=32):  22%|â–ˆâ–ˆâ–       | 123167/558128 [00:27<01:07, 6435.73 examples/s]Map (num_proc=32):  22%|â–ˆâ–ˆâ–       | 123859/558128 [00:27<01:07, 6479.03 examples/s]Map (num_proc=32):  22%|â–ˆâ–ˆâ–       | 124652/558128 [00:28<01:10, 6146.56 examples/s]Map (num_proc=32):  22%|â–ˆâ–ˆâ–       | 125330/558128 [00:28<01:50, 3904.78 examples/s]Map (num_proc=32):  23%|â–ˆâ–ˆâ–       | 126652/558128 [00:28<01:16, 5621.36 examples/s]Map (num_proc=32):  23%|â–ˆâ–ˆâ–       | 127508/558128 [00:28<01:20, 5382.20 examples/s]Map (num_proc=32):  23%|â–ˆâ–ˆâ–       | 129329/558128 [00:28<00:54, 7849.06 examples/s]Map (num_proc=32):  23%|â–ˆâ–ˆâ–       | 130373/558128 [00:29<01:27, 4901.41 examples/s]Map (num_proc=32):  23%|â–ˆâ–ˆâ–       | 131125/558128 [00:29<01:22, 5166.65 examples/s]Map (num_proc=32):  24%|â–ˆâ–ˆâ–       | 131970/558128 [00:29<01:33, 4565.01 examples/s]Map (num_proc=32):  24%|â–ˆâ–ˆâ–       | 132598/558128 [00:29<01:43, 4093.90 examples/s]Map (num_proc=32):  24%|â–ˆâ–ˆâ–       | 133176/558128 [00:29<01:54, 3707.02 examples/s]Map (num_proc=32):  24%|â–ˆâ–ˆâ–       | 133850/558128 [00:30<01:42, 4153.81 examples/s]Map (num_proc=32):  24%|â–ˆâ–ˆâ–       | 134531/558128 [00:30<01:31, 4649.01 examples/s]Map (num_proc=32):  24%|â–ˆâ–ˆâ–       | 135117/558128 [00:30<01:29, 4730.45 examples/s]Map (num_proc=32):  24%|â–ˆâ–ˆâ–       | 135810/558128 [00:30<01:20, 5229.69 examples/s]Map (num_proc=32):  24%|â–ˆâ–ˆâ–       | 136673/558128 [00:30<01:13, 5747.61 examples/s]Map (num_proc=32):  25%|â–ˆâ–ˆâ–       | 137407/558128 [00:30<01:14, 5630.73 examples/s]Map (num_proc=32):  25%|â–ˆâ–ˆâ–       | 138267/558128 [00:30<01:08, 6100.33 examples/s]Map (num_proc=32):  25%|â–ˆâ–ˆâ–       | 138929/558128 [00:30<01:09, 6055.65 examples/s]Map (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 139601/558128 [00:31<01:07, 6217.07 examples/s]Map (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 140292/558128 [00:31<01:05, 6401.49 examples/s]Map (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 140960/558128 [00:31<01:06, 6256.19 examples/s]Map (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 141606/558128 [00:31<01:06, 6306.63 examples/s]Map (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 142295/558128 [00:31<01:04, 6466.35 examples/s]Map (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 142959/558128 [00:31<01:05, 6292.34 examples/s]Map (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 143779/558128 [00:31<01:06, 6203.28 examples/s]Map (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 144468/558128 [00:31<01:43, 3988.87 examples/s]Map (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 146392/558128 [00:32<01:02, 6569.59 examples/s]Map (num_proc=32):  26%|â–ˆâ–ˆâ–‹       | 147336/558128 [00:32<01:24, 4880.08 examples/s]Map (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 148105/558128 [00:32<01:37, 4208.98 examples/s]Map (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 148700/558128 [00:32<01:44, 3918.75 examples/s]Map (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 149177/558128 [00:33<01:49, 3746.63 examples/s]Map (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 149666/558128 [00:33<01:46, 3852.46 examples/s]Map (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 150445/558128 [00:33<01:34, 4326.49 examples/s]Map (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 151135/558128 [00:33<01:23, 4860.53 examples/s]Map (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 151915/558128 [00:33<01:18, 5154.02 examples/s]Map (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 152575/558128 [00:33<01:13, 5489.48 examples/s]Map (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 153257/558128 [00:33<01:12, 5579.31 examples/s]Map (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 153881/558128 [00:33<01:10, 5740.17 examples/s]Map (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 154558/558128 [00:33<01:07, 6012.16 examples/s]Map (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 155244/558128 [00:34<01:04, 6201.93 examples/s]Map (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 156047/558128 [00:34<01:03, 6333.43 examples/s]Map (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 156911/558128 [00:34<01:03, 6273.76 examples/s]Map (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 157564/558128 [00:34<01:04, 6225.97 examples/s]Map (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 158222/558128 [00:34<01:04, 6232.02 examples/s]Map (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 158909/558128 [00:34<01:02, 6403.17 examples/s]Map (num_proc=32):  29%|â–ˆâ–ˆâ–Š       | 159638/558128 [00:34<01:03, 6271.63 examples/s]Map (num_proc=32):  29%|â–ˆâ–ˆâ–Š       | 160326/558128 [00:34<01:07, 5905.03 examples/s]Map (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 161356/558128 [00:34<00:57, 6923.02 examples/s]Map (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 162107/558128 [00:35<01:00, 6561.38 examples/s]Map (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 162937/558128 [00:35<01:43, 3828.41 examples/s]Map (num_proc=32):  30%|â–ˆâ–ˆâ–‰       | 164663/558128 [00:35<01:04, 6068.44 examples/s]Map (num_proc=32):  30%|â–ˆâ–ˆâ–‰       | 165576/558128 [00:36<02:01, 3219.94 examples/s]Map (num_proc=32):  30%|â–ˆâ–ˆâ–‰       | 167352/558128 [00:36<01:19, 4891.22 examples/s]Map (num_proc=32):  30%|â–ˆâ–ˆâ–ˆ       | 169005/558128 [00:36<00:59, 6501.08 examples/s]Map (num_proc=32):  31%|â–ˆâ–ˆâ–ˆ       | 170247/558128 [00:36<01:00, 6365.05 examples/s]Map (num_proc=32):  31%|â–ˆâ–ˆâ–ˆ       | 171274/558128 [00:36<00:59, 6527.26 examples/s]Map (num_proc=32):  31%|â–ˆâ–ˆâ–ˆ       | 172177/558128 [00:37<01:00, 6337.33 examples/s]Map (num_proc=32):  31%|â–ˆâ–ˆâ–ˆ       | 173042/558128 [00:37<01:00, 6417.77 examples/s]Map (num_proc=32):  31%|â–ˆâ–ˆâ–ˆ       | 173838/558128 [00:37<00:59, 6458.76 examples/s]Map (num_proc=32):  31%|â–ˆâ–ˆâ–ˆâ–      | 174663/558128 [00:37<01:01, 6231.55 examples/s]Map (num_proc=32):  31%|â–ˆâ–ˆâ–ˆâ–      | 175350/558128 [00:37<01:00, 6370.57 examples/s]Map (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 176146/558128 [00:37<01:19, 4775.46 examples/s]Map (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 177681/558128 [00:37<00:55, 6806.27 examples/s]Map (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 178637/558128 [00:38<00:56, 6686.08 examples/s]Map (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 179494/558128 [00:38<01:00, 6244.77 examples/s]Map (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 180488/558128 [00:38<00:54, 6926.55 examples/s]Map (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 181339/558128 [00:38<00:58, 6441.83 examples/s]Map (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–      | 182146/558128 [00:38<00:56, 6614.70 examples/s]Map (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–      | 182935/558128 [00:38<00:58, 6383.65 examples/s]Map (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–      | 183723/558128 [00:39<02:53, 2157.13 examples/s]Map (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–      | 185306/558128 [00:39<01:48, 3444.03 examples/s]Map (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–      | 186761/558128 [00:39<01:17, 4777.67 examples/s]Map (num_proc=32):  34%|â–ˆâ–ˆâ–ˆâ–      | 188135/558128 [00:40<01:01, 6064.16 examples/s]Map (num_proc=32):  34%|â–ˆâ–ˆâ–ˆâ–      | 189250/558128 [00:40<01:01, 5995.55 examples/s]Map (num_proc=32):  34%|â–ˆâ–ˆâ–ˆâ–      | 190277/558128 [00:40<01:01, 5946.71 examples/s]Map (num_proc=32):  34%|â–ˆâ–ˆâ–ˆâ–      | 191242/558128 [00:40<01:00, 6098.45 examples/s]Map (num_proc=32):  34%|â–ˆâ–ˆâ–ˆâ–      | 192101/558128 [00:40<00:56, 6511.04 examples/s]Map (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–      | 193037/558128 [00:40<01:13, 4979.25 examples/s]Map (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–      | 194648/558128 [00:41<00:54, 6658.10 examples/s]Map (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 195512/558128 [00:41<00:54, 6713.51 examples/s]Map (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 196335/558128 [00:41<00:56, 6421.26 examples/s]Map (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 197294/558128 [00:41<00:54, 6670.11 examples/s]Map (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 198105/558128 [00:41<00:53, 6747.53 examples/s]Map (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 198858/558128 [00:41<00:59, 6066.22 examples/s]Map (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 199719/558128 [00:41<01:00, 5947.82 examples/s]Map (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 200471/558128 [00:42<01:22, 4314.35 examples/s]Map (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 201080/558128 [00:42<01:24, 4201.20 examples/s]Map (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 201560/558128 [00:42<01:30, 3939.44 examples/s]Map (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 202183/558128 [00:42<01:21, 4343.37 examples/s]Map (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–‹      | 202865/558128 [00:42<01:12, 4882.57 examples/s]Map (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–‹      | 203550/558128 [00:42<01:07, 5220.90 examples/s]Map (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 204457/558128 [00:42<01:04, 5509.25 examples/s]Map (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 205146/558128 [00:43<01:00, 5839.14 examples/s]Map (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 205933/558128 [00:43<00:56, 6234.87 examples/s]Map (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 206746/558128 [00:43<00:58, 6047.38 examples/s]Map (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 207372/558128 [00:43<00:57, 6053.78 examples/s]Map (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 208044/558128 [00:43<00:56, 6225.91 examples/s]Map (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 208703/558128 [00:43<00:55, 6324.34 examples/s]Map (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 209380/558128 [00:43<00:56, 6164.88 examples/s]Map (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 210065/558128 [00:43<00:54, 6349.04 examples/s]Map (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 210876/558128 [00:44<01:17, 4457.53 examples/s]Map (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 212545/558128 [00:44<00:50, 6883.79 examples/s]Map (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 213533/558128 [00:44<00:49, 6946.25 examples/s]Map (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 214441/558128 [00:44<00:51, 6645.11 examples/s]Map (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 215296/558128 [00:44<00:50, 6757.17 examples/s]Map (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 216041/558128 [00:44<00:55, 6166.66 examples/s]Map (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 216898/558128 [00:44<00:51, 6581.66 examples/s]Map (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 217631/558128 [00:45<01:24, 4033.46 examples/s]Map (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 218469/558128 [00:45<01:15, 4473.38 examples/s]Map (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 219139/558128 [00:45<01:22, 4115.36 examples/s]Map (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 219639/558128 [00:45<01:29, 3765.53 examples/s]Map (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 220138/558128 [00:45<01:31, 3681.18 examples/s]Map (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–‰      | 220803/558128 [00:46<01:23, 4055.02 examples/s]Map (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–‰      | 221664/558128 [00:46<01:07, 4988.55 examples/s]Map (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–‰      | 222328/558128 [00:46<01:05, 5117.89 examples/s]Map (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–‰      | 222989/558128 [00:46<01:02, 5404.46 examples/s]Map (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 223650/558128 [00:46<00:58, 5705.18 examples/s]Map (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 224316/558128 [00:46<00:57, 5767.76 examples/s]Map (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 224986/558128 [00:46<00:56, 5908.74 examples/s]Map (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 225844/558128 [00:46<00:55, 5976.43 examples/s]Map (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 226476/558128 [00:47<01:27, 3782.15 examples/s]Map (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 227983/558128 [00:47<00:56, 5870.29 examples/s]Map (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 229332/558128 [00:47<00:44, 7471.46 examples/s]Map (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 230323/558128 [00:47<00:44, 7365.28 examples/s]Map (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 231330/558128 [00:47<00:46, 6973.12 examples/s]Map (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 232151/558128 [00:47<00:51, 6385.39 examples/s]Map (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 232987/558128 [00:47<00:49, 6594.84 examples/s]Map (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 233746/558128 [00:48<00:53, 6076.11 examples/s]Map (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 234469/558128 [00:49<02:25, 2228.81 examples/s]Map (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 236246/558128 [00:49<01:24, 3797.11 examples/s]Map (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 237857/558128 [00:49<00:59, 5342.46 examples/s]Map (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 239299/558128 [00:49<00:47, 6691.22 examples/s]Map (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 240481/558128 [00:49<00:42, 7523.68 examples/s]Map (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 241782/558128 [00:49<00:44, 7134.35 examples/s]Map (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 242784/558128 [00:49<00:45, 6978.36 examples/s]Map (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 243780/558128 [00:49<00:45, 6847.58 examples/s]Map (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 244749/558128 [00:50<01:02, 4988.55 examples/s]Map (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 246263/558128 [00:50<00:47, 6622.54 examples/s]Map (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 247261/558128 [00:50<00:47, 6510.79 examples/s]Map (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 248253/558128 [00:50<00:44, 6957.25 examples/s]Map (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 249090/558128 [00:50<00:43, 7049.91 examples/s]Map (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 249948/558128 [00:50<00:45, 6754.81 examples/s]Map (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 250755/558128 [00:51<00:46, 6640.81 examples/s]Map (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 251595/558128 [00:51<00:47, 6443.90 examples/s]Map (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 252426/558128 [00:51<00:46, 6540.16 examples/s]Map (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 253188/558128 [00:51<00:48, 6224.93 examples/s]Map (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 253852/558128 [00:51<01:19, 3842.71 examples/s]Map (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 254896/558128 [00:51<01:01, 4952.52 examples/s]Map (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 255577/558128 [00:52<00:58, 5216.21 examples/s]Map (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 256357/558128 [00:52<00:57, 5282.91 examples/s]Map (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 257048/558128 [00:52<00:54, 5573.55 examples/s]Map (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 257713/558128 [00:52<00:51, 5823.66 examples/s]Map (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 258358/558128 [00:52<00:52, 5732.33 examples/s]Map (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 259222/558128 [00:52<00:47, 6268.36 examples/s]Map (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 259880/558128 [00:52<00:47, 6251.59 examples/s]Map (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 260534/558128 [00:52<00:47, 6236.24 examples/s]Map (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 261224/558128 [00:52<00:46, 6409.06 examples/s]Map (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 261878/558128 [00:53<00:47, 6288.41 examples/s]Map (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 262530/558128 [00:53<00:46, 6348.38 examples/s]Map (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 263215/558128 [00:53<01:06, 4462.66 examples/s]Map (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 264634/558128 [00:53<00:44, 6546.14 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 265669/558128 [00:53<00:40, 7159.36 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 266495/558128 [00:53<00:42, 6884.76 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 267281/558128 [00:53<00:43, 6652.40 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 268106/558128 [00:54<00:45, 6319.39 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 268807/558128 [00:54<00:45, 6417.99 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 269595/558128 [00:55<02:16, 2106.13 examples/s]Map (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 271296/558128 [00:55<01:19, 3598.77 examples/s]Map (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 273114/558128 [00:55<00:53, 5374.28 examples/s]Map (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 274625/558128 [00:55<00:41, 6790.26 examples/s]Map (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 275926/558128 [00:55<00:37, 7507.22 examples/s]Map (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 277182/558128 [00:55<00:39, 7044.85 examples/s]Map (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 278218/558128 [00:55<00:39, 7067.75 examples/s]Map (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 279284/558128 [00:56<00:43, 6476.07 examples/s]Map (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 280146/558128 [00:56<00:41, 6750.25 examples/s]Map (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 280970/558128 [00:56<00:55, 5031.22 examples/s]Map (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 282173/558128 [00:56<00:46, 5984.70 examples/s]Map (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 283660/558128 [00:56<00:38, 7174.93 examples/s]Map (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 284630/558128 [00:56<00:41, 6653.56 examples/s]Map (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 285448/558128 [00:57<00:39, 6846.15 examples/s]Map (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 286294/558128 [00:57<00:40, 6683.24 examples/s]Map (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 287050/558128 [00:57<00:41, 6481.93 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 287830/558128 [00:57<00:42, 6318.56 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 288547/558128 [00:57<01:02, 4302.12 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 289384/558128 [00:57<00:53, 5034.58 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 290143/558128 [00:58<00:51, 5206.68 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 290813/558128 [00:58<00:48, 5526.84 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 291576/558128 [00:58<00:48, 5525.61 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 292267/558128 [00:58<00:45, 5846.97 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 292938/558128 [00:58<00:45, 5879.27 examples/s]Map (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 293590/558128 [00:58<00:44, 5968.52 examples/s]Map (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 294447/558128 [00:58<00:40, 6481.78 examples/s]Map (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 295276/558128 [00:58<00:42, 6119.83 examples/s]Map (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 295932/558128 [00:58<00:42, 6229.39 examples/s]Map (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 296591/558128 [00:59<00:42, 6174.51 examples/s]Map (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 297276/558128 [00:59<00:41, 6352.56 examples/s]Map (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 298074/558128 [00:59<00:40, 6486.66 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 298753/558128 [00:59<00:40, 6386.27 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 299415/558128 [00:59<00:40, 6415.25 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 300277/558128 [00:59<00:39, 6520.97 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 300931/558128 [00:59<00:40, 6335.55 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 301587/558128 [00:59<00:40, 6306.80 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 302267/558128 [01:00<00:55, 4577.41 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 303749/558128 [01:00<00:37, 6759.99 examples/s]Map (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 304678/558128 [01:00<00:41, 6090.20 examples/s]Map (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 305465/558128 [01:01<01:46, 2365.74 examples/s]Map (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 306919/558128 [01:01<01:09, 3601.93 examples/s]Map (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 308349/558128 [01:01<00:50, 4944.39 examples/s]Map (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 310200/558128 [01:01<00:35, 6934.00 examples/s]Map (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 311854/558128 [01:01<00:28, 8513.89 examples/s]Map (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 313241/558128 [01:01<00:29, 8250.58 examples/s]Map (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 314518/558128 [01:02<00:32, 7518.68 examples/s]Map (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 315513/558128 [01:02<00:40, 5954.63 examples/s]Map (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 317052/558128 [01:02<00:33, 7248.97 examples/s]Map (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 318027/558128 [01:02<00:34, 6993.89 examples/s]Map (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 318885/558128 [01:02<00:33, 7243.73 examples/s]Map (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 319852/558128 [01:02<00:34, 6850.66 examples/s]Map (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 320712/558128 [01:03<00:36, 6527.41 examples/s]Map (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 321513/558128 [01:03<00:38, 6198.06 examples/s]Map (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 322208/558128 [01:03<00:37, 6316.33 examples/s]Map (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 322883/558128 [01:03<00:56, 4194.04 examples/s]Map (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 323603/558128 [01:03<00:50, 4619.13 examples/s]Map (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 324191/558128 [01:03<00:57, 4086.55 examples/s]Map (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 324858/558128 [01:04<00:54, 4316.99 examples/s]Map (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 325545/558128 [01:04<00:48, 4841.13 examples/s]Map (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 326147/558128 [01:04<00:47, 4892.03 examples/s]Map (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 326839/558128 [01:04<00:43, 5373.39 examples/s]Map (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 327529/558128 [01:04<00:40, 5742.40 examples/s]Map (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 328159/558128 [01:04<00:39, 5796.11 examples/s]Map (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 328814/558128 [01:04<00:38, 5916.19 examples/s]Map (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 329504/558128 [01:04<00:37, 6056.46 examples/s]Map (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 330162/558128 [01:04<00:37, 6143.10 examples/s]Map (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 330811/558128 [01:05<00:36, 6236.79 examples/s]Map (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 331500/558128 [01:05<00:35, 6384.21 examples/s]Map (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 332153/558128 [01:05<00:57, 3921.13 examples/s]Map (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 333645/558128 [01:05<00:36, 6129.93 examples/s]Map (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 334812/558128 [01:05<00:30, 7349.84 examples/s]Map (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 335816/558128 [01:05<00:30, 7173.69 examples/s]Map (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 336813/558128 [01:05<00:32, 6887.62 examples/s]Map (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 337643/558128 [01:06<00:32, 6825.35 examples/s]Map (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 338395/558128 [01:06<00:33, 6553.63 examples/s]Map (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 339196/558128 [01:07<01:40, 2177.27 examples/s]Map (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 340559/558128 [01:07<01:06, 3286.61 examples/s]Map (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 342144/558128 [01:07<00:45, 4773.46 examples/s]Map (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 343611/558128 [01:07<00:34, 6201.59 examples/s]Map (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 344792/558128 [01:07<00:32, 6509.45 examples/s]Map (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 345909/558128 [01:07<00:32, 6448.73 examples/s]Map (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 346947/558128 [01:08<00:31, 6618.43 examples/s]Map (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 347909/558128 [01:08<00:32, 6529.27 examples/s]Map (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 348775/558128 [01:08<00:33, 6274.39 examples/s]Map (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 349565/558128 [01:08<00:42, 4938.34 examples/s]Map (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 351221/558128 [01:08<00:30, 6855.46 examples/s]Map (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 352179/558128 [01:08<00:30, 6808.72 examples/s]Map (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 353035/558128 [01:08<00:30, 6687.07 examples/s]Map (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 353796/558128 [01:09<00:32, 6358.30 examples/s]Map (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 354659/558128 [01:09<00:31, 6536.54 examples/s]Map (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 355482/558128 [01:09<00:31, 6364.18 examples/s]Map (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 356236/558128 [01:09<00:31, 6451.13 examples/s]Map (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 356995/558128 [01:10<01:35, 2111.92 examples/s]Map (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 358378/558128 [01:10<01:01, 3262.03 examples/s]Map (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 359824/558128 [01:10<00:42, 4628.36 examples/s]Map (num_proc=32):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 361465/558128 [01:10<00:31, 6312.06 examples/s]Map (num_proc=32):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 363081/558128 [01:10<00:24, 7914.88 examples/s]Map (num_proc=32):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 364500/558128 [01:11<00:22, 8593.65 examples/s]Map (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 365691/558128 [01:11<00:23, 8146.96 examples/s]Map (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 366858/558128 [01:11<00:35, 5423.00 examples/s]Map (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 368692/558128 [01:11<00:25, 7361.44 examples/s]Map (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 369833/558128 [01:11<00:25, 7394.13 examples/s]Map (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 370961/558128 [01:12<00:25, 7365.68 examples/s]Map (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 371964/558128 [01:12<00:26, 6904.50 examples/s]Map (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 372931/558128 [01:12<00:27, 6769.31 examples/s]Map (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 373749/558128 [01:12<00:27, 6619.40 examples/s]Map (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 374508/558128 [01:12<00:28, 6349.73 examples/s]Map (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 375277/558128 [01:12<00:32, 5648.29 examples/s]Map (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 376004/558128 [01:13<01:16, 2384.77 examples/s]Map (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 377616/558128 [01:13<00:47, 3821.36 examples/s]Map (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 379054/558128 [01:13<00:34, 5211.57 examples/s]Map (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 380685/558128 [01:13<00:25, 6936.15 examples/s]Map (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 382100/558128 [01:14<00:21, 8216.47 examples/s]Map (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 383386/558128 [01:14<00:21, 8312.86 examples/s]Map (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 384560/558128 [01:14<00:23, 7422.85 examples/s]Map (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 385508/558128 [01:14<00:33, 5200.83 examples/s]Map (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 387304/558128 [01:14<00:23, 7127.16 examples/s]Map (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 388469/558128 [01:14<00:22, 7479.09 examples/s]Map (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 389476/558128 [01:15<00:23, 7086.58 examples/s]Map (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 390468/558128 [01:15<00:24, 6826.46 examples/s]Map (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 391290/558128 [01:15<00:24, 6763.25 examples/s]Map (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 392052/558128 [01:15<00:27, 6115.17 examples/s]Map (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 392755/558128 [01:15<00:36, 4487.82 examples/s]Map (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 393425/558128 [01:16<00:37, 4446.95 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 394223/558128 [01:16<00:32, 4975.91 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 394899/558128 [01:16<00:31, 5253.17 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 395568/558128 [01:16<00:29, 5542.43 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 396236/558128 [01:16<00:27, 5809.68 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 396912/558128 [01:16<00:27, 5899.59 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 397576/558128 [01:16<00:26, 5973.85 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 398230/558128 [01:16<00:26, 6115.48 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 398917/558128 [01:16<00:25, 6232.29 examples/s]Map (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 399575/558128 [01:16<00:25, 6210.69 examples/s]Map (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 400226/558128 [01:17<00:25, 6131.62 examples/s]Map (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 401083/558128 [01:17<00:26, 5961.70 examples/s]Map (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 401915/558128 [01:17<00:24, 6408.32 examples/s]Map (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 402743/558128 [01:17<00:24, 6235.47 examples/s]Map (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 403401/558128 [01:17<00:24, 6218.20 examples/s]Map (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 404088/558128 [01:17<00:35, 4341.41 examples/s]Map (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 406079/558128 [01:18<00:21, 6998.23 examples/s]Map (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 406912/558128 [01:18<00:21, 7187.18 examples/s]Map (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 407740/558128 [01:18<00:22, 6591.10 examples/s]Map (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 408565/558128 [01:18<00:22, 6601.73 examples/s]Map (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 409322/558128 [01:18<00:24, 6003.98 examples/s]Map (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 410105/558128 [01:19<00:57, 2571.88 examples/s]Map (num_proc=32):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 411493/558128 [01:19<00:37, 3860.61 examples/s]Map (num_proc=32):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 413002/558128 [01:19<00:26, 5419.48 examples/s]Map (num_proc=32):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 414353/558128 [01:19<00:21, 6766.61 examples/s]Map (num_proc=32):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 415506/558128 [01:19<00:21, 6607.59 examples/s]Map (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 416530/558128 [01:19<00:21, 6612.43 examples/s]Map (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 417503/558128 [01:20<00:21, 6413.44 examples/s]Map (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 418362/558128 [01:20<00:21, 6575.80 examples/s]Map (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 419182/558128 [01:20<00:21, 6511.68 examples/s]Map (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 419996/558128 [01:20<00:21, 6450.12 examples/s]Map (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 420841/558128 [01:20<00:21, 6407.20 examples/s]Map (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 421678/558128 [01:20<00:23, 5860.19 examples/s]Map (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 422672/558128 [01:20<00:20, 6605.11 examples/s]Map (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 423505/558128 [01:21<00:20, 6477.84 examples/s]Map (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 424186/558128 [01:21<00:20, 6395.56 examples/s]Map (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 425009/558128 [01:21<00:26, 5065.15 examples/s]Map (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 426341/558128 [01:21<00:19, 6655.11 examples/s]Map (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 427259/558128 [01:21<00:29, 4471.17 examples/s]Map (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 427993/558128 [01:22<00:27, 4781.68 examples/s]Map (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 428666/558128 [01:22<00:25, 5090.46 examples/s]Map (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 429443/558128 [01:22<00:24, 5211.98 examples/s]Map (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 430089/558128 [01:22<00:23, 5424.16 examples/s]Map (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 430778/558128 [01:22<00:22, 5763.83 examples/s]Map (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 431459/558128 [01:22<00:21, 5818.80 examples/s]Map (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 432115/558128 [01:22<00:21, 5956.28 examples/s]Map (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 432803/558128 [01:22<00:20, 6197.41 examples/s]Map (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 433453/558128 [01:22<00:20, 6056.07 examples/s]Map (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 434119/558128 [01:22<00:20, 6180.52 examples/s]Map (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 434970/558128 [01:23<00:18, 6619.30 examples/s]Map (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 435795/558128 [01:23<00:19, 6215.82 examples/s]Map (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 436455/558128 [01:23<00:19, 6310.53 examples/s]Map (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 437131/558128 [01:23<00:27, 4331.64 examples/s]Map (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 438777/558128 [01:23<00:17, 6772.15 examples/s]Map (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 439776/558128 [01:23<00:17, 6843.13 examples/s]Map (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 440629/558128 [01:24<00:17, 6708.62 examples/s]Map (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 441448/558128 [01:24<00:17, 6776.79 examples/s]Map (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 442281/558128 [01:24<00:17, 6756.51 examples/s]Map (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 443111/558128 [01:24<00:18, 6311.00 examples/s]Map (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 443930/558128 [01:24<00:17, 6590.10 examples/s]Map (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 444622/558128 [01:25<00:52, 2163.12 examples/s]Map (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 445865/558128 [01:25<00:34, 3209.36 examples/s]Map (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 447498/558128 [01:25<00:22, 4836.85 examples/s]Map (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 448900/558128 [01:25<00:17, 6212.36 examples/s]Map (num_proc=32):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 450387/558128 [01:25<00:14, 7659.66 examples/s]Map (num_proc=32):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 451724/558128 [01:26<00:13, 7677.07 examples/s]Map (num_proc=32):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 452901/558128 [01:26<00:14, 7265.48 examples/s]Map (num_proc=32):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 453890/558128 [01:26<00:15, 6908.29 examples/s]Map (num_proc=32):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 454728/558128 [01:26<00:20, 4939.42 examples/s]Map (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 456235/558128 [01:26<00:15, 6563.95 examples/s]Map (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 457414/558128 [01:26<00:13, 7363.92 examples/s]Map (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 458382/558128 [01:27<00:13, 7234.89 examples/s]Map (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 459333/558128 [01:27<00:14, 6969.07 examples/s]Map (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 460160/558128 [01:27<00:15, 6473.30 examples/s]Map (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 460929/558128 [01:27<00:14, 6584.78 examples/s]Map (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 461647/558128 [01:27<00:16, 5941.25 examples/s]Map (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 462411/558128 [01:28<00:42, 2273.43 examples/s]Map (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 463983/558128 [01:28<00:25, 3643.97 examples/s]Map (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 465655/558128 [01:28<00:17, 5270.91 examples/s]Map (num_proc=32):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 467166/558128 [01:28<00:13, 6694.55 examples/s]Map (num_proc=32):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 468435/558128 [01:29<00:12, 7253.46 examples/s]Map (num_proc=32):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 469569/558128 [01:29<00:12, 7197.60 examples/s]Map (num_proc=32):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 470568/558128 [01:29<00:12, 7016.04 examples/s]Map (num_proc=32):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 471502/558128 [01:29<00:12, 6793.94 examples/s]Map (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 472328/558128 [01:29<00:18, 4696.22 examples/s]Map (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 473996/558128 [01:29<00:12, 6631.40 examples/s]Map (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 475000/558128 [01:30<00:11, 7002.14 examples/s]Map (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 475986/558128 [01:30<00:11, 7010.32 examples/s]Map (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 476845/558128 [01:30<00:11, 7068.84 examples/s]Map (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 477678/558128 [01:30<00:12, 6520.25 examples/s]Map (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 478502/558128 [01:30<00:11, 6638.51 examples/s]Map (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 479255/558128 [01:31<00:34, 2272.51 examples/s]Map (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 480633/558128 [01:31<00:22, 3414.45 examples/s]Map (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 482088/558128 [01:31<00:15, 4786.75 examples/s]Map (num_proc=32):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 483679/558128 [01:31<00:11, 6384.26 examples/s]Map (num_proc=32):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 485385/558128 [01:31<00:09, 7928.80 examples/s]Map (num_proc=32):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 486644/558128 [01:32<00:09, 7236.31 examples/s]Map (num_proc=32):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 487776/558128 [01:32<00:09, 7184.58 examples/s]Map (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 488774/558128 [01:32<00:09, 6956.16 examples/s]Map (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 489756/558128 [01:32<00:10, 6790.08 examples/s]Map (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 490603/558128 [01:32<00:13, 5090.39 examples/s]Map (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 492297/558128 [01:33<00:09, 6927.84 examples/s]Map (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 493285/558128 [01:33<00:09, 6879.28 examples/s]Map (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 494113/558128 [01:33<00:09, 7049.12 examples/s]Map (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 494942/558128 [01:33<00:09, 6462.99 examples/s]Map (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 495778/558128 [01:33<00:09, 6636.93 examples/s]Map (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 496531/558128 [01:33<00:10, 6050.44 examples/s]Map (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 497318/558128 [01:34<00:26, 2325.42 examples/s]Map (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 498696/558128 [01:34<00:16, 3505.26 examples/s]Map (num_proc=32):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 500364/558128 [01:34<00:11, 5150.05 examples/s]Map (num_proc=32):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 501900/558128 [01:34<00:08, 6676.76 examples/s]Map (num_proc=32):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 503049/558128 [01:35<00:07, 6956.36 examples/s]Map (num_proc=32):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 504222/558128 [01:35<00:07, 6930.68 examples/s]Map (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 505214/558128 [01:35<00:07, 6820.01 examples/s]Map (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 506192/558128 [01:35<00:07, 6587.80 examples/s]Map (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 506974/558128 [01:35<00:07, 6469.26 examples/s]Map (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 507837/558128 [01:35<00:07, 6610.71 examples/s]Map (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 508591/558128 [01:36<00:10, 4888.99 examples/s]Map (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 510442/558128 [01:36<00:06, 6911.46 examples/s]Map (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 511404/558128 [01:36<00:06, 6769.23 examples/s]Map (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 512231/558128 [01:36<00:06, 6704.57 examples/s]Map (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 513040/558128 [01:36<00:07, 6267.32 examples/s]Map (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 513805/558128 [01:37<00:10, 4157.02 examples/s]Map (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 514806/558128 [01:37<00:09, 4769.55 examples/s]Map (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 515477/558128 [01:37<00:09, 4336.60 examples/s]Map (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 516130/558128 [01:37<00:09, 4532.15 examples/s]Map (num_proc=32):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 516808/558128 [01:37<00:08, 4971.77 examples/s]Map (num_proc=32):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 517461/558128 [01:37<00:07, 5267.68 examples/s]Map (num_proc=32):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 518063/558128 [01:37<00:07, 5196.12 examples/s]Map (num_proc=32):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 518724/558128 [01:37<00:07, 5538.64 examples/s]Map (num_proc=32):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 519415/558128 [01:38<00:06, 5886.09 examples/s]Map (num_proc=32):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 520039/558128 [01:38<00:06, 5824.27 examples/s]Map (num_proc=32):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 521032/558128 [01:38<00:06, 6103.28 examples/s]Map (num_proc=32):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 521663/558128 [01:38<00:05, 6094.99 examples/s]Map (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 522320/558128 [01:38<00:05, 6204.31 examples/s]Map (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 523003/558128 [01:38<00:05, 6368.17 examples/s]Map (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 523800/558128 [01:38<00:05, 6440.41 examples/s]Map (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 524462/558128 [01:38<00:05, 6430.38 examples/s]Map (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 525321/558128 [01:39<00:05, 5766.89 examples/s]Map (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 526422/558128 [01:39<00:04, 6712.66 examples/s]Map (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 527275/558128 [01:39<00:04, 6445.28 examples/s]Map (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 528029/558128 [01:39<00:04, 6445.52 examples/s]Map (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 528714/558128 [01:39<00:04, 6544.15 examples/s]Map (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 529391/558128 [01:39<00:04, 6598.50 examples/s]Map (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 530135/558128 [01:39<00:04, 6371.36 examples/s]Map (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 530908/558128 [01:39<00:04, 6279.95 examples/s]Map (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 531571/558128 [01:40<00:06, 4254.90 examples/s]Map (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 532680/558128 [01:40<00:04, 5551.78 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 533455/558128 [01:40<00:04, 5923.02 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 534238/558128 [01:40<00:04, 5793.71 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 535022/558128 [01:40<00:03, 6151.33 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 535813/558128 [01:40<00:03, 5877.79 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 536495/558128 [01:40<00:03, 6098.36 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 537138/558128 [01:40<00:03, 6154.09 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 537925/558128 [01:41<00:03, 6449.77 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 538605/558128 [01:41<00:02, 6536.95 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 539416/558128 [01:41<00:03, 5913.88 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 540272/558128 [01:41<00:02, 6517.56 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 541093/558128 [01:41<00:02, 6201.26 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 541858/558128 [01:41<00:02, 6339.44 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 542537/558128 [01:41<00:02, 6451.47 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 543320/558128 [01:41<00:02, 6151.81 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 543984/558128 [01:42<00:02, 6272.43 examples/s]Map (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 544669/558128 [01:42<00:02, 6418.49 examples/s]Map (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 545370/558128 [01:42<00:02, 5776.72 examples/s]Map (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 546014/558128 [01:42<00:02, 4581.47 examples/s]Map (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 546589/558128 [01:42<00:02, 4834.70 examples/s]Map (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 547278/558128 [01:42<00:02, 5321.60 examples/s]Map (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 547936/558128 [01:42<00:01, 5479.17 examples/s]Map (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 548580/558128 [01:42<00:01, 5726.74 examples/s]Map (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 549203/558128 [01:43<00:01, 5556.84 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 549853/558128 [01:43<00:01, 4357.21 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 550369/558128 [01:43<00:01, 4078.02 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 550855/558128 [01:43<00:01, 3724.68 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 551374/558128 [01:43<00:01, 3645.05 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 551857/558128 [01:43<00:01, 3446.72 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 552370/558128 [01:44<00:01, 3434.94 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 552857/558128 [01:44<00:01, 3254.70 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 553205/558128 [01:44<00:01, 3300.90 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 553687/558128 [01:44<00:01, 3194.83 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 554032/558128 [01:44<00:01, 3248.62 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 554376/558128 [01:44<00:01, 3293.28 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 554858/558128 [01:44<00:01, 3156.60 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 555206/558128 [01:44<00:00, 3231.25 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 555547/558128 [01:45<00:00, 3275.19 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 556030/558128 [01:45<00:00, 3184.06 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 556365/558128 [01:45<00:00, 3222.87 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 556857/558128 [01:45<00:00, 3135.37 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 557205/558128 [01:45<00:00, 3214.79 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 557540/558128 [01:45<00:00, 3248.32 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 558031/558128 [01:45<00:00, 3163.70 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 558128/558128 [01:46<00:00, 5252.86 examples/s]
Filter (num_proc=32):   0%|          | 0/558128 [00:00<?, ? examples/s]Filter (num_proc=32):   0%|          | 1000/558128 [00:00<05:03, 1837.39 examples/s]Filter (num_proc=32):   1%|          | 3000/558128 [00:00<01:39, 5562.02 examples/s]Filter (num_proc=32):   1%|â–         | 7000/558128 [00:00<00:43, 12681.01 examples/s]Filter (num_proc=32):   2%|â–         | 13000/558128 [00:00<00:23, 23590.76 examples/s]Filter (num_proc=32):   4%|â–         | 23000/558128 [00:00<00:13, 40701.29 examples/s]Filter (num_proc=32):   6%|â–Œ         | 34000/558128 [00:01<00:09, 57745.87 examples/s]Filter (num_proc=32):   8%|â–Š         | 47000/558128 [00:01<00:07, 70858.54 examples/s]Filter (num_proc=32):  11%|â–ˆ         | 61000/558128 [00:01<00:05, 88460.56 examples/s]Filter (num_proc=32):  13%|â–ˆâ–        | 71442/558128 [00:01<00:05, 90615.80 examples/s]Filter (num_proc=32):  16%|â–ˆâ–Œ        | 87884/558128 [00:01<00:04, 108609.00 examples/s]Filter (num_proc=32):  18%|â–ˆâ–Š        | 101884/558128 [00:01<00:03, 117136.40 examples/s]Filter (num_proc=32):  21%|â–ˆâ–ˆ        | 116768/558128 [00:01<00:03, 116754.77 examples/s]Filter (num_proc=32):  24%|â–ˆâ–ˆâ–       | 135768/558128 [00:01<00:03, 136697.57 examples/s]Filter (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 152652/558128 [00:02<00:03, 134738.85 examples/s]Filter (num_proc=32):  31%|â–ˆâ–ˆâ–ˆ       | 171094/558128 [00:02<00:02, 139581.40 examples/s]Filter (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–      | 185536/558128 [00:02<00:02, 138808.02 examples/s]Filter (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 200536/558128 [00:02<00:02, 130255.36 examples/s]Filter (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 214978/558128 [00:02<00:02, 123824.30 examples/s]Filter (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 229420/558128 [00:02<00:02, 119821.01 examples/s]Filter (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 244862/558128 [00:02<00:02, 127813.09 examples/s]Filter (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 257862/558128 [00:02<00:02, 124486.19 examples/s]Filter (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 279746/558128 [00:02<00:02, 138420.24 examples/s]Filter (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 294188/558128 [00:03<00:01, 138033.26 examples/s]Filter (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 311630/558128 [00:03<00:01, 136649.50 examples/s]Filter (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 328072/558128 [00:03<00:01, 137561.99 examples/s]Filter (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 342072/558128 [00:03<00:01, 135036.68 examples/s]Filter (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 357513/558128 [00:03<00:02, 91300.18 examples/s] Filter (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 388395/558128 [00:03<00:01, 133713.07 examples/s]Filter (num_proc=32):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 412277/558128 [00:03<00:00, 156932.15 examples/s]Filter (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 431718/558128 [00:04<00:00, 137678.94 examples/s]Filter (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 448159/558128 [00:04<00:00, 125093.23 examples/s]Filter (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 463600/558128 [00:04<00:00, 120365.21 examples/s]Filter (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 477041/558128 [00:04<00:00, 119321.35 examples/s]Filter (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 491482/558128 [00:04<00:00, 114316.35 examples/s]Filter (num_proc=32):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 504482/558128 [00:04<00:00, 110007.16 examples/s]Filter (num_proc=32):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 520923/558128 [00:04<00:00, 119921.46 examples/s]Filter (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 534364/558128 [00:05<00:00, 110724.64 examples/s]Filter (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 547805/558128 [00:05<00:00, 102936.93 examples/s]Filter (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 558128/558128 [00:05<00:00, 98931.95 examples/s] 
Map (num_proc=32):   0%|          | 0/558128 [00:00<?, ? examples/s]Map (num_proc=32):   0%|          | 600/558128 [00:05<1:28:51, 104.58 examples/s]Map (num_proc=32):   0%|          | 1605/558128 [00:05<26:49, 345.68 examples/s] Map (num_proc=32):   0%|          | 2601/558128 [00:05<13:53, 666.88 examples/s]Map (num_proc=32):   1%|          | 3596/558128 [00:06<08:30, 1086.07 examples/s]Map (num_proc=32):   1%|          | 4600/558128 [00:06<05:39, 1629.23 examples/s]Map (num_proc=32):   1%|          | 5576/558128 [00:06<04:03, 2272.20 examples/s]Map (num_proc=32):   1%|          | 6601/558128 [00:06<03:00, 3063.30 examples/s]Map (num_proc=32):   1%|â–         | 7599/558128 [00:06<02:20, 3921.53 examples/s]Map (num_proc=32):   2%|â–         | 8601/558128 [00:06<01:55, 4747.98 examples/s]Map (num_proc=32):   2%|â–         | 9601/558128 [00:06<01:37, 5641.50 examples/s]Map (num_proc=32):   2%|â–         | 11000/558128 [00:06<01:23, 6569.95 examples/s]Map (num_proc=32):   2%|â–         | 12000/558128 [00:06<01:16, 7159.64 examples/s]Map (num_proc=32):   2%|â–         | 13000/558128 [00:07<01:11, 7624.89 examples/s]Map (num_proc=32):   3%|â–         | 14597/558128 [00:07<01:02, 8673.26 examples/s]Map (num_proc=32):   3%|â–         | 15587/558128 [00:07<01:53, 4795.67 examples/s]Map (num_proc=32):   3%|â–         | 18034/558128 [00:10<06:09, 1463.37 examples/s]Map (num_proc=32):   3%|â–         | 19047/558128 [00:10<04:59, 1802.39 examples/s]Map (num_proc=32):   4%|â–         | 20040/558128 [00:10<04:00, 2232.97 examples/s]Map (num_proc=32):   4%|â–         | 21043/558128 [00:10<03:14, 2756.64 examples/s]Map (num_proc=32):   4%|â–         | 22034/558128 [00:11<02:37, 3399.81 examples/s]Map (num_proc=32):   4%|â–         | 23041/558128 [00:11<02:09, 4145.29 examples/s]Map (num_proc=32):   4%|â–         | 24048/558128 [00:11<01:48, 4940.92 examples/s]Map (num_proc=32):   4%|â–         | 25030/558128 [00:11<01:33, 5671.66 examples/s]Map (num_proc=32):   5%|â–         | 26040/558128 [00:11<01:22, 6465.28 examples/s]Map (num_proc=32):   5%|â–         | 27040/558128 [00:11<01:38, 5371.67 examples/s]Map (num_proc=32):   5%|â–Œ         | 29442/558128 [00:11<01:00, 8675.00 examples/s]Map (num_proc=32):   6%|â–Œ         | 31037/558128 [00:11<00:58, 9007.36 examples/s]Map (num_proc=32):   6%|â–Œ         | 32442/558128 [00:12<00:58, 9006.15 examples/s]Map (num_proc=32):   6%|â–Œ         | 34033/558128 [00:12<00:56, 9296.48 examples/s]Map (num_proc=32):   6%|â–‹         | 35455/558128 [00:15<05:49, 1494.21 examples/s]Map (num_proc=32):   7%|â–‹         | 36483/558128 [00:15<04:40, 1856.69 examples/s]Map (num_proc=32):   7%|â–‹         | 37479/558128 [00:15<04:15, 2039.06 examples/s]Map (num_proc=32):   7%|â–‹         | 40884/558128 [00:15<02:09, 3986.12 examples/s]Map (num_proc=32):   8%|â–Š         | 42476/558128 [00:15<01:47, 4797.50 examples/s]Map (num_proc=32):   8%|â–Š         | 43884/558128 [00:16<01:35, 5366.86 examples/s]Map (num_proc=32):   8%|â–Š         | 45481/558128 [00:16<01:21, 6268.78 examples/s]Map (num_proc=32):   8%|â–Š         | 46884/558128 [00:16<01:15, 6799.21 examples/s]Map (num_proc=32):   9%|â–Š         | 47884/558128 [00:16<01:12, 7032.98 examples/s]Map (num_proc=32):   9%|â–‰         | 48884/558128 [00:16<01:07, 7552.98 examples/s]Map (num_proc=32):   9%|â–‰         | 49884/558128 [00:16<01:04, 7890.90 examples/s]Map (num_proc=32):   9%|â–‰         | 50884/558128 [00:16<01:01, 8272.62 examples/s]Map (num_proc=32):   9%|â–‰         | 51884/558128 [00:16<00:58, 8680.86 examples/s]Map (num_proc=32):   9%|â–‰         | 52862/558128 [00:20<07:59, 1054.77 examples/s]Map (num_proc=32):  10%|â–‰         | 53927/558128 [00:20<05:50, 1440.29 examples/s]Map (num_proc=32):  10%|â–‰         | 54924/558128 [00:20<04:24, 1904.69 examples/s]Map (num_proc=32):  10%|â–ˆ         | 55919/558128 [00:20<03:22, 2475.99 examples/s]Map (num_proc=32):  10%|â–ˆ         | 56914/558128 [00:20<02:38, 3164.50 examples/s]Map (num_proc=32):  10%|â–ˆ         | 58326/558128 [00:20<02:02, 4092.02 examples/s]Map (num_proc=32):  11%|â–ˆ         | 59918/558128 [00:20<01:33, 5302.58 examples/s]Map (num_proc=32):  11%|â–ˆ         | 61326/558128 [00:20<01:23, 5978.97 examples/s]Map (num_proc=32):  11%|â–ˆâ–        | 62925/558128 [00:21<01:09, 7128.06 examples/s]Map (num_proc=32):  11%|â–ˆâ–        | 63921/558128 [00:21<01:05, 7585.08 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 64922/558128 [00:21<01:02, 7906.84 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 65916/558128 [00:21<00:59, 8274.97 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 67326/558128 [00:21<01:24, 5788.89 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 70313/558128 [00:24<04:06, 1979.19 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 71368/558128 [00:24<03:26, 2358.66 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 72309/558128 [00:24<02:54, 2786.26 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 73362/558128 [00:24<02:23, 3387.25 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 74335/558128 [00:24<02:00, 4026.39 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 75366/558128 [00:24<01:40, 4824.17 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 76364/558128 [00:24<01:29, 5378.60 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 77768/558128 [00:25<01:16, 6266.80 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 79371/558128 [00:25<01:04, 7377.23 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 80365/558128 [00:25<01:24, 5648.12 examples/s]Map (num_proc=32):  15%|â–ˆâ–        | 83361/558128 [00:25<00:51, 9237.59 examples/s]Map (num_proc=32):  15%|â–ˆâ–Œ        | 84768/558128 [00:25<00:52, 9086.64 examples/s]Map (num_proc=32):  15%|â–ˆâ–Œ        | 86188/558128 [00:25<00:51, 9083.14 examples/s]Map (num_proc=32):  16%|â–ˆâ–Œ        | 87759/558128 [00:28<04:24, 1780.68 examples/s]Map (num_proc=32):  16%|â–ˆâ–Œ        | 88806/558128 [00:28<03:35, 2174.07 examples/s]Map (num_proc=32):  16%|â–ˆâ–Œ        | 89758/558128 [00:28<02:59, 2611.85 examples/s]Map (num_proc=32):  16%|â–ˆâ–‹        | 90804/558128 [00:28<02:24, 3235.02 examples/s]Map (num_proc=32):  16%|â–ˆâ–‹        | 91804/558128 [00:28<01:59, 3913.74 examples/s]Map (num_proc=32):  17%|â–ˆâ–‹        | 92799/558128 [00:29<01:59, 3898.19 examples/s]Map (num_proc=32):  17%|â–ˆâ–‹        | 95210/558128 [00:29<01:12, 6400.15 examples/s]Map (num_proc=32):  17%|â–ˆâ–‹        | 96806/558128 [00:29<01:03, 7225.14 examples/s]Map (num_proc=32):  18%|â–ˆâ–Š        | 98210/558128 [00:29<01:00, 7627.41 examples/s]Map (num_proc=32):  18%|â–ˆâ–Š        | 99807/558128 [00:29<00:55, 8218.88 examples/s]Map (num_proc=32):  18%|â–ˆâ–Š        | 101210/558128 [00:29<00:54, 8337.68 examples/s]Map (num_proc=32):  18%|â–ˆâ–Š        | 102799/558128 [00:30<00:51, 8863.51 examples/s]Map (num_proc=32):  19%|â–ˆâ–Š        | 104210/558128 [00:30<00:52, 8694.27 examples/s]Map (num_proc=32):  19%|â–ˆâ–‰        | 105204/558128 [00:32<05:00, 1504.75 examples/s]Map (num_proc=32):  19%|â–ˆâ–‰        | 106254/558128 [00:32<03:56, 1909.61 examples/s]Map (num_proc=32):  19%|â–ˆâ–‰        | 107192/558128 [00:33<03:10, 2362.56 examples/s]Map (num_proc=32):  19%|â–ˆâ–‰        | 108251/558128 [00:33<02:29, 3003.16 examples/s]Map (num_proc=32):  20%|â–ˆâ–‰        | 109652/558128 [00:33<01:54, 3921.32 examples/s]Map (num_proc=32):  20%|â–ˆâ–‰        | 110652/558128 [00:33<01:37, 4602.74 examples/s]Map (num_proc=32):  20%|â–ˆâ–ˆ        | 111652/558128 [00:33<01:23, 5366.22 examples/s]Map (num_proc=32):  20%|â–ˆâ–ˆ        | 112652/558128 [00:33<01:14, 5952.31 examples/s]Map (num_proc=32):  20%|â–ˆâ–ˆ        | 114253/558128 [00:33<01:01, 7267.22 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆ        | 115251/558128 [00:33<00:57, 7682.57 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆ        | 116250/558128 [00:34<00:54, 8118.53 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆ        | 117221/558128 [00:34<00:52, 8345.63 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆ        | 118249/558128 [00:34<00:50, 8672.03 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆâ–       | 119249/558128 [00:34<00:49, 8862.09 examples/s]Map (num_proc=32):  22%|â–ˆâ–ˆâ–       | 120238/558128 [00:34<00:48, 9037.50 examples/s]Map (num_proc=32):  22%|â–ˆâ–ˆâ–       | 121241/558128 [00:34<00:47, 9183.95 examples/s]Map (num_proc=32):  22%|â–ˆâ–ˆâ–       | 122688/558128 [00:37<05:21, 1352.30 examples/s]Map (num_proc=32):  22%|â–ˆâ–ˆâ–       | 123697/558128 [00:37<04:05, 1766.86 examples/s]Map (num_proc=32):  22%|â–ˆâ–ˆâ–       | 124666/558128 [00:37<03:11, 2268.95 examples/s]Map (num_proc=32):  23%|â–ˆâ–ˆâ–       | 125686/558128 [00:37<02:28, 2916.66 examples/s]Map (num_proc=32):  23%|â–ˆâ–ˆâ–       | 127094/558128 [00:37<01:52, 3842.13 examples/s]Map (num_proc=32):  23%|â–ˆâ–ˆâ–       | 128094/558128 [00:37<01:34, 4565.34 examples/s]Map (num_proc=32):  23%|â–ˆâ–ˆâ–       | 129632/558128 [00:37<01:14, 5765.05 examples/s]Map (num_proc=32):  23%|â–ˆâ–ˆâ–       | 130692/558128 [00:37<01:05, 6483.01 examples/s]Map (num_proc=32):  24%|â–ˆâ–ˆâ–       | 131699/558128 [00:38<00:59, 7114.63 examples/s]Map (num_proc=32):  24%|â–ˆâ–ˆâ–       | 132668/558128 [00:38<01:14, 5729.37 examples/s]Map (num_proc=32):  24%|â–ˆâ–ˆâ–       | 135094/558128 [00:38<00:48, 8782.94 examples/s]Map (num_proc=32):  24%|â–ˆâ–ˆâ–       | 136688/558128 [00:38<00:45, 9209.31 examples/s]Map (num_proc=32):  25%|â–ˆâ–ˆâ–       | 138094/558128 [00:38<00:47, 8877.27 examples/s]Map (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 139536/558128 [00:38<00:45, 9140.87 examples/s]Map (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 141143/558128 [00:41<04:13, 1646.67 examples/s]Map (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 142079/558128 [00:41<03:53, 1781.09 examples/s]Map (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 145536/558128 [00:42<01:56, 3529.87 examples/s]Map (num_proc=32):  26%|â–ˆâ–ˆâ–‹       | 147138/558128 [00:42<01:36, 4256.04 examples/s]Map (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 148536/558128 [00:42<01:24, 4822.12 examples/s]Map (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 150134/558128 [00:42<01:11, 5716.91 examples/s]Map (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 151536/558128 [00:42<01:04, 6268.29 examples/s]Map (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 153102/558128 [00:42<00:57, 7055.82 examples/s]Map (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 154536/558128 [00:43<00:54, 7446.31 examples/s]Map (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 155536/558128 [00:43<00:51, 7800.68 examples/s]Map (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 156536/558128 [00:43<00:49, 8115.08 examples/s]Map (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 157526/558128 [00:46<05:27, 1223.41 examples/s]Map (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 158577/558128 [00:46<04:08, 1610.66 examples/s]Map (num_proc=32):  29%|â–ˆâ–ˆâ–Š       | 159568/558128 [00:46<03:12, 2073.21 examples/s]Map (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 160565/558128 [00:46<02:29, 2654.42 examples/s]Map (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 161545/558128 [00:46<01:59, 3332.43 examples/s]Map (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 162575/558128 [00:46<01:35, 4145.84 examples/s]Map (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 163581/558128 [00:46<01:18, 5006.47 examples/s]Map (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 164571/558128 [00:46<01:09, 5643.86 examples/s]Map (num_proc=32):  30%|â–ˆâ–ˆâ–‰       | 165576/558128 [00:46<01:00, 6447.16 examples/s]Map (num_proc=32):  30%|â–ˆâ–ˆâ–‰       | 166978/558128 [00:47<00:53, 7292.51 examples/s]Map (num_proc=32):  30%|â–ˆâ–ˆâ–ˆ       | 167978/558128 [00:47<00:50, 7690.32 examples/s]Map (num_proc=32):  30%|â–ˆâ–ˆâ–ˆ       | 168978/558128 [00:47<00:48, 8030.24 examples/s]Map (num_proc=32):  30%|â–ˆâ–ˆâ–ˆ       | 169978/558128 [00:47<00:46, 8421.48 examples/s]Map (num_proc=32):  31%|â–ˆâ–ˆâ–ˆ       | 170978/558128 [00:47<00:44, 8666.60 examples/s]Map (num_proc=32):  31%|â–ˆâ–ˆâ–ˆ       | 171978/558128 [00:47<00:43, 8877.56 examples/s]Map (num_proc=32):  31%|â–ˆâ–ˆâ–ˆ       | 172978/558128 [00:47<00:42, 9043.76 examples/s]Map (num_proc=32):  31%|â–ˆâ–ˆâ–ˆ       | 173978/558128 [00:47<00:43, 8919.58 examples/s]Map (num_proc=32):  31%|â–ˆâ–ˆâ–ˆâ–      | 174991/558128 [00:50<05:56, 1073.46 examples/s]Map (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 176021/558128 [00:50<04:20, 1469.64 examples/s]Map (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 176993/558128 [00:50<03:15, 1946.75 examples/s]Map (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 178020/558128 [00:51<02:27, 2573.87 examples/s]Map (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 179420/558128 [00:51<01:48, 3479.68 examples/s]Map (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 181016/558128 [00:51<01:20, 4657.88 examples/s]Map (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–      | 181992/558128 [00:51<01:10, 5306.90 examples/s]Map (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–      | 183018/558128 [00:51<01:01, 6054.87 examples/s]Map (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–      | 184016/558128 [00:51<00:55, 6742.02 examples/s]Map (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–      | 185015/558128 [00:51<00:50, 7359.87 examples/s]Map (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–      | 186004/558128 [00:52<01:03, 5888.63 examples/s]Map (num_proc=32):  34%|â–ˆâ–ˆâ–ˆâ–      | 188420/558128 [00:52<00:41, 8917.43 examples/s]Map (num_proc=32):  34%|â–ˆâ–ˆâ–ˆâ–      | 190007/558128 [00:52<00:40, 9169.27 examples/s]Map (num_proc=32):  34%|â–ˆâ–ˆâ–ˆâ–      | 191420/558128 [00:52<00:40, 8986.75 examples/s]Map (num_proc=32):  34%|â–ˆâ–ˆâ–ˆâ–      | 192454/558128 [00:55<04:03, 1501.82 examples/s]Map (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–      | 193462/558128 [00:55<03:13, 1884.83 examples/s]Map (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–      | 194436/558128 [00:55<02:34, 2359.39 examples/s]Map (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 195455/558128 [00:55<02:01, 2973.73 examples/s]Map (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 196465/558128 [00:55<01:38, 3689.76 examples/s]Map (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 197458/558128 [00:55<01:21, 4443.87 examples/s]Map (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 198459/558128 [00:55<01:25, 4207.94 examples/s]Map (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 200862/558128 [00:56<00:50, 7030.82 examples/s]Map (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–‹      | 202459/558128 [00:56<00:45, 7839.40 examples/s]Map (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 203862/558128 [00:56<00:43, 8091.41 examples/s]Map (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 205451/558128 [00:56<00:41, 8585.59 examples/s]Map (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 206862/558128 [00:56<00:40, 8695.53 examples/s]Map (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 207862/558128 [00:56<00:40, 8747.06 examples/s]Map (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 209304/558128 [00:56<00:38, 8996.59 examples/s]Map (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 210304/558128 [00:59<04:16, 1358.68 examples/s]Map (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 212304/558128 [00:59<02:39, 2164.36 examples/s]Map (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 213304/558128 [00:59<02:11, 2620.11 examples/s]Map (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 214304/558128 [00:59<01:48, 3177.03 examples/s]Map (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 215304/558128 [01:00<01:29, 3830.94 examples/s]Map (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 216304/558128 [01:00<01:15, 4505.56 examples/s]Map (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 217304/558128 [01:00<01:04, 5289.61 examples/s]Map (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 218304/558128 [01:00<00:56, 6062.43 examples/s]Map (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 219304/558128 [01:00<00:49, 6784.89 examples/s]Map (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–‰      | 220869/558128 [01:00<00:43, 7802.41 examples/s]Map (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–‰      | 221900/558128 [01:00<00:41, 8155.40 examples/s]Map (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–‰      | 222898/558128 [01:00<00:39, 8449.92 examples/s]Map (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 223891/558128 [01:00<00:38, 8712.56 examples/s]Map (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 224890/558128 [01:01<00:38, 8662.09 examples/s]Map (num_proc=32):  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 225893/558128 [01:01<00:37, 8900.31 examples/s]Map (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 227332/558128 [01:03<04:20, 1270.69 examples/s]Map (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 228344/558128 [01:04<03:18, 1663.11 examples/s]Map (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 229337/558128 [01:04<02:32, 2150.84 examples/s]Map (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 230301/558128 [01:04<02:00, 2731.39 examples/s]Map (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 231335/558128 [01:04<01:33, 3476.98 examples/s]Map (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 232282/558128 [01:04<01:17, 4205.89 examples/s]Map (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 233337/558128 [01:04<01:03, 5079.42 examples/s]Map (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 234295/558128 [01:04<00:55, 5824.27 examples/s]Map (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 235336/558128 [01:04<00:48, 6629.81 examples/s]Map (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 236343/558128 [01:04<00:44, 7276.88 examples/s]Map (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 237323/558128 [01:05<01:00, 5278.75 examples/s]Map (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 240335/558128 [01:05<00:33, 9515.77 examples/s]Map (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 241741/558128 [01:05<00:33, 9388.81 examples/s]Map (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 243050/558128 [01:05<00:34, 9042.31 examples/s]Map (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 244188/558128 [01:05<00:34, 9221.73 examples/s]Map (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 245789/558128 [01:08<03:28, 1495.81 examples/s]Map (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 247188/558128 [01:08<02:35, 1999.48 examples/s]Map (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 248188/558128 [01:09<02:22, 2170.91 examples/s]Map (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 251781/558128 [01:09<01:11, 4304.24 examples/s]Map (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 253188/558128 [01:09<01:02, 4887.96 examples/s]Map (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 254782/558128 [01:09<00:53, 5711.96 examples/s]Map (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 256188/558128 [01:09<00:48, 6250.89 examples/s]Map (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 257781/558128 [01:09<00:43, 6962.76 examples/s]Map (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 259188/558128 [01:10<00:40, 7383.00 examples/s]Map (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 260775/558128 [01:10<00:36, 8134.48 examples/s]Map (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 262221/558128 [01:12<03:02, 1619.08 examples/s]Map (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 263230/558128 [01:13<02:29, 1974.19 examples/s]Map (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 264630/558128 [01:13<01:54, 2567.51 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 266231/558128 [01:13<01:24, 3455.32 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 267226/558128 [01:13<01:12, 4022.61 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 268229/558128 [01:13<01:02, 4675.35 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 269630/558128 [01:13<00:51, 5602.50 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 270630/558128 [01:13<00:46, 6121.46 examples/s]Map (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 272227/558128 [01:13<00:39, 7304.42 examples/s]Map (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 273226/558128 [01:14<00:37, 7649.54 examples/s]Map (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 274226/558128 [01:14<00:35, 7982.53 examples/s]Map (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 275217/558128 [01:14<00:33, 8362.26 examples/s]Map (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 276222/558128 [01:14<00:32, 8653.36 examples/s]Map (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 277630/558128 [01:14<00:32, 8558.09 examples/s]Map (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 278630/558128 [01:14<00:33, 8439.43 examples/s]Map (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 279663/558128 [01:17<04:20, 1068.27 examples/s]Map (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 280676/558128 [01:17<03:14, 1426.42 examples/s]Map (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 281669/558128 [01:18<02:26, 1882.52 examples/s]Map (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 282666/558128 [01:18<01:52, 2454.32 examples/s]Map (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 283668/558128 [01:18<01:27, 3139.49 examples/s]Map (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 284668/558128 [01:18<01:09, 3922.82 examples/s]Map (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 285646/558128 [01:18<00:57, 4718.61 examples/s]Map (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 286665/558128 [01:18<00:48, 5586.59 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 287623/558128 [01:18<00:42, 6313.81 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 288667/558128 [01:18<00:37, 7146.23 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 289660/558128 [01:18<00:35, 7636.83 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 290616/558128 [01:18<00:33, 8002.40 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 291661/558128 [01:19<00:31, 8330.40 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 292653/558128 [01:19<00:30, 8618.67 examples/s]Map (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 293655/558128 [01:19<00:30, 8723.43 examples/s]Map (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 294658/558128 [01:19<00:29, 8928.90 examples/s]Map (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 295624/558128 [01:19<00:43, 6104.06 examples/s]Map (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 297107/558128 [01:22<03:35, 1211.13 examples/s]Map (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 298114/558128 [01:22<02:43, 1593.54 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 299099/558128 [01:22<02:05, 2064.09 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 300109/558128 [01:22<01:36, 2678.76 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 301106/558128 [01:22<01:16, 3373.41 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 302114/558128 [01:22<01:01, 4154.90 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 303106/558128 [01:23<00:51, 4965.38 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 304104/558128 [01:23<00:43, 5798.60 examples/s]Map (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 305084/558128 [01:23<00:58, 4350.59 examples/s]Map (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 308513/558128 [01:23<00:28, 8851.92 examples/s]Map (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 310107/558128 [01:23<00:27, 9059.25 examples/s]Map (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 311513/558128 [01:23<00:27, 9080.17 examples/s]Map (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 313101/558128 [01:24<00:26, 9413.09 examples/s]Map (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 314548/558128 [01:26<02:28, 1641.48 examples/s]Map (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 315548/558128 [01:27<02:08, 1885.03 examples/s]Map (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 317954/558128 [01:27<01:19, 3034.51 examples/s]Map (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 318954/558128 [01:27<01:08, 3504.67 examples/s]Map (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 319954/558128 [01:27<00:58, 4088.75 examples/s]Map (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 320954/558128 [01:27<00:50, 4714.70 examples/s]Map (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 321954/558128 [01:27<00:43, 5399.70 examples/s]Map (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 322954/558128 [01:27<00:38, 6150.16 examples/s]Map (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 323954/558128 [01:27<00:34, 6748.50 examples/s]Map (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 324954/558128 [01:28<00:31, 7356.04 examples/s]Map (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 325954/558128 [01:28<00:29, 7890.63 examples/s]Map (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 326954/558128 [01:28<00:28, 8205.89 examples/s]Map (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 327954/558128 [01:28<00:26, 8545.30 examples/s]Map (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 328954/558128 [01:28<00:26, 8572.58 examples/s]Map (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 330541/558128 [01:28<00:24, 9472.81 examples/s]Map (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 331970/558128 [01:31<02:45, 1369.12 examples/s]Map (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 332922/558128 [01:31<02:10, 1724.26 examples/s]Map (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 333989/558128 [01:31<01:40, 2237.69 examples/s]Map (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 334988/558128 [01:31<01:18, 2831.07 examples/s]Map (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 335986/558128 [01:31<01:02, 3539.62 examples/s]Map (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 336988/558128 [01:31<00:51, 4303.76 examples/s]Map (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 337989/558128 [01:32<00:43, 5114.88 examples/s]Map (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 338978/558128 [01:32<00:36, 5928.23 examples/s]Map (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 339989/558128 [01:32<00:32, 6672.02 examples/s]Map (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 340990/558128 [01:32<00:29, 7342.28 examples/s]Map (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 341988/558128 [01:32<00:27, 7886.45 examples/s]Map (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 343395/558128 [01:32<00:27, 7938.71 examples/s]Map (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 344395/558128 [01:32<00:25, 8344.22 examples/s]Map (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 345986/558128 [01:33<00:33, 6368.19 examples/s]Map (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 348836/558128 [01:33<00:20, 9966.67 examples/s]Map (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 350410/558128 [01:36<02:00, 1725.33 examples/s]Map (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 351432/558128 [01:36<01:39, 2082.87 examples/s]Map (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 352429/558128 [01:36<01:21, 2514.08 examples/s]Map (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 353836/558128 [01:36<01:03, 3221.71 examples/s]Map (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 354836/558128 [01:36<00:52, 3840.47 examples/s]Map (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 355836/558128 [01:36<00:55, 3661.13 examples/s]Map (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 358836/558128 [01:36<00:30, 6560.43 examples/s]Map (num_proc=32):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 360428/558128 [01:37<00:27, 7313.01 examples/s]Map (num_proc=32):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 361836/558128 [01:37<00:25, 7584.42 examples/s]Map (num_proc=32):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 363426/558128 [01:37<00:23, 8223.81 examples/s]Map (num_proc=32):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 364836/558128 [01:37<00:23, 8344.14 examples/s]Map (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 366277/558128 [01:37<00:22, 8639.46 examples/s]Map (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 367277/558128 [01:40<02:11, 1456.34 examples/s]Map (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 369870/558128 [01:40<01:15, 2488.31 examples/s]Map (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 371277/558128 [01:40<01:01, 3058.39 examples/s]Map (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 372872/558128 [01:40<00:48, 3838.05 examples/s]Map (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 373868/558128 [01:40<00:42, 4367.55 examples/s]Map (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 374872/558128 [01:41<00:36, 5001.19 examples/s]Map (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 376277/558128 [01:41<00:31, 5803.47 examples/s]Map (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 377277/558128 [01:41<00:28, 6306.87 examples/s]Map (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 378277/558128 [01:41<00:25, 6923.95 examples/s]Map (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 379871/558128 [01:41<00:22, 7984.11 examples/s]Map (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 380861/558128 [01:41<00:21, 8302.87 examples/s]Map (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 381863/558128 [01:41<00:20, 8468.73 examples/s]Map (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 382863/558128 [01:41<00:20, 8726.66 examples/s]Map (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 384312/558128 [01:44<02:13, 1297.49 examples/s]Map (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 385251/558128 [01:44<01:44, 1650.84 examples/s]Map (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 386314/558128 [01:44<01:18, 2176.13 examples/s]Map (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 387310/558128 [01:45<01:01, 2761.99 examples/s]Map (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 388718/558128 [01:45<00:46, 3653.65 examples/s]Map (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 390309/558128 [01:45<00:34, 4811.91 examples/s]Map (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 391302/558128 [01:45<00:30, 5482.69 examples/s]Map (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 392718/558128 [01:45<00:26, 6346.67 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 393718/558128 [01:45<00:23, 6940.03 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 394718/558128 [01:45<00:21, 7549.21 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 395718/558128 [01:45<00:21, 7589.84 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 397312/558128 [01:46<00:18, 8547.45 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 398308/558128 [01:46<00:18, 8769.96 examples/s]Map (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 399305/558128 [01:46<00:17, 8842.49 examples/s]Map (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 400305/558128 [01:46<00:24, 6470.03 examples/s]Map (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 401740/558128 [01:49<02:06, 1235.34 examples/s]Map (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 403159/558128 [01:49<01:28, 1749.20 examples/s]Map (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 404159/558128 [01:49<01:09, 2204.40 examples/s]Map (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 405159/558128 [01:49<00:55, 2774.46 examples/s]Map (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 406763/558128 [01:49<00:39, 3845.24 examples/s]Map (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 407760/558128 [01:50<00:33, 4506.31 examples/s]Map (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 408750/558128 [01:50<00:35, 4202.71 examples/s]Map (num_proc=32):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 411761/558128 [01:50<00:20, 7282.19 examples/s]Map (num_proc=32):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 413159/558128 [01:50<00:19, 7549.03 examples/s]Map (num_proc=32):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 414752/558128 [01:50<00:17, 8189.87 examples/s]Map (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 416159/558128 [01:50<00:17, 8350.67 examples/s]Map (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 417159/558128 [01:51<00:16, 8660.48 examples/s]Map (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 418159/558128 [01:51<00:15, 8751.31 examples/s]Map (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 419195/558128 [01:53<01:37, 1424.59 examples/s]Map (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 420194/558128 [01:53<01:19, 1732.29 examples/s]Map (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 422600/558128 [01:53<00:45, 3008.72 examples/s]Map (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 424195/558128 [01:54<00:34, 3858.29 examples/s]Map (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 425197/558128 [01:54<00:29, 4441.57 examples/s]Map (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 426192/558128 [01:54<00:26, 5057.13 examples/s]Map (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 427195/558128 [01:54<00:22, 5730.31 examples/s]Map (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 428198/558128 [01:54<00:20, 6424.81 examples/s]Map (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 429167/558128 [01:54<00:18, 7003.85 examples/s]Map (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 430194/558128 [01:54<00:16, 7688.58 examples/s]Map (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 431181/558128 [01:54<00:15, 8118.24 examples/s]Map (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 432193/558128 [01:54<00:15, 8313.55 examples/s]Map (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 433194/558128 [01:55<00:14, 8647.49 examples/s]Map (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 434188/558128 [01:55<00:13, 8887.55 examples/s]Map (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 435187/558128 [01:55<00:13, 8799.46 examples/s]Map (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 436631/558128 [01:58<01:38, 1239.01 examples/s]Map (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 437640/558128 [01:58<01:14, 1627.61 examples/s]Map (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 438639/558128 [01:58<00:56, 2114.52 examples/s]Map (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 439638/558128 [01:58<00:43, 2717.86 examples/s]Map (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 440634/558128 [01:58<00:34, 3417.41 examples/s]Map (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 441634/558128 [01:58<00:27, 4212.94 examples/s]Map (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 442635/558128 [01:58<00:23, 5018.24 examples/s]Map (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 443603/558128 [01:58<00:19, 5796.67 examples/s]Map (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 444636/558128 [01:58<00:17, 6617.45 examples/s]Map (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 446041/558128 [01:59<00:15, 7158.06 examples/s]Map (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 447635/558128 [01:59<00:13, 8221.13 examples/s]Map (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 448604/558128 [01:59<00:13, 8392.23 examples/s]Map (num_proc=32):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 449635/558128 [01:59<00:12, 8620.04 examples/s]Map (num_proc=32):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 450634/558128 [01:59<00:16, 6668.99 examples/s]Map (num_proc=32):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 453041/558128 [01:59<00:10, 9630.93 examples/s]Map (num_proc=32):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 454482/558128 [02:02<01:06, 1567.00 examples/s]Map (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 455482/558128 [02:02<00:53, 1930.58 examples/s]Map (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 456482/558128 [02:02<00:42, 2391.11 examples/s]Map (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 458074/558128 [02:03<00:30, 3293.80 examples/s]Map (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 459046/558128 [02:03<00:25, 3891.53 examples/s]Map (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 460074/558128 [02:03<00:26, 3697.89 examples/s]Map (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 463073/558128 [02:03<00:14, 6700.18 examples/s]Map (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 464482/558128 [02:03<00:13, 7122.45 examples/s]Map (num_proc=32):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 466074/558128 [02:03<00:11, 7788.01 examples/s]Map (num_proc=32):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 467482/558128 [02:04<00:11, 8019.11 examples/s]Map (num_proc=32):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 469067/558128 [02:04<00:10, 8575.61 examples/s]Map (num_proc=32):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 470482/558128 [02:04<00:10, 8700.22 examples/s]Map (num_proc=32):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 471519/558128 [02:06<00:56, 1546.09 examples/s]Map (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 472523/558128 [02:07<00:49, 1735.51 examples/s]Map (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 475923/558128 [02:07<00:23, 3465.88 examples/s]Map (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 477376/558128 [02:07<00:19, 4099.85 examples/s]Map (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 478513/558128 [02:07<00:16, 4689.69 examples/s]Map (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 479923/558128 [02:07<00:14, 5388.07 examples/s]Map (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 481488/558128 [02:07<00:12, 6245.42 examples/s]Map (num_proc=32):  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 482515/558128 [02:08<00:11, 6793.99 examples/s]Map (num_proc=32):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 483519/558128 [02:08<00:10, 7224.25 examples/s]Map (num_proc=32):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 484510/558128 [02:08<00:09, 7679.90 examples/s]Map (num_proc=32):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 485923/558128 [02:08<00:09, 7789.74 examples/s]Map (num_proc=32):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 486923/558128 [02:08<00:08, 8234.41 examples/s]Map (num_proc=32):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 487923/558128 [02:08<00:08, 8535.45 examples/s]Map (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 488959/558128 [02:11<00:56, 1223.68 examples/s]Map (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 489967/558128 [02:11<00:41, 1623.55 examples/s]Map (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 490962/558128 [02:11<00:31, 2114.55 examples/s]Map (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 491957/558128 [02:11<00:24, 2730.13 examples/s]Map (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 492962/558128 [02:11<00:18, 3460.29 examples/s]Map (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 493958/558128 [02:11<00:15, 4267.60 examples/s]Map (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 495364/558128 [02:12<00:11, 5273.45 examples/s]Map (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 496364/558128 [02:12<00:10, 5995.58 examples/s]Map (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 497364/558128 [02:12<00:09, 6651.93 examples/s]Map (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 498364/558128 [02:12<00:08, 7306.15 examples/s]Map (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 499364/558128 [02:12<00:07, 7813.91 examples/s]Map (num_proc=32):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 500364/558128 [02:12<00:07, 8148.83 examples/s]Map (num_proc=32):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 501364/558128 [02:12<00:06, 8518.28 examples/s]Map (num_proc=32):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 502364/558128 [02:12<00:06, 8443.68 examples/s]Map (num_proc=32):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 503364/558128 [02:13<00:10, 5423.84 examples/s]Map (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 506400/558128 [02:15<00:30, 1678.27 examples/s]Map (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 507405/558128 [02:16<00:25, 2026.00 examples/s]Map (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 508403/558128 [02:16<00:20, 2475.06 examples/s]Map (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 509398/558128 [02:16<00:16, 3033.95 examples/s]Map (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 510400/558128 [02:16<00:12, 3699.09 examples/s]Map (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 511408/558128 [02:16<00:10, 4428.73 examples/s]Map (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 512401/558128 [02:16<00:08, 5171.42 examples/s]Map (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 513392/558128 [02:16<00:09, 4804.63 examples/s]Map (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 515805/558128 [02:16<00:05, 7573.15 examples/s]Map (num_proc=32):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 517400/558128 [02:17<00:04, 8242.58 examples/s]Map (num_proc=32):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 518805/558128 [02:17<00:04, 8215.78 examples/s]Map (num_proc=32):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 520399/558128 [02:17<00:04, 9007.04 examples/s]Map (num_proc=32):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 521805/558128 [02:17<00:04, 8878.92 examples/s]Map (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 522805/558128 [02:17<00:03, 8889.19 examples/s]Map (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 523837/558128 [02:20<00:25, 1321.08 examples/s]Map (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 526246/558128 [02:20<00:13, 2310.55 examples/s]Map (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 527837/558128 [02:20<00:10, 3020.64 examples/s]Map (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 529246/558128 [02:20<00:07, 3676.85 examples/s]Map (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 530837/558128 [02:21<00:05, 4583.48 examples/s]Map (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 532246/558128 [02:21<00:04, 5285.78 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 533246/558128 [02:21<00:04, 5838.92 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 534246/558128 [02:21<00:03, 6420.40 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 535246/558128 [02:21<00:03, 6955.83 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 536246/558128 [02:21<00:02, 7565.73 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 537246/558128 [02:21<00:02, 8015.76 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 538246/558128 [02:21<00:02, 8148.22 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 539246/558128 [02:21<00:02, 8394.54 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 540246/558128 [02:22<00:02, 8693.41 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 541279/558128 [02:24<00:14, 1132.32 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 542288/558128 [02:24<00:10, 1528.69 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 543281/558128 [02:25<00:07, 2029.61 examples/s]Map (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 544280/558128 [02:25<00:05, 2643.72 examples/s]Map (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 545272/558128 [02:25<00:03, 3365.27 examples/s]Map (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 546235/558128 [02:25<00:02, 4129.24 examples/s]Map (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 547281/558128 [02:25<00:02, 5016.06 examples/s]Map (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 548257/558128 [02:25<00:01, 5809.44 examples/s]Map (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 549281/558128 [02:25<00:01, 6624.21 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 550283/558128 [02:25<00:01, 7165.09 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 551282/558128 [02:25<00:00, 7751.15 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 552277/558128 [02:26<00:00, 8211.41 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 553257/558128 [02:26<00:00, 8431.16 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 554282/558128 [02:26<00:00, 8764.45 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 555279/558128 [02:26<00:00, 8970.89 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 556269/558128 [02:26<00:00, 9105.88 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 557274/558128 [02:26<00:00, 9224.29 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 558128/558128 [02:27<00:00, 3786.74 examples/s]
06/30 11:26:31 - mmengine - WARNING - Dataset LLaVADataset has no metainfo. ``dataset_meta`` in visualizer will be None.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.
Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.70s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.00s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.10s/it]
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.57s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.65s/it]06/30 11:26:36 - mmengine - INFO - Dispatch Phi3FlashAttention2 forward. Due to the implementation of the PyTorch version of flash attention, even when the `output_attentions` flag is set to True, it is not possible to return the `attn_weights`.
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.67s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.51s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.58s/it]You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.62s/it]Loading checkpoint shards:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [00:02<00:02,  2.60s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.86s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.97s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  1.89s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.00s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.82s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.93s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  1.90s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:04<00:00,  2.01s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.87s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.98s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.87s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.99s/it]
Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.87s/it]Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:03<00:00,  1.98s/it]
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-30 11:26:38,801] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed info: version=0.14.4, git-hash=unknown, git-branch=unknown
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-30 11:26:48,154] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Flops Profiler Enabled: False
[2024-06-30 11:26:48,157] [INFO] [logging.py:96:log_dist] [Rank 0] Using client Optimizer as basic optimizer
[2024-06-30 11:26:48,157] [INFO] [logging.py:96:log_dist] [Rank 0] Removing param_group that has no 'params' in the basic Optimizer
[2024-06-30 11:26:48,215] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Basic Optimizer = AdamW
[2024-06-30 11:26:48,215] [INFO] [utils.py:56:is_zero_supported_optimizer] Checking ZeRO support for optimizer=AdamW type=<class 'torch.optim.adamw.AdamW'>
[2024-06-30 11:26:48,216] [INFO] [logging.py:96:log_dist] [Rank 0] Creating torch.bfloat16 ZeRO stage 2 optimizer
[2024-06-30 11:26:48,216] [INFO] [stage_1_and_2.py:148:__init__] Reduce bucket size 500,000,000
[2024-06-30 11:26:48,216] [INFO] [stage_1_and_2.py:149:__init__] Allgather bucket size 500,000,000
[2024-06-30 11:26:48,216] [INFO] [stage_1_and_2.py:150:__init__] CPU Offload: False
[2024-06-30 11:26:48,216] [INFO] [stage_1_and_2.py:151:__init__] Round robin gradient partitioning: False
[2024-06-30 11:26:48,749] [INFO] [utils.py:781:see_memory_usage] Before initializing optimizer states
[2024-06-30 11:26:48,750] [INFO] [utils.py:782:see_memory_usage] MA 7.71 GB         Max_MA 7.72 GB         CA 7.72 GB         Max_CA 8 GB 
[2024-06-30 11:26:48,750] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.84 GB, percent = 4.9%
[2024-06-30 11:26:48,917] [INFO] [utils.py:781:see_memory_usage] After initializing optimizer states
[2024-06-30 11:26:48,918] [INFO] [utils.py:782:see_memory_usage] MA 7.71 GB         Max_MA 7.72 GB         CA 7.74 GB         Max_CA 8 GB 
[2024-06-30 11:26:48,918] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.85 GB, percent = 4.9%
[2024-06-30 11:26:48,918] [INFO] [stage_1_and_2.py:543:__init__] optimizer state initialized
[2024-06-30 11:26:49,071] [INFO] [utils.py:781:see_memory_usage] After initializing ZeRO optimizer
[2024-06-30 11:26:49,072] [INFO] [utils.py:782:see_memory_usage] MA 7.71 GB         Max_MA 7.71 GB         CA 7.74 GB         Max_CA 8 GB 
[2024-06-30 11:26:49,072] [INFO] [utils.py:789:see_memory_usage] CPU Virtual Memory:  used = 48.87 GB, percent = 4.9%
[2024-06-30 11:26:49,072] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed Final Optimizer = DeepSpeedZeroOptimizer
[2024-06-30 11:26:49,073] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed using client LR scheduler
[2024-06-30 11:26:49,073] [INFO] [logging.py:96:log_dist] [Rank 0] DeepSpeed LR Scheduler = None
[2024-06-30 11:26:49,073] [INFO] [logging.py:96:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001], mom=[(0.9, 0.999)]
[2024-06-30 11:26:49,074] [INFO] [config.py:997:print] DeepSpeedEngine configuration:
[2024-06-30 11:26:49,074] [INFO] [config.py:1001:print]   activation_checkpointing_config  {
    "partition_activations": false, 
    "contiguous_memory_optimization": false, 
    "cpu_checkpointing": false, 
    "number_checkpoints": null, 
    "synchronize_checkpoint_boundary": false, 
    "profile": false
}
[2024-06-30 11:26:49,074] [INFO] [config.py:1001:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}
[2024-06-30 11:26:49,074] [INFO] [config.py:1001:print]   amp_enabled .................. False
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   amp_params ................... False
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   autotuning_config ............ {
    "enabled": false, 
    "start_step": null, 
    "end_step": null, 
    "metric_path": null, 
    "arg_mappings": null, 
    "metric": "throughput", 
    "model_info": null, 
    "results_dir": "autotuning_results", 
    "exps_dir": "autotuning_exps", 
    "overwrite": true, 
    "fast": true, 
    "start_profile_step": 3, 
    "end_profile_step": 5, 
    "tuner_type": "gridsearch", 
    "tuner_early_stopping": 5, 
    "tuner_num_trials": 50, 
    "model_info_path": null, 
    "mp_size": 1, 
    "max_train_batch_size": null, 
    "min_train_batch_size": 1, 
    "max_train_micro_batch_size_per_gpu": 1.024000e+03, 
    "min_train_micro_batch_size_per_gpu": 1, 
    "num_tuning_micro_batch_sizes": 3
}
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   bfloat16_enabled ............. True
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   bfloat16_immediate_grad_update  False
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   checkpoint_parallel_write_pipeline  False
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   checkpoint_tag_validation_enabled  True
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   checkpoint_tag_validation_fail  False
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   comms_config ................. <deepspeed.comm.config.DeepSpeedCommsConfig object at 0x7f5f080be380>
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   communication_data_type ...... None
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   compression_config ........... {'weight_quantization': {'shared_parameters': {'enabled': False, 'quantizer_kernel': False, 'schedule_offset': 0, 'quantize_groups': 1, 'quantize_verbose': False, 'quantization_type': 'symmetric', 'quantize_weight_in_forward': False, 'rounding': 'nearest', 'fp16_mixed_quantize': False, 'quantize_change_ratio': 0.001}, 'different_groups': {}}, 'activation_quantization': {'shared_parameters': {'enabled': False, 'quantization_type': 'symmetric', 'range_calibration': 'dynamic', 'schedule_offset': 1000}, 'different_groups': {}}, 'sparse_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'row_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'head_pruning': {'shared_parameters': {'enabled': False, 'method': 'topk', 'schedule_offset': 1000}, 'different_groups': {}}, 'channel_pruning': {'shared_parameters': {'enabled': False, 'method': 'l1', 'schedule_offset': 1000}, 'different_groups': {}}, 'layer_reduction': {'enabled': False}}
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   curriculum_enabled_legacy .... False
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   curriculum_params_legacy ..... False
[2024-06-30 11:26:49,075] [INFO] [config.py:1001:print]   data_efficiency_config ....... {'enabled': False, 'seed': 1234, 'data_sampling': {'enabled': False, 'num_epochs': 1000, 'num_workers': 0, 'curriculum_learning': {'enabled': False}}, 'data_routing': {'enabled': False, 'random_ltd': {'enabled': False, 'layer_token_lr_schedule': {'enabled': False}}}}
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   data_efficiency_enabled ...... False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   dataloader_drop_last ......... False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   disable_allgather ............ False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   dump_state ................... False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   dynamic_loss_scale_args ...... None
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   eigenvalue_enabled ........... False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   eigenvalue_gas_boundary_resolution  1
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   eigenvalue_layer_name ........ bert.encoder.layer
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   eigenvalue_layer_num ......... 0
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   eigenvalue_max_iter .......... 100
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   eigenvalue_stability ......... 1e-06
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   eigenvalue_tol ............... 0.01
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   eigenvalue_verbose ........... False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   elasticity_enabled ........... False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   flops_profiler_config ........ {
    "enabled": false, 
    "recompute_fwd_factor": 0.0, 
    "profile_step": 1, 
    "module_depth": -1, 
    "top_modules": 1, 
    "detailed": true, 
    "output_file": null
}
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   fp16_auto_cast ............... None
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   fp16_enabled ................. False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   fp16_master_weights_and_gradients  False
[2024-06-30 11:26:49,076] [INFO] [config.py:1001:print]   global_rank .................. 0
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   grad_accum_dtype ............. None
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   gradient_accumulation_steps .. 1
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   gradient_clipping ............ 1
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   gradient_predivide_factor .... 1.0
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   graph_harvesting ............. False
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   hybrid_engine ................ enabled=False max_out_tokens=512 inference_tp_size=1 release_inference_cache=False pin_parameters=True tp_gather_partition_size=8
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   initial_dynamic_scale ........ 1
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   load_universal_checkpoint .... False
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   loss_scale ................... 1.0
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   memory_breakdown ............. False
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   mics_hierarchial_params_gather  False
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   mics_shard_size .............. -1
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   monitor_config ............... tensorboard=TensorBoardConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') comet=CometConfig(enabled=False, samples_log_interval=100, project=None, workspace=None, api_key=None, experiment_name=None, experiment_key=None, online=None, mode=None) wandb=WandbConfig(enabled=False, group=None, team=None, project='deepspeed') csv_monitor=CSVConfig(enabled=False, output_path='', job_name='DeepSpeedJobName') enabled=False
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   nebula_config ................ {
    "enabled": false, 
    "persistent_storage_path": null, 
    "persistent_time_interval": 100, 
    "num_of_version_in_retention": 2, 
    "enable_nebula_load": true, 
    "load_path": null
}
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   optimizer_legacy_fusion ...... False
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   optimizer_name ............... None
[2024-06-30 11:26:49,077] [INFO] [config.py:1001:print]   optimizer_params ............. None
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0, 'pipe_partitioned': True, 'grad_partitioned': True}
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   pld_enabled .................. False
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   pld_params ................... False
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   prescale_gradients ........... False
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   scheduler_name ............... None
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   scheduler_params ............. None
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   seq_parallel_communication_data_type  torch.float32
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   sparse_attention ............. None
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   sparse_gradients_enabled ..... False
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   steps_per_print .............. 10000000000000
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   timers_config ................ enabled=True synchronized=True
[2024-06-30 11:26:49,078] [INFO] [config.py:1001:print]   train_batch_size ............. 256
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   train_micro_batch_size_per_gpu  32
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   use_data_before_expert_parallel_  False
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   use_node_local_storage ....... False
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   wall_clock_breakdown ......... False
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   weight_quantization_config ... None
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   world_size ................... 8
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   zero_allow_untested_optimizer  True
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   zero_config .................. stage=2 contiguous_gradients=True reduce_scatter=True reduce_bucket_size=500,000,000 use_multi_rank_bucket_allreduce=True allgather_partitions=True allgather_bucket_size=500,000,000 overlap_comm=True load_from_fp32_weights=True elastic_checkpoint=False offload_param=None offload_optimizer=None sub_group_size=1,000,000,000 cpu_offload_param=None cpu_offload_use_pin_memory=None cpu_offload=None prefetch_bucket_size=50,000,000 param_persistence_threshold=100,000 model_persistence_threshold=sys.maxsize max_live_parameters=1,000,000,000 max_reuse_distance=1,000,000,000 gather_16bit_weights_on_model_save=False use_all_reduce_for_fetch_params=False stage3_gather_fp16_weights_on_model_save=False ignore_unused_parameters=True legacy_stage1=False round_robin_gradients=False zero_hpz_partition_size=1 zero_quantized_weights=False zero_quantized_nontrainable_weights=False zero_quantized_gradients=False mics_shard_size=-1 mics_hierarchical_params_gather=False memory_efficient_linear=True pipeline_loading_checkpoint=False override_module_apply=True
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   zero_enabled ................. True
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   zero_force_ds_cpu_optimizer .. False
[2024-06-30 11:26:49,160] [INFO] [config.py:1001:print]   zero_optimization_stage ...... 2
[2024-06-30 11:26:49,161] [INFO] [config.py:987:print_user_config]   json = {
    "gradient_accumulation_steps": 1, 
    "train_micro_batch_size_per_gpu": 32, 
    "gradient_clipping": 1, 
    "zero_allow_untested_optimizer": true, 
    "zero_force_ds_cpu_optimizer": false, 
    "zero_optimization": {
        "stage": 2, 
        "overlap_comm": true
    }, 
    "fp16": {
        "enabled": false, 
        "initial_scale_power": 16
    }, 
    "bf16": {
        "enabled": true
    }, 
    "steps_per_print": 1.000000e+13
}
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 11:26:50 - mmengine - INFO - Num train samples 558128
06/30 11:26:50 - mmengine - INFO - train example:
06/30 11:26:50 - mmengine - INFO - <s><|user|><image>
Render a clear and concise summary of the photo.<|end|><|assistant|> select luxury furniture 3 - inch gel memory foam mattress topper<|end|> 

06/30 11:26:50 - mmengine - INFO - before_train in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 11:26:53 - mmengine - INFO - Sample output:
<|user|>
<image>
è¯·æè¿°ä¸€ä¸‹è¿™å¼ ç…§ç‰‡<|end|>
<|assistant|>
ä¸‹ï¼Œè¿™ä¸ªäººçš„æ„æ€ï¼Œè¿™ä¸ªäººçš„æ„æ€ï¼Œè¿™ä¸ªäººçš„æ„æ€ï¼Œè¿™ä¸ªäººçš„æ„æ€ï¼Œè¿™ä¸ªäººçš„æ„æ€ï¼Œè¿™ä¸ªäººçš„æ„æ€ï¼Œè¿™ä¸ªäººçš„æ„æ€

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 11:26:54 - mmengine - INFO - Sample output:
<|user|>
<image>
Please describe this picture<|end|>
<|assistant|>
The year was 1999, and the world was a bustling place. The economy was booming, and the stock market was soaring. The world was a place of opportunity and prosperity. But amidst all this prosper

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 11:26:54 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
06/30 11:26:54 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
06/30 11:26:54 - mmengine - INFO - Checkpoints will be saved to /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/30 11:27:45 - mmengine - INFO - Iter(train) [  10/2181]  lr: 1.4063e-04  eta: 3:05:25  time: 5.1248  data_time: 0.0035  memory: 22328  loss: 5.4189
06/30 11:28:31 - mmengine - INFO - Iter(train) [  20/2181]  lr: 2.9688e-04  eta: 2:54:18  time: 4.5549  data_time: 0.0036  memory: 22328  loss: 4.1209
06/30 11:29:14 - mmengine - INFO - Iter(train) [  30/2181]  lr: 4.5313e-04  eta: 2:47:14  time: 4.3152  data_time: 0.0035  memory: 22383  loss: 3.6412
06/30 11:29:55 - mmengine - INFO - Iter(train) [  40/2181]  lr: 6.0938e-04  eta: 2:41:21  time: 4.0932  data_time: 0.0034  memory: 22217  loss: 3.5165
06/30 11:30:35 - mmengine - INFO - Iter(train) [  50/2181]  lr: 7.6563e-04  eta: 2:36:44  time: 3.9775  data_time: 0.0034  memory: 22290  loss: 3.2703
06/30 11:31:12 - mmengine - INFO - Iter(train) [  60/2181]  lr: 9.2188e-04  eta: 2:32:00  time: 3.7364  data_time: 0.0034  memory: 22290  loss: 3.2619
06/30 11:31:53 - mmengine - INFO - Iter(train) [  70/2181]  lr: 9.9999e-04  eta: 2:30:10  time: 4.0752  data_time: 0.0035  memory: 22290  loss: 3.2020
06/30 11:32:30 - mmengine - INFO - Iter(train) [  80/2181]  lr: 9.9989e-04  eta: 2:27:14  time: 3.7634  data_time: 0.0034  memory: 22290  loss: 3.0848
06/30 11:33:08 - mmengine - INFO - Iter(train) [  90/2181]  lr: 9.9968e-04  eta: 2:24:45  time: 3.7427  data_time: 0.0034  memory: 22364  loss: 3.0193
06/30 11:33:45 - mmengine - INFO - Iter(train) [ 100/2181]  lr: 9.9936e-04  eta: 2:22:42  time: 3.7642  data_time: 0.0034  memory: 22328  loss: 2.9876
06/30 11:34:25 - mmengine - INFO - Iter(train) [ 110/2181]  lr: 9.9893e-04  eta: 2:21:24  time: 3.9176  data_time: 0.0034  memory: 22420  loss: 2.8497
06/30 11:35:02 - mmengine - INFO - Iter(train) [ 120/2181]  lr: 9.9839e-04  eta: 2:19:44  time: 3.7506  data_time: 0.0034  memory: 22290  loss: 2.8580
06/30 11:35:42 - mmengine - INFO - Iter(train) [ 130/2181]  lr: 9.9774e-04  eta: 2:18:54  time: 4.0134  data_time: 0.0034  memory: 22290  loss: 2.7027
06/30 11:36:22 - mmengine - INFO - Iter(train) [ 140/2181]  lr: 9.9699e-04  eta: 2:17:56  time: 3.9404  data_time: 0.0033  memory: 22346  loss: 2.6753
06/30 11:37:01 - mmengine - INFO - Iter(train) [ 150/2181]  lr: 9.9612e-04  eta: 2:16:54  time: 3.8962  data_time: 0.0034  memory: 22364  loss: 2.6431
06/30 11:37:38 - mmengine - INFO - Iter(train) [ 160/2181]  lr: 9.9514e-04  eta: 2:15:39  time: 3.7721  data_time: 0.0033  memory: 22346  loss: 2.5772
06/30 11:38:18 - mmengine - INFO - Iter(train) [ 170/2181]  lr: 9.9405e-04  eta: 2:14:50  time: 3.9586  data_time: 0.0034  memory: 22309  loss: 2.4770
06/30 11:38:56 - mmengine - INFO - Iter(train) [ 180/2181]  lr: 9.9286e-04  eta: 2:13:46  time: 3.8061  data_time: 0.0034  memory: 22309  loss: 2.5037
06/30 11:39:33 - mmengine - INFO - Iter(train) [ 190/2181]  lr: 9.9155e-04  eta: 2:12:38  time: 3.7439  data_time: 0.0034  memory: 22290  loss: 2.5734
06/30 11:40:11 - mmengine - INFO - Iter(train) [ 200/2181]  lr: 9.9014e-04  eta: 2:11:31  time: 3.7252  data_time: 0.0033  memory: 22309  loss: 2.4938
06/30 11:40:48 - mmengine - INFO - Iter(train) [ 210/2181]  lr: 9.8862e-04  eta: 2:10:28  time: 3.7339  data_time: 0.0033  memory: 22328  loss: 2.5438
06/30 11:41:25 - mmengine - INFO - Iter(train) [ 220/2181]  lr: 9.8699e-04  eta: 2:09:27  time: 3.7310  data_time: 0.0033  memory: 22346  loss: 2.4178
06/30 11:42:03 - mmengine - INFO - Iter(train) [ 230/2181]  lr: 9.8525e-04  eta: 2:08:28  time: 3.7328  data_time: 0.0033  memory: 22217  loss: 2.5027
06/30 11:42:40 - mmengine - INFO - Iter(train) [ 240/2181]  lr: 9.8341e-04  eta: 2:07:30  time: 3.7246  data_time: 0.0033  memory: 22328  loss: 2.4832
06/30 11:43:17 - mmengine - INFO - Iter(train) [ 250/2181]  lr: 9.8146e-04  eta: 2:06:34  time: 3.7244  data_time: 0.0034  memory: 22272  loss: 2.3665
06/30 11:43:56 - mmengine - INFO - Iter(train) [ 260/2181]  lr: 9.7940e-04  eta: 2:05:47  time: 3.8384  data_time: 0.0033  memory: 22290  loss: 2.5354
06/30 11:44:33 - mmengine - INFO - Iter(train) [ 270/2181]  lr: 9.7724e-04  eta: 2:04:54  time: 3.7332  data_time: 0.0033  memory: 22346  loss: 2.5168
06/30 11:45:10 - mmengine - INFO - Iter(train) [ 280/2181]  lr: 9.7497e-04  eta: 2:04:02  time: 3.7325  data_time: 0.0034  memory: 22254  loss: 2.4012
06/30 11:45:48 - mmengine - INFO - Iter(train) [ 290/2181]  lr: 9.7260e-04  eta: 2:03:12  time: 3.7405  data_time: 0.0034  memory: 22272  loss: 2.4464
06/30 11:46:25 - mmengine - INFO - Iter(train) [ 300/2181]  lr: 9.7013e-04  eta: 2:02:23  time: 3.7616  data_time: 0.0034  memory: 22236  loss: 2.4710
06/30 11:47:05 - mmengine - INFO - Iter(train) [ 310/2181]  lr: 9.6755e-04  eta: 2:01:49  time: 3.9817  data_time: 0.0034  memory: 22328  loss: 2.4591
06/30 11:47:43 - mmengine - INFO - Iter(train) [ 320/2181]  lr: 9.6487e-04  eta: 2:01:01  time: 3.7549  data_time: 0.0033  memory: 22364  loss: 2.3807
06/30 11:48:20 - mmengine - INFO - Iter(train) [ 330/2181]  lr: 9.6208e-04  eta: 2:00:14  time: 3.7561  data_time: 0.0034  memory: 22272  loss: 2.4158
06/30 11:48:58 - mmengine - INFO - Iter(train) [ 340/2181]  lr: 9.5920e-04  eta: 1:59:27  time: 3.7555  data_time: 0.0034  memory: 22309  loss: 2.3892
06/30 11:49:35 - mmengine - INFO - Iter(train) [ 350/2181]  lr: 9.5621e-04  eta: 1:58:41  time: 3.7550  data_time: 0.0034  memory: 22420  loss: 2.3951
06/30 11:50:13 - mmengine - INFO - Iter(train) [ 360/2181]  lr: 9.5312e-04  eta: 1:57:54  time: 3.7391  data_time: 0.0034  memory: 22236  loss: 2.4522
06/30 11:50:50 - mmengine - INFO - Iter(train) [ 370/2181]  lr: 9.4993e-04  eta: 1:57:09  time: 3.7449  data_time: 0.0034  memory: 22346  loss: 2.4308
06/30 11:51:29 - mmengine - INFO - Iter(train) [ 380/2181]  lr: 9.4664e-04  eta: 1:56:28  time: 3.8489  data_time: 0.0035  memory: 22272  loss: 2.4329
06/30 11:52:08 - mmengine - INFO - Iter(train) [ 390/2181]  lr: 9.4326e-04  eta: 1:55:54  time: 3.9707  data_time: 0.0034  memory: 22272  loss: 2.4149
06/30 11:52:46 - mmengine - INFO - Iter(train) [ 400/2181]  lr: 9.3977e-04  eta: 1:55:08  time: 3.7315  data_time: 0.0034  memory: 22290  loss: 2.3032
06/30 11:53:23 - mmengine - INFO - Iter(train) [ 410/2181]  lr: 9.3619e-04  eta: 1:54:23  time: 3.7381  data_time: 0.0034  memory: 22328  loss: 2.4403
06/30 11:54:03 - mmengine - INFO - Iter(train) [ 420/2181]  lr: 9.3252e-04  eta: 1:53:48  time: 3.9668  data_time: 0.0034  memory: 22290  loss: 2.4526
06/30 11:54:40 - mmengine - INFO - Iter(train) [ 430/2181]  lr: 9.2874e-04  eta: 1:53:04  time: 3.7314  data_time: 0.0034  memory: 22254  loss: 2.4691
06/30 11:55:17 - mmengine - INFO - Iter(train) [ 440/2181]  lr: 9.2488e-04  eta: 1:52:19  time: 3.7248  data_time: 0.0034  memory: 22236  loss: 2.3144
06/30 11:55:55 - mmengine - INFO - Iter(train) [ 450/2181]  lr: 9.2092e-04  eta: 1:51:35  time: 3.7288  data_time: 0.0034  memory: 22309  loss: 2.3953
06/30 11:56:32 - mmengine - INFO - Iter(train) [ 460/2181]  lr: 9.1687e-04  eta: 1:50:51  time: 3.7414  data_time: 0.0033  memory: 22272  loss: 2.3094
06/30 11:57:09 - mmengine - INFO - Iter(train) [ 470/2181]  lr: 9.1272e-04  eta: 1:50:08  time: 3.7337  data_time: 0.0034  memory: 22364  loss: 2.3849
06/30 11:57:47 - mmengine - INFO - Iter(train) [ 480/2181]  lr: 9.0848e-04  eta: 1:49:25  time: 3.7362  data_time: 0.0034  memory: 22290  loss: 2.3559
06/30 11:58:26 - mmengine - INFO - Iter(train) [ 490/2181]  lr: 9.0416e-04  eta: 1:48:48  time: 3.9165  data_time: 0.0034  memory: 22457  loss: 2.3983
06/30 11:59:03 - mmengine - INFO - Iter(train) [ 500/2181]  lr: 8.9974e-04  eta: 1:48:05  time: 3.7370  data_time: 0.0034  memory: 22272  loss: 2.3084
06/30 11:59:03 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 11:59:04 - mmengine - INFO - Sample output:
<|user|>
<image>
è¯·æè¿°ä¸€ä¸‹è¿™å¼ ç…§ç‰‡<|end|>
<|assistant|>
a wooden boardwalk over a lake with mountains in the background<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 11:59:04 - mmengine - INFO - Sample output:
<|user|>
<image>
Please describe this picture<|end|>
<|assistant|>
a wooden boardwalk overlooking a lake in the mountains<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 11:59:04 - mmengine - INFO - Saving checkpoint at 500 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-30 11:59:05,710] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_500.pth is about to be saved!
[2024-06-30 11:59:05,721] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/mp_rank_00_model_states.pt
[2024-06-30 11:59:05,722] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/mp_rank_00_model_states.pt...
[2024-06-30 11:59:05,749] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/mp_rank_00_model_states.pt.
[2024-06-30 11:59:05,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-30 11:59:05,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-30 11:59:05,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-30 11:59:05,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-30 11:59:05,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-30 11:59:05,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-30 11:59:05,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-30 11:59:05,751] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-30 11:59:05,766] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-30 11:59:05,766] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-30 11:59:05,766] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-30 11:59:05,766] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-30 11:59:05,766] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-30 11:59:05,766] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-30 11:59:05,767] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-30 11:59:05,767] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-30 11:59:05,767] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-30 11:59:05,767] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-30 11:59:05,767] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-30 11:59:05,767] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
[2024-06-30 11:59:05,768] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-30 11:59:05,769] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-30 11:59:05,769] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_500.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/30 11:59:43 - mmengine - INFO - Iter(train) [ 510/2181]  lr: 8.9524e-04  eta: 1:47:30  time: 3.9500  data_time: 0.2142  memory: 22401  loss: 2.3000
06/30 12:00:19 - mmengine - INFO - Iter(train) [ 520/2181]  lr: 8.9065e-04  eta: 1:46:45  time: 3.6643  data_time: 0.0034  memory: 22309  loss: 2.2710
06/30 12:00:57 - mmengine - INFO - Iter(train) [ 530/2181]  lr: 8.8597e-04  eta: 1:46:03  time: 3.7600  data_time: 0.0035  memory: 22290  loss: 2.3523
06/30 12:01:35 - mmengine - INFO - Iter(train) [ 540/2181]  lr: 8.8121e-04  eta: 1:45:24  time: 3.8259  data_time: 0.0035  memory: 22254  loss: 2.3184
06/30 12:02:13 - mmengine - INFO - Iter(train) [ 550/2181]  lr: 8.7637e-04  eta: 1:44:43  time: 3.7801  data_time: 0.0034  memory: 22309  loss: 2.3358
06/30 12:02:52 - mmengine - INFO - Iter(train) [ 560/2181]  lr: 8.7144e-04  eta: 1:44:06  time: 3.8837  data_time: 0.0034  memory: 22290  loss: 2.3142
06/30 12:03:33 - mmengine - INFO - Iter(train) [ 570/2181]  lr: 8.6643e-04  eta: 1:43:34  time: 4.0929  data_time: 0.0035  memory: 22767  loss: 2.3243
06/30 12:04:10 - mmengine - INFO - Iter(train) [ 580/2181]  lr: 8.6134e-04  eta: 1:42:52  time: 3.7409  data_time: 0.0034  memory: 22383  loss: 2.3293
06/30 12:04:48 - mmengine - INFO - Iter(train) [ 590/2181]  lr: 8.5617e-04  eta: 1:42:10  time: 3.7368  data_time: 0.0035  memory: 22346  loss: 2.3013
06/30 12:05:25 - mmengine - INFO - Iter(train) [ 600/2181]  lr: 8.5092e-04  eta: 1:41:29  time: 3.7334  data_time: 0.0034  memory: 22309  loss: 2.4125
06/30 12:06:02 - mmengine - INFO - Iter(train) [ 610/2181]  lr: 8.4559e-04  eta: 1:40:47  time: 3.7222  data_time: 0.0034  memory: 22328  loss: 2.3031
06/30 12:06:39 - mmengine - INFO - Iter(train) [ 620/2181]  lr: 8.4019e-04  eta: 1:40:05  time: 3.7310  data_time: 0.0034  memory: 22254  loss: 2.2368
06/30 12:07:17 - mmengine - INFO - Iter(train) [ 630/2181]  lr: 8.3471e-04  eta: 1:39:24  time: 3.7331  data_time: 0.0035  memory: 22217  loss: 2.3121
06/30 12:07:54 - mmengine - INFO - Iter(train) [ 640/2181]  lr: 8.2916e-04  eta: 1:38:43  time: 3.7288  data_time: 0.0034  memory: 22254  loss: 2.2650
06/30 12:08:31 - mmengine - INFO - Iter(train) [ 650/2181]  lr: 8.2354e-04  eta: 1:38:01  time: 3.7251  data_time: 0.0034  memory: 22217  loss: 2.3090
06/30 12:09:09 - mmengine - INFO - Iter(train) [ 660/2181]  lr: 8.1784e-04  eta: 1:37:21  time: 3.7350  data_time: 0.0034  memory: 22383  loss: 2.3298
06/30 12:09:47 - mmengine - INFO - Iter(train) [ 670/2181]  lr: 8.1208e-04  eta: 1:36:42  time: 3.8246  data_time: 0.0034  memory: 22309  loss: 2.3201
06/30 12:10:24 - mmengine - INFO - Iter(train) [ 680/2181]  lr: 8.0624e-04  eta: 1:36:01  time: 3.7269  data_time: 0.0034  memory: 22272  loss: 2.2888
06/30 12:11:01 - mmengine - INFO - Iter(train) [ 690/2181]  lr: 8.0034e-04  eta: 1:35:20  time: 3.7288  data_time: 0.0034  memory: 22328  loss: 2.3736
06/30 12:11:39 - mmengine - INFO - Iter(train) [ 700/2181]  lr: 7.9437e-04  eta: 1:34:40  time: 3.7454  data_time: 0.0034  memory: 22290  loss: 2.3332
06/30 12:12:16 - mmengine - INFO - Iter(train) [ 710/2181]  lr: 7.8834e-04  eta: 1:33:59  time: 3.7232  data_time: 0.0034  memory: 22290  loss: 2.3028
06/30 12:12:53 - mmengine - INFO - Iter(train) [ 720/2181]  lr: 7.8224e-04  eta: 1:33:19  time: 3.7372  data_time: 0.0034  memory: 22254  loss: 2.2757
06/30 12:13:31 - mmengine - INFO - Iter(train) [ 730/2181]  lr: 7.7609e-04  eta: 1:32:39  time: 3.7437  data_time: 0.0034  memory: 22346  loss: 2.3211
06/30 12:14:08 - mmengine - INFO - Iter(train) [ 740/2181]  lr: 7.6987e-04  eta: 1:31:59  time: 3.7440  data_time: 0.0034  memory: 22290  loss: 2.2881
06/30 12:14:46 - mmengine - INFO - Iter(train) [ 750/2181]  lr: 7.6359e-04  eta: 1:31:19  time: 3.7494  data_time: 0.0034  memory: 22272  loss: 2.2319
06/30 12:15:23 - mmengine - INFO - Iter(train) [ 760/2181]  lr: 7.5725e-04  eta: 1:30:39  time: 3.7534  data_time: 0.0035  memory: 22309  loss: 2.2759
06/30 12:16:01 - mmengine - INFO - Iter(train) [ 770/2181]  lr: 7.5086e-04  eta: 1:30:00  time: 3.7732  data_time: 0.0034  memory: 22328  loss: 2.3518
06/30 12:16:39 - mmengine - INFO - Iter(train) [ 780/2181]  lr: 7.4441e-04  eta: 1:29:21  time: 3.7636  data_time: 0.0034  memory: 22346  loss: 2.3366
06/30 12:17:16 - mmengine - INFO - Iter(train) [ 790/2181]  lr: 7.3790e-04  eta: 1:28:41  time: 3.7574  data_time: 0.0035  memory: 22290  loss: 2.2962
06/30 12:17:54 - mmengine - INFO - Iter(train) [ 800/2181]  lr: 7.3135e-04  eta: 1:28:02  time: 3.7504  data_time: 0.0034  memory: 22309  loss: 2.2350
06/30 12:18:31 - mmengine - INFO - Iter(train) [ 810/2181]  lr: 7.2474e-04  eta: 1:27:22  time: 3.7300  data_time: 0.0034  memory: 22290  loss: 2.2661
06/30 12:19:08 - mmengine - INFO - Iter(train) [ 820/2181]  lr: 7.1809e-04  eta: 1:26:42  time: 3.7300  data_time: 0.0034  memory: 22181  loss: 2.3263
06/30 12:19:46 - mmengine - INFO - Iter(train) [ 830/2181]  lr: 7.1138e-04  eta: 1:26:02  time: 3.7281  data_time: 0.0034  memory: 22401  loss: 2.2632
06/30 12:20:23 - mmengine - INFO - Iter(train) [ 840/2181]  lr: 7.0463e-04  eta: 1:25:23  time: 3.7344  data_time: 0.0034  memory: 22474  loss: 2.2490
06/30 12:21:00 - mmengine - INFO - Iter(train) [ 850/2181]  lr: 6.9784e-04  eta: 1:24:43  time: 3.7415  data_time: 0.0034  memory: 22346  loss: 2.2583
06/30 12:21:38 - mmengine - INFO - Iter(train) [ 860/2181]  lr: 6.9100e-04  eta: 1:24:04  time: 3.7518  data_time: 0.0034  memory: 22364  loss: 2.2959
06/30 12:22:15 - mmengine - INFO - Iter(train) [ 870/2181]  lr: 6.8412e-04  eta: 1:23:24  time: 3.7336  data_time: 0.0034  memory: 22420  loss: 2.2623
06/30 12:22:53 - mmengine - INFO - Iter(train) [ 880/2181]  lr: 6.7720e-04  eta: 1:22:45  time: 3.7306  data_time: 0.0034  memory: 22383  loss: 2.2306
06/30 12:23:30 - mmengine - INFO - Iter(train) [ 890/2181]  lr: 6.7023e-04  eta: 1:22:06  time: 3.7372  data_time: 0.0034  memory: 22272  loss: 2.3479
06/30 12:24:07 - mmengine - INFO - Iter(train) [ 900/2181]  lr: 6.6324e-04  eta: 1:21:26  time: 3.7393  data_time: 0.0034  memory: 22383  loss: 2.2349
06/30 12:24:45 - mmengine - INFO - Iter(train) [ 910/2181]  lr: 6.5620e-04  eta: 1:20:47  time: 3.7296  data_time: 0.0034  memory: 22420  loss: 2.1865
06/30 12:25:22 - mmengine - INFO - Iter(train) [ 920/2181]  lr: 6.4913e-04  eta: 1:20:08  time: 3.7381  data_time: 0.0034  memory: 22346  loss: 2.3290
06/30 12:26:00 - mmengine - INFO - Iter(train) [ 930/2181]  lr: 6.4203e-04  eta: 1:19:29  time: 3.7463  data_time: 0.0034  memory: 22272  loss: 2.2768
06/30 12:26:37 - mmengine - INFO - Iter(train) [ 940/2181]  lr: 6.3490e-04  eta: 1:18:50  time: 3.7454  data_time: 0.0034  memory: 22364  loss: 2.2136
06/30 12:27:14 - mmengine - INFO - Iter(train) [ 950/2181]  lr: 6.2774e-04  eta: 1:18:11  time: 3.7406  data_time: 0.0035  memory: 22290  loss: 2.2745
06/30 12:27:52 - mmengine - INFO - Iter(train) [ 960/2181]  lr: 6.2054e-04  eta: 1:17:32  time: 3.7427  data_time: 0.0034  memory: 22199  loss: 2.3281
06/30 12:28:29 - mmengine - INFO - Iter(train) [ 970/2181]  lr: 6.1333e-04  eta: 1:16:53  time: 3.7526  data_time: 0.0034  memory: 22364  loss: 2.2899
06/30 12:29:10 - mmengine - INFO - Iter(train) [ 980/2181]  lr: 6.0608e-04  eta: 1:16:18  time: 4.0323  data_time: 0.0034  memory: 22309  loss: 2.3621
06/30 12:29:47 - mmengine - INFO - Iter(train) [ 990/2181]  lr: 5.9882e-04  eta: 1:15:39  time: 3.7500  data_time: 0.0034  memory: 22328  loss: 2.2988
06/30 12:30:25 - mmengine - INFO - Exp name: llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy_20240630_112119
06/30 12:30:25 - mmengine - INFO - Iter(train) [1000/2181]  lr: 5.9153e-04  eta: 1:15:00  time: 3.7572  data_time: 0.0034  memory: 22346  loss: 2.3662
06/30 12:30:25 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 12:30:25 - mmengine - INFO - Sample output:
<|user|>
<image>
è¯·æè¿°ä¸€ä¸‹è¿™å¼ ç…§ç‰‡<|end|>
<|assistant|>
a wooden dock with a boat on it<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 12:30:25 - mmengine - INFO - Sample output:
<|user|>
<image>
Please describe this picture<|end|>
<|assistant|>
a wooden dock with a boat on it in the water<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 12:30:25 - mmengine - INFO - Saving checkpoint at 1000 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-30 12:30:26,969] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_1000.pth is about to be saved!
[2024-06-30 12:30:26,979] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/mp_rank_00_model_states.pt
[2024-06-30 12:30:26,980] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/mp_rank_00_model_states.pt...
[2024-06-30 12:30:27,007] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/mp_rank_00_model_states.pt.
[2024-06-30 12:30:27,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-30 12:30:27,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-30 12:30:27,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-30 12:30:27,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-30 12:30:27,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-30 12:30:27,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-30 12:30:27,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-30 12:30:27,009] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-30 12:30:27,024] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-30 12:30:27,024] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-30 12:30:27,024] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-30 12:30:27,024] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-30 12:30:27,024] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-30 12:30:27,024] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-30 12:30:27,024] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-30 12:30:27,025] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-30 12:30:27,024] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-30 12:30:27,025] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-30 12:30:27,025] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-30 12:30:27,025] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-30 12:30:27,025] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-30 12:30:27,025] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-30 12:30:27,025] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-30 12:30:27,025] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-30 12:30:27,025] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-30 12:30:27,025] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-30 12:30:27,025] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-30 12:30:27,025] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-30 12:30:27,025] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
[2024-06-30 12:30:27,027] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-30 12:30:27,028] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-30 12:30:27,028] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1000.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/30 12:31:04 - mmengine - INFO - Iter(train) [1010/2181]  lr: 5.8422e-04  eta: 1:14:23  time: 3.9344  data_time: 0.1841  memory: 22328  loss: 2.3502
06/30 12:31:42 - mmengine - INFO - Iter(train) [1020/2181]  lr: 5.7690e-04  eta: 1:13:44  time: 3.7451  data_time: 0.0034  memory: 22290  loss: 2.2754
06/30 12:32:20 - mmengine - INFO - Iter(train) [1030/2181]  lr: 5.6955e-04  eta: 1:13:07  time: 3.8719  data_time: 0.0034  memory: 22346  loss: 2.3220
06/30 12:32:58 - mmengine - INFO - Iter(train) [1040/2181]  lr: 5.6220e-04  eta: 1:12:28  time: 3.7347  data_time: 0.0034  memory: 22217  loss: 2.2762
06/30 12:33:35 - mmengine - INFO - Iter(train) [1050/2181]  lr: 5.5482e-04  eta: 1:11:49  time: 3.7464  data_time: 0.0033  memory: 22290  loss: 2.2137
06/30 12:34:12 - mmengine - INFO - Iter(train) [1060/2181]  lr: 5.4744e-04  eta: 1:11:10  time: 3.7346  data_time: 0.0034  memory: 22328  loss: 2.2861
06/30 12:34:50 - mmengine - INFO - Iter(train) [1070/2181]  lr: 5.4004e-04  eta: 1:10:31  time: 3.7325  data_time: 0.0034  memory: 22309  loss: 2.1892
06/30 12:35:27 - mmengine - INFO - Iter(train) [1080/2181]  lr: 5.3264e-04  eta: 1:09:53  time: 3.7326  data_time: 0.0034  memory: 22309  loss: 2.2229
06/30 12:36:04 - mmengine - INFO - Iter(train) [1090/2181]  lr: 5.2523e-04  eta: 1:09:14  time: 3.7245  data_time: 0.0034  memory: 22328  loss: 2.2364
06/30 12:36:42 - mmengine - INFO - Iter(train) [1100/2181]  lr: 5.1781e-04  eta: 1:08:35  time: 3.7281  data_time: 0.0034  memory: 22309  loss: 2.2602
06/30 12:37:19 - mmengine - INFO - Iter(train) [1110/2181]  lr: 5.1039e-04  eta: 1:07:56  time: 3.7292  data_time: 0.0034  memory: 22290  loss: 2.2704
06/30 12:37:56 - mmengine - INFO - Iter(train) [1120/2181]  lr: 5.0297e-04  eta: 1:07:17  time: 3.7339  data_time: 0.0034  memory: 22492  loss: 2.2377
06/30 12:38:33 - mmengine - INFO - Iter(train) [1130/2181]  lr: 4.9555e-04  eta: 1:06:38  time: 3.7289  data_time: 0.0034  memory: 22217  loss: 2.2733
06/30 12:39:11 - mmengine - INFO - Iter(train) [1140/2181]  lr: 4.8812e-04  eta: 1:06:00  time: 3.7350  data_time: 0.0034  memory: 22290  loss: 2.2478
06/30 12:39:48 - mmengine - INFO - Iter(train) [1150/2181]  lr: 4.8070e-04  eta: 1:05:21  time: 3.7376  data_time: 0.0034  memory: 22328  loss: 2.3062
06/30 12:40:26 - mmengine - INFO - Iter(train) [1160/2181]  lr: 4.7329e-04  eta: 1:04:42  time: 3.7398  data_time: 0.0034  memory: 22328  loss: 2.1635
06/30 12:41:03 - mmengine - INFO - Iter(train) [1170/2181]  lr: 4.6588e-04  eta: 1:04:04  time: 3.7547  data_time: 0.0034  memory: 22364  loss: 2.2957
06/30 12:41:41 - mmengine - INFO - Iter(train) [1180/2181]  lr: 4.5848e-04  eta: 1:03:26  time: 3.7631  data_time: 0.0034  memory: 22236  loss: 2.2871
06/30 12:42:18 - mmengine - INFO - Iter(train) [1190/2181]  lr: 4.5108e-04  eta: 1:02:47  time: 3.7636  data_time: 0.0034  memory: 22272  loss: 2.2905
06/30 12:42:56 - mmengine - INFO - Iter(train) [1200/2181]  lr: 4.4370e-04  eta: 1:02:09  time: 3.7543  data_time: 0.0034  memory: 22401  loss: 2.2898
06/30 12:43:34 - mmengine - INFO - Iter(train) [1210/2181]  lr: 4.3633e-04  eta: 1:01:31  time: 3.7588  data_time: 0.0034  memory: 22236  loss: 2.2326
06/30 12:44:11 - mmengine - INFO - Iter(train) [1220/2181]  lr: 4.2898e-04  eta: 1:00:52  time: 3.7595  data_time: 0.0033  memory: 22328  loss: 2.2563
06/30 12:44:51 - mmengine - INFO - Iter(train) [1230/2181]  lr: 4.2164e-04  eta: 1:00:15  time: 3.9410  data_time: 0.0034  memory: 22290  loss: 2.3032
06/30 12:45:28 - mmengine - INFO - Iter(train) [1240/2181]  lr: 4.1431e-04  eta: 0:59:37  time: 3.7659  data_time: 0.0034  memory: 22364  loss: 2.2466
06/30 12:46:06 - mmengine - INFO - Iter(train) [1250/2181]  lr: 4.0701e-04  eta: 0:58:58  time: 3.7366  data_time: 0.0033  memory: 22217  loss: 2.2488
06/30 12:46:43 - mmengine - INFO - Iter(train) [1260/2181]  lr: 3.9973e-04  eta: 0:58:20  time: 3.7443  data_time: 0.0034  memory: 22364  loss: 2.2601
06/30 12:47:20 - mmengine - INFO - Iter(train) [1270/2181]  lr: 3.9246e-04  eta: 0:57:42  time: 3.7404  data_time: 0.0034  memory: 22364  loss: 2.2467
06/30 12:47:58 - mmengine - INFO - Iter(train) [1280/2181]  lr: 3.8523e-04  eta: 0:57:03  time: 3.7411  data_time: 0.0033  memory: 22309  loss: 2.1962
06/30 12:48:35 - mmengine - INFO - Iter(train) [1290/2181]  lr: 3.7801e-04  eta: 0:56:25  time: 3.7263  data_time: 0.0034  memory: 22236  loss: 2.2576
06/30 12:49:12 - mmengine - INFO - Iter(train) [1300/2181]  lr: 3.7083e-04  eta: 0:55:46  time: 3.7226  data_time: 0.0034  memory: 22401  loss: 2.1759
06/30 12:49:50 - mmengine - INFO - Iter(train) [1310/2181]  lr: 3.6367e-04  eta: 0:55:08  time: 3.7326  data_time: 0.0034  memory: 22254  loss: 2.2035
06/30 12:50:27 - mmengine - INFO - Iter(train) [1320/2181]  lr: 3.5655e-04  eta: 0:54:29  time: 3.7362  data_time: 0.0034  memory: 22309  loss: 2.2827
06/30 12:51:04 - mmengine - INFO - Iter(train) [1330/2181]  lr: 3.4945e-04  eta: 0:53:51  time: 3.7307  data_time: 0.0034  memory: 22346  loss: 2.2061
06/30 12:51:42 - mmengine - INFO - Iter(train) [1340/2181]  lr: 3.4239e-04  eta: 0:53:13  time: 3.7360  data_time: 0.0034  memory: 22272  loss: 2.1921
06/30 12:52:19 - mmengine - INFO - Iter(train) [1350/2181]  lr: 3.3536e-04  eta: 0:52:34  time: 3.7364  data_time: 0.0034  memory: 22290  loss: 2.2107
06/30 12:52:56 - mmengine - INFO - Iter(train) [1360/2181]  lr: 3.2837e-04  eta: 0:51:56  time: 3.7315  data_time: 0.0033  memory: 22364  loss: 2.2199
06/30 12:53:34 - mmengine - INFO - Iter(train) [1370/2181]  lr: 3.2142e-04  eta: 0:51:18  time: 3.7401  data_time: 0.0033  memory: 22328  loss: 2.2026
06/30 12:54:11 - mmengine - INFO - Iter(train) [1380/2181]  lr: 3.1450e-04  eta: 0:50:39  time: 3.7435  data_time: 0.0034  memory: 22328  loss: 2.2467
06/30 12:54:49 - mmengine - INFO - Iter(train) [1390/2181]  lr: 3.0763e-04  eta: 0:50:01  time: 3.7405  data_time: 0.0034  memory: 22309  loss: 2.2717
06/30 12:55:26 - mmengine - INFO - Iter(train) [1400/2181]  lr: 3.0080e-04  eta: 0:49:23  time: 3.7576  data_time: 0.0034  memory: 22438  loss: 2.1880
06/30 12:56:04 - mmengine - INFO - Iter(train) [1410/2181]  lr: 2.9401e-04  eta: 0:48:45  time: 3.7548  data_time: 0.0034  memory: 22309  loss: 2.2205
06/30 12:56:41 - mmengine - INFO - Iter(train) [1420/2181]  lr: 2.8727e-04  eta: 0:48:07  time: 3.7692  data_time: 0.0034  memory: 22364  loss: 2.2834
06/30 12:57:19 - mmengine - INFO - Iter(train) [1430/2181]  lr: 2.8058e-04  eta: 0:47:29  time: 3.7576  data_time: 0.0035  memory: 22383  loss: 2.2878
06/30 12:57:57 - mmengine - INFO - Iter(train) [1440/2181]  lr: 2.7393e-04  eta: 0:46:50  time: 3.7538  data_time: 0.0034  memory: 22309  loss: 2.2943
06/30 12:58:34 - mmengine - INFO - Iter(train) [1450/2181]  lr: 2.6734e-04  eta: 0:46:12  time: 3.7498  data_time: 0.0034  memory: 22364  loss: 2.2019
06/30 12:59:11 - mmengine - INFO - Iter(train) [1460/2181]  lr: 2.6079e-04  eta: 0:45:34  time: 3.7443  data_time: 0.0034  memory: 22346  loss: 2.1839
06/30 12:59:49 - mmengine - INFO - Iter(train) [1470/2181]  lr: 2.5430e-04  eta: 0:44:56  time: 3.7397  data_time: 0.0035  memory: 22309  loss: 2.2517
06/30 13:00:26 - mmengine - INFO - Iter(train) [1480/2181]  lr: 2.4786e-04  eta: 0:44:17  time: 3.6692  data_time: 0.0034  memory: 22272  loss: 2.3358
06/30 13:01:03 - mmengine - INFO - Iter(train) [1490/2181]  lr: 2.4148e-04  eta: 0:43:39  time: 3.7565  data_time: 0.0035  memory: 22383  loss: 2.1782
06/30 13:01:41 - mmengine - INFO - Iter(train) [1500/2181]  lr: 2.3515e-04  eta: 0:43:01  time: 3.7554  data_time: 0.0034  memory: 22236  loss: 2.2282
06/30 13:01:41 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:01:41 - mmengine - INFO - Sample output:
<|user|>
<image>
è¯·æè¿°ä¸€ä¸‹è¿™å¼ ç…§ç‰‡<|end|>
<|assistant|>
a wooden dock with a boat in the water<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:01:42 - mmengine - INFO - Sample output:
<|user|>
<image>
Please describe this picture<|end|>
<|assistant|>
a wooden dock with a boat in the water and a mountain in the background<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:01:42 - mmengine - INFO - Saving checkpoint at 1500 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-30 13:01:43,295] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_1500.pth is about to be saved!
[2024-06-30 13:01:43,306] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/mp_rank_00_model_states.pt
[2024-06-30 13:01:43,306] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/mp_rank_00_model_states.pt...
[2024-06-30 13:01:43,334] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/mp_rank_00_model_states.pt.
[2024-06-30 13:01:43,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-30 13:01:43,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-30 13:01:43,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-30 13:01:43,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-30 13:01:43,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-30 13:01:43,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-30 13:01:43,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-30 13:01:43,336] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-30 13:01:43,352] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-30 13:01:43,352] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-30 13:01:43,352] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-30 13:01:43,352] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-30 13:01:43,352] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-30 13:01:43,352] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-30 13:01:43,353] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-30 13:01:43,353] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-30 13:01:43,353] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-30 13:01:43,353] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-30 13:01:43,353] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
[2024-06-30 13:01:43,355] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-30 13:01:43,356] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_1500.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-30 13:01:43,356] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_1500.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/30 13:02:20 - mmengine - INFO - Iter(train) [1510/2181]  lr: 2.2889e-04  eta: 0:42:24  time: 3.9713  data_time: 0.2195  memory: 22328  loss: 2.3119
06/30 13:02:58 - mmengine - INFO - Iter(train) [1520/2181]  lr: 2.2268e-04  eta: 0:41:46  time: 3.7465  data_time: 0.0034  memory: 22254  loss: 2.3203
06/30 13:03:35 - mmengine - INFO - Iter(train) [1530/2181]  lr: 2.1653e-04  eta: 0:41:08  time: 3.7434  data_time: 0.0034  memory: 22254  loss: 2.2701
06/30 13:04:13 - mmengine - INFO - Iter(train) [1540/2181]  lr: 2.1045e-04  eta: 0:40:30  time: 3.7400  data_time: 0.0034  memory: 22236  loss: 2.2331
06/30 13:04:50 - mmengine - INFO - Iter(train) [1550/2181]  lr: 2.0443e-04  eta: 0:39:52  time: 3.7489  data_time: 0.0034  memory: 22254  loss: 2.1601
06/30 13:05:28 - mmengine - INFO - Iter(train) [1560/2181]  lr: 1.9847e-04  eta: 0:39:14  time: 3.7444  data_time: 0.0034  memory: 22420  loss: 2.1488
06/30 13:06:05 - mmengine - INFO - Iter(train) [1570/2181]  lr: 1.9259e-04  eta: 0:38:35  time: 3.7164  data_time: 0.0034  memory: 22383  loss: 2.1855
06/30 13:06:42 - mmengine - INFO - Iter(train) [1580/2181]  lr: 1.8676e-04  eta: 0:37:57  time: 3.7292  data_time: 0.0034  memory: 22254  loss: 2.1865
06/30 13:07:19 - mmengine - INFO - Iter(train) [1590/2181]  lr: 1.8101e-04  eta: 0:37:19  time: 3.7279  data_time: 0.0033  memory: 22272  loss: 2.2061
06/30 13:07:57 - mmengine - INFO - Iter(train) [1600/2181]  lr: 1.7533e-04  eta: 0:36:41  time: 3.7574  data_time: 0.0034  memory: 22272  loss: 2.2565
06/30 13:08:38 - mmengine - INFO - Iter(train) [1610/2181]  lr: 1.6972e-04  eta: 0:36:04  time: 4.0829  data_time: 0.0034  memory: 22878  loss: 2.1653
06/30 13:09:15 - mmengine - INFO - Iter(train) [1620/2181]  lr: 1.6419e-04  eta: 0:35:26  time: 3.7374  data_time: 0.0034  memory: 22217  loss: 2.2625
06/30 13:09:53 - mmengine - INFO - Iter(train) [1630/2181]  lr: 1.5872e-04  eta: 0:34:48  time: 3.7533  data_time: 0.0034  memory: 22345  loss: 2.1852
06/30 13:10:30 - mmengine - INFO - Iter(train) [1640/2181]  lr: 1.5334e-04  eta: 0:34:10  time: 3.7625  data_time: 0.0036  memory: 22309  loss: 2.2018
06/30 13:11:08 - mmengine - INFO - Iter(train) [1650/2181]  lr: 1.4802e-04  eta: 0:33:32  time: 3.7522  data_time: 0.0034  memory: 22254  loss: 2.1891
06/30 13:11:45 - mmengine - INFO - Iter(train) [1660/2181]  lr: 1.4279e-04  eta: 0:32:54  time: 3.7573  data_time: 0.0033  memory: 22419  loss: 2.1903
06/30 13:12:23 - mmengine - INFO - Iter(train) [1670/2181]  lr: 1.3764e-04  eta: 0:32:16  time: 3.7506  data_time: 0.0034  memory: 22364  loss: 2.2373
06/30 13:13:00 - mmengine - INFO - Iter(train) [1680/2181]  lr: 1.3256e-04  eta: 0:31:38  time: 3.7555  data_time: 0.0034  memory: 22345  loss: 2.2000
06/30 13:13:38 - mmengine - INFO - Iter(train) [1690/2181]  lr: 1.2757e-04  eta: 0:31:00  time: 3.7373  data_time: 0.0034  memory: 22364  loss: 2.2011
06/30 13:14:15 - mmengine - INFO - Iter(train) [1700/2181]  lr: 1.2266e-04  eta: 0:30:22  time: 3.7400  data_time: 0.0034  memory: 22254  loss: 2.1362
06/30 13:14:53 - mmengine - INFO - Iter(train) [1710/2181]  lr: 1.1783e-04  eta: 0:29:44  time: 3.7945  data_time: 0.0034  memory: 22290  loss: 2.2297
06/30 13:15:31 - mmengine - INFO - Iter(train) [1720/2181]  lr: 1.1308e-04  eta: 0:29:06  time: 3.7322  data_time: 0.0034  memory: 22272  loss: 2.1294
06/30 13:16:08 - mmengine - INFO - Iter(train) [1730/2181]  lr: 1.0842e-04  eta: 0:28:28  time: 3.7375  data_time: 0.0034  memory: 22327  loss: 2.2123
06/30 13:16:45 - mmengine - INFO - Iter(train) [1740/2181]  lr: 1.0385e-04  eta: 0:27:50  time: 3.7428  data_time: 0.0034  memory: 22437  loss: 2.1067
06/30 13:17:23 - mmengine - INFO - Iter(train) [1750/2181]  lr: 9.9366e-05  eta: 0:27:12  time: 3.7370  data_time: 0.0034  memory: 22327  loss: 2.2489
06/30 13:18:00 - mmengine - INFO - Iter(train) [1760/2181]  lr: 9.4969e-05  eta: 0:26:34  time: 3.7388  data_time: 0.0034  memory: 22272  loss: 2.0724
06/30 13:18:37 - mmengine - INFO - Iter(train) [1770/2181]  lr: 9.0661e-05  eta: 0:25:56  time: 3.7313  data_time: 0.0034  memory: 22290  loss: 2.1771
06/30 13:19:15 - mmengine - INFO - Iter(train) [1780/2181]  lr: 8.6443e-05  eta: 0:25:18  time: 3.7316  data_time: 0.0034  memory: 22290  loss: 2.2135
06/30 13:19:52 - mmengine - INFO - Iter(train) [1790/2181]  lr: 8.2317e-05  eta: 0:24:40  time: 3.7472  data_time: 0.0034  memory: 22345  loss: 2.2265
06/30 13:20:30 - mmengine - INFO - Iter(train) [1800/2181]  lr: 7.8282e-05  eta: 0:24:02  time: 3.7351  data_time: 0.0034  memory: 22290  loss: 2.1568
06/30 13:21:07 - mmengine - INFO - Iter(train) [1810/2181]  lr: 7.4341e-05  eta: 0:23:24  time: 3.7304  data_time: 0.0034  memory: 22272  loss: 2.2128
06/30 13:21:44 - mmengine - INFO - Iter(train) [1820/2181]  lr: 7.0493e-05  eta: 0:22:46  time: 3.7340  data_time: 0.0034  memory: 22327  loss: 2.2643
06/30 13:22:22 - mmengine - INFO - Iter(train) [1830/2181]  lr: 6.6740e-05  eta: 0:22:08  time: 3.7408  data_time: 0.0034  memory: 22309  loss: 2.2133
06/30 13:22:59 - mmengine - INFO - Iter(train) [1840/2181]  lr: 6.3083e-05  eta: 0:21:30  time: 3.7462  data_time: 0.0034  memory: 22364  loss: 2.1937
06/30 13:23:37 - mmengine - INFO - Iter(train) [1850/2181]  lr: 5.9521e-05  eta: 0:20:52  time: 3.7502  data_time: 0.0034  memory: 22382  loss: 2.1538
06/30 13:24:14 - mmengine - INFO - Iter(train) [1860/2181]  lr: 5.6057e-05  eta: 0:20:14  time: 3.7496  data_time: 0.0034  memory: 22290  loss: 2.2268
06/30 13:24:52 - mmengine - INFO - Iter(train) [1870/2181]  lr: 5.2691e-05  eta: 0:19:37  time: 3.7546  data_time: 0.0034  memory: 22290  loss: 2.1864
06/30 13:25:29 - mmengine - INFO - Iter(train) [1880/2181]  lr: 4.9424e-05  eta: 0:18:59  time: 3.7556  data_time: 0.0034  memory: 22290  loss: 2.2450
06/30 13:26:07 - mmengine - INFO - Iter(train) [1890/2181]  lr: 4.6255e-05  eta: 0:18:21  time: 3.7528  data_time: 0.0034  memory: 22290  loss: 2.1061
06/30 13:26:44 - mmengine - INFO - Iter(train) [1900/2181]  lr: 4.3187e-05  eta: 0:17:43  time: 3.7489  data_time: 0.0034  memory: 22199  loss: 2.1542
06/30 13:27:22 - mmengine - INFO - Iter(train) [1910/2181]  lr: 4.0219e-05  eta: 0:17:05  time: 3.7582  data_time: 0.0034  memory: 22345  loss: 2.1790
06/30 13:27:59 - mmengine - INFO - Iter(train) [1920/2181]  lr: 3.7353e-05  eta: 0:16:27  time: 3.7405  data_time: 0.0034  memory: 22236  loss: 2.2560
06/30 13:28:37 - mmengine - INFO - Iter(train) [1930/2181]  lr: 3.4589e-05  eta: 0:15:49  time: 3.7397  data_time: 0.0034  memory: 22364  loss: 2.2401
06/30 13:29:14 - mmengine - INFO - Iter(train) [1940/2181]  lr: 3.1927e-05  eta: 0:15:11  time: 3.7379  data_time: 0.0034  memory: 22272  loss: 2.2077
06/30 13:29:51 - mmengine - INFO - Iter(train) [1950/2181]  lr: 2.9369e-05  eta: 0:14:33  time: 3.7340  data_time: 0.0034  memory: 22217  loss: 2.1186
06/30 13:30:29 - mmengine - INFO - Iter(train) [1960/2181]  lr: 2.6914e-05  eta: 0:13:56  time: 3.7364  data_time: 0.0034  memory: 22272  loss: 2.2360
06/30 13:31:06 - mmengine - INFO - Iter(train) [1970/2181]  lr: 2.4564e-05  eta: 0:13:18  time: 3.7346  data_time: 0.0034  memory: 22290  loss: 2.1644
06/30 13:31:44 - mmengine - INFO - Iter(train) [1980/2181]  lr: 2.2318e-05  eta: 0:12:40  time: 3.8223  data_time: 0.0034  memory: 22455  loss: 2.1820
06/30 13:32:22 - mmengine - INFO - Iter(train) [1990/2181]  lr: 2.0178e-05  eta: 0:12:02  time: 3.7306  data_time: 0.0034  memory: 22290  loss: 2.1353
06/30 13:32:59 - mmengine - INFO - Exp name: llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy_20240630_112119
06/30 13:32:59 - mmengine - INFO - Iter(train) [2000/2181]  lr: 1.8143e-05  eta: 0:11:24  time: 3.7400  data_time: 0.0034  memory: 22345  loss: 2.1626
06/30 13:32:59 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:32:59 - mmengine - INFO - Sample output:
<|user|>
<image>
è¯·æè¿°ä¸€ä¸‹è¿™å¼ ç…§ç‰‡<|end|>
<|assistant|>
a wooden dock in the middle of a lake<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:33:00 - mmengine - INFO - Sample output:
<|user|>
<image>
Please describe this picture<|end|>
<|assistant|>
a wooden dock in the middle of a lake with a mountain in the background<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:33:00 - mmengine - INFO - Saving checkpoint at 2000 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-30 13:33:01,484] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_2000.pth is about to be saved!
[2024-06-30 13:33:01,495] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/mp_rank_00_model_states.pt
[2024-06-30 13:33:01,495] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/mp_rank_00_model_states.pt...
[2024-06-30 13:33:01,524] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/mp_rank_00_model_states.pt.
[2024-06-30 13:33:01,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-30 13:33:01,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-30 13:33:01,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-30 13:33:01,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-30 13:33:01,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-30 13:33:01,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-30 13:33:01,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-30 13:33:01,525] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-30 13:33:01,540] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-30 13:33:01,540] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-30 13:33:01,540] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-30 13:33:01,540] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-30 13:33:01,541] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-30 13:33:01,541] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-30 13:33:01,541] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-30 13:33:01,541] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-30 13:33:01,541] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-30 13:33:01,541] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-30 13:33:01,541] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-30 13:33:01,542] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
[2024-06-30 13:33:01,542] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-30 13:33:01,542] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2000.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-30 13:33:01,542] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2000.pth is ready now!
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
/root/autodl-tmp/tzn/anaconda3/envs/xtuner/lib/python3.10/site-packages/torch/utils/checkpoint.py:464: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.
  warnings.warn(
06/30 13:33:38 - mmengine - INFO - Iter(train) [2010/2181]  lr: 1.6215e-05  eta: 0:10:46  time: 3.9512  data_time: 0.2174  memory: 22364  loss: 2.1187
06/30 13:34:16 - mmengine - INFO - Iter(train) [2020/2181]  lr: 1.4393e-05  eta: 0:10:09  time: 3.7315  data_time: 0.0034  memory: 22327  loss: 2.1123
06/30 13:34:53 - mmengine - INFO - Iter(train) [2030/2181]  lr: 1.2678e-05  eta: 0:09:31  time: 3.7305  data_time: 0.0034  memory: 22290  loss: 2.1649
06/30 13:35:30 - mmengine - INFO - Iter(train) [2040/2181]  lr: 1.1071e-05  eta: 0:08:53  time: 3.7331  data_time: 0.0034  memory: 22272  loss: 2.1061
06/30 13:36:08 - mmengine - INFO - Iter(train) [2050/2181]  lr: 9.5712e-06  eta: 0:08:15  time: 3.7391  data_time: 0.0034  memory: 22272  loss: 2.1824
06/30 13:36:45 - mmengine - INFO - Iter(train) [2060/2181]  lr: 8.1798e-06  eta: 0:07:37  time: 3.7492  data_time: 0.0034  memory: 22309  loss: 2.2707
06/30 13:37:24 - mmengine - INFO - Iter(train) [2070/2181]  lr: 6.8967e-06  eta: 0:06:59  time: 3.9159  data_time: 0.0034  memory: 22290  loss: 2.1893
06/30 13:38:02 - mmengine - INFO - Iter(train) [2080/2181]  lr: 5.7224e-06  eta: 0:06:22  time: 3.7529  data_time: 0.0034  memory: 22345  loss: 2.1276
06/30 13:38:40 - mmengine - INFO - Iter(train) [2090/2181]  lr: 4.6570e-06  eta: 0:05:44  time: 3.7567  data_time: 0.0033  memory: 22382  loss: 2.2513
06/30 13:39:20 - mmengine - INFO - Iter(train) [2100/2181]  lr: 3.7008e-06  eta: 0:05:06  time: 4.0665  data_time: 0.0034  memory: 22382  loss: 2.1354
06/30 13:39:58 - mmengine - INFO - Iter(train) [2110/2181]  lr: 2.8540e-06  eta: 0:04:28  time: 3.7489  data_time: 0.0034  memory: 22272  loss: 2.1474
06/30 13:40:35 - mmengine - INFO - Iter(train) [2120/2181]  lr: 2.1168e-06  eta: 0:03:50  time: 3.7644  data_time: 0.0034  memory: 22327  loss: 2.1634
06/30 13:41:13 - mmengine - INFO - Iter(train) [2130/2181]  lr: 1.4894e-06  eta: 0:03:12  time: 3.7523  data_time: 0.0034  memory: 22327  loss: 2.1567
06/30 13:41:50 - mmengine - INFO - Iter(train) [2140/2181]  lr: 9.7178e-07  eta: 0:02:35  time: 3.7423  data_time: 0.0034  memory: 22364  loss: 2.2191
06/30 13:42:28 - mmengine - INFO - Iter(train) [2150/2181]  lr: 5.6419e-07  eta: 0:01:57  time: 3.8197  data_time: 0.0034  memory: 22254  loss: 2.2741
06/30 13:43:06 - mmengine - INFO - Iter(train) [2160/2181]  lr: 2.6670e-07  eta: 0:01:19  time: 3.7342  data_time: 0.0034  memory: 22327  loss: 2.1979
06/30 13:43:43 - mmengine - INFO - Iter(train) [2170/2181]  lr: 7.9352e-08  eta: 0:00:41  time: 3.7372  data_time: 0.0034  memory: 22345  loss: 2.2571
06/30 13:44:21 - mmengine - INFO - Iter(train) [2180/2181]  lr: 2.2043e-09  eta: 0:00:03  time: 3.7354  data_time: 0.0037  memory: 22364  loss: 2.0672
06/30 13:44:21 - mmengine - INFO - Exp name: llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy_20240630_112119
06/30 13:44:21 - mmengine - INFO - after_train_iter in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:44:22 - mmengine - INFO - Sample output:
<|user|>
<image>
è¯·æè¿°ä¸€ä¸‹è¿™å¼ ç…§ç‰‡<|end|>
<|assistant|>
a wooden dock in the middle of a lake<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:44:22 - mmengine - INFO - Sample output:
<|user|>
<image>
Please describe this picture<|end|>
<|assistant|>
a wooden dock in the middle of a lake with a mountain in the background<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:44:22 - mmengine - INFO - Saving checkpoint at 2181 iterations
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
[2024-06-30 13:44:23,669] [INFO] [logging.py:96:log_dist] [Rank 0] [Torch] Checkpoint iter_2181.pth is about to be saved!
[2024-06-30 13:44:23,680] [INFO] [logging.py:96:log_dist] [Rank 0] Saving model checkpoint: /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/mp_rank_00_model_states.pt
[2024-06-30 13:44:23,680] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/mp_rank_00_model_states.pt...
[2024-06-30 13:44:23,709] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/mp_rank_00_model_states.pt.
[2024-06-30 13:44:23,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt...
[2024-06-30 13:44:23,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt...
[2024-06-30 13:44:23,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt...
[2024-06-30 13:44:23,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt...
[2024-06-30 13:44:23,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt...
[2024-06-30 13:44:23,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt...
[2024-06-30 13:44:23,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt...
[2024-06-30 13:44:23,711] [INFO] [torch_checkpoint_engine.py:21:save] [Torch] Saving /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt...
[2024-06-30 13:44:23,726] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt.
[2024-06-30 13:44:23,726] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_4_mp_rank_00_optim_states.pt
[2024-06-30 13:44:23,726] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2181.pth is ready now!
[2024-06-30 13:44:23,726] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt.
[2024-06-30 13:44:23,726] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_3_mp_rank_00_optim_states.pt
[2024-06-30 13:44:23,726] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt.
[2024-06-30 13:44:23,726] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2181.pth is ready now!
[2024-06-30 13:44:23,726] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt.
[2024-06-30 13:44:23,726] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_5_mp_rank_00_optim_states.pt
[2024-06-30 13:44:23,726] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt.
[2024-06-30 13:44:23,727] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_7_mp_rank_00_optim_states.pt
[2024-06-30 13:44:23,727] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2181.pth is ready now!
[2024-06-30 13:44:23,727] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_6_mp_rank_00_optim_states.pt
[2024-06-30 13:44:23,727] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2181.pth is ready now!
[2024-06-30 13:44:23,727] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2181.pth is ready now!
[2024-06-30 13:44:23,727] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt.
[2024-06-30 13:44:23,727] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_2_mp_rank_00_optim_states.pt
[2024-06-30 13:44:23,727] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2181.pth is ready now!
[2024-06-30 13:44:23,727] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt.
[2024-06-30 13:44:23,727] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_1_mp_rank_00_optim_states.pt
[2024-06-30 13:44:23,727] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2181.pth is ready now!
[2024-06-30 13:44:23,727] [INFO] [torch_checkpoint_engine.py:23:save] [Torch] Saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt.
[2024-06-30 13:44:23,728] [INFO] [engine.py:3478:_save_zero_checkpoint] zero checkpoint saved /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth/bf16_zero_pp_rank_0_mp_rank_00_optim_states.pt
[2024-06-30 13:44:23,728] [INFO] [torch_checkpoint_engine.py:33:commit] [Torch] Checkpoint iter_2181.pth is ready now!
06/30 13:44:23 - mmengine - INFO - after_train in EvaluateChatHook.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:44:24 - mmengine - INFO - Sample output:
<|user|>
<image>
è¯·æè¿°ä¸€ä¸‹è¿™å¼ ç…§ç‰‡<|end|>
<|assistant|>
a wooden dock in the lake with a boat in the background<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
06/30 13:44:24 - mmengine - INFO - Sample output:
<|user|>
<image>
Please describe this picture<|end|>
<|assistant|>
a wooden dock in the middle of a lake with a mountain in the background<|end|>

You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
You are using an old version of the checkpointing format that is deprecated (We will also silently ignore `gradient_checkpointing_kwargs` in case you passed it).Please update to the new format on your modeling file. To use the new format, you need to completely remove the definition of the method `_set_gradient_checkpointing` in your model.
wandb: - 4.930 MB of 4.930 MB uploaded (4.903 MB deduped)wandb: \ 4.930 MB of 4.930 MB uploaded (4.903 MB deduped)wandb: | 4.930 MB of 4.930 MB uploaded (4.903 MB deduped)wandb: / 4.930 MB of 4.930 MB uploaded (4.903 MB deduped)wandb: - 4.930 MB of 4.930 MB uploaded (4.903 MB deduped)wandb: \ 4.930 MB of 4.930 MB uploaded (4.903 MB deduped)wandb: | 4.930 MB of 4.930 MB uploaded (4.903 MB deduped)wandb: / 4.930 MB of 4.930 MB uploaded (4.903 MB deduped)wandb: - 4.965 MB of 4.965 MB uploaded (4.903 MB deduped)wandb: \ 4.973 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: | 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: / 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: - 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: \ 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: | 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: / 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: - 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: \ 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: | 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: / 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: - 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: \ 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: | 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: / 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: - 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: \ 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: | 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: / 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: - 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: \ 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: | 5.048 MB of 5.054 MB uploaded (4.903 MB deduped)wandb: / 5.054 MB of 5.054 MB uploaded (4.903 MB deduped)wandb:                                                                                
wandb: W&B sync reduced upload amount by 97.0%             
wandb: 
wandb: Run history:
wandb: data_time â–‡â–ƒâ–‚â–‚â–â–‚â–ƒâ–‚â–â–‚â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–‚â–‚â–‚â–‚â–â–ƒâ–ƒâ–‚â–ƒâ–ƒâ–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–ƒâ–ˆ
wandb:      iter â–â–â–â–‚â–‚â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–„â–…â–…â–…â–…â–…â–…â–†â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ
wandb:      loss â–ˆâ–…â–„â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–‚â–â–â–â–â–‚â–â–‚â–â–â–â–â–â–â–â–‚â–â–
wandb:        lr â–ƒâ–‡â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–‡â–‡â–†â–†â–†â–†â–…â–…â–…â–…â–„â–„â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–
wandb:    memory â–„â–ƒâ–ƒâ–„â–„â–‚â–„â–ƒâ–ƒâ–ƒâ–ƒâ–‚â–ƒâ–ƒâ–…â–ˆâ–…â–â–…â–„â–ˆâ–‚â–„â–„â–ƒâ–‡â–…â–‚â–†â–â–„â–ƒâ–ƒâ–…â–â–ƒâ–„â–„â–„â–…
wandb:      time â–ˆâ–â–â–‚â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–
wandb: 
wandb: Run summary:
wandb: data_time 0.00368
wandb:      iter 2180
wandb:      loss 2.0672
wandb:        lr 0.0
wandb:    memory 22364
wandb:      time 3.73535
wandb: 
wandb: ğŸš€ View run charmed-durian-8 at: https://wandb.ai/jzyztzn/lava-clipL32-Phi-3-mini/runs/r4hb3aac
wandb: â­ï¸ View project at: https://wandb.ai/jzyztzn/lava-clipL32-Phi-3-mini
wandb: Synced 6 W&B file(s), 0 media file(s), 736 artifact file(s) and 0 other file(s)
wandb: Find logs at: ./work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/20240630_112119/vis_data/wandb/run-20240630_112126-r4hb3aac/logs
wandb: WARNING The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require("core")`! See https://wandb.me/wandb-core for more information.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
06/30 13:55:12 - mmengine - WARNING - Use random port: 20781
[2024-06-30 13:55:14,118] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 13:55:22,372] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 13:55:22,386] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 13:55:22,441] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 13:55:22,458] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 13:55:22,463] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 13:55:22,476] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 13:55:22,485] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[2024-06-30 13:55:22,489] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m async_io requires the dev libaio .so object and headers but these were not found.
[93m [WARNING] [0m async_io: please install the libaio-dev package with apt
[93m [WARNING] [0m If libaio is already installed (perhaps from source), try setting the CFLAGS and LDFLAGS environment variables to where it can be found.
[93m [WARNING] [0m Please specify the CUTLASS repo directory as environment variable $CUTLASS_PATH
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[93m [WARNING] [0m sparse_attn requires a torch version >= 1.5 and < 2.0 but detected 2.3
[93m [WARNING] [0m using untested triton version (2.3.1), only 1.0.0 is known to be compatible
[2024-06-30 13:55:24,299] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 13:55:24,299] [INFO] [comm.py:668:init_distributed] Initializing TorchBackend in DeepSpeed with backend nccl
[2024-06-30 13:55:25,480] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 13:55:25,513] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 13:55:25,528] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 13:55:25,529] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 13:55:25,529] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 13:55:25,530] [INFO] [comm.py:637:init_distributed] cdb=None
[2024-06-30 13:55:25,566] [INFO] [comm.py:637:init_distributed] cdb=None
06/30 13:55:29 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: linux
    Python: 3.10.14 (main, May  6 2024, 19:42:50) [GCC 11.2.0]
    CUDA available: True
    MUSA available: False
    numpy_random_seed: 541056542
    GPU 0,1,2,3,4,5,6,7: NVIDIA A800 80GB PCIe
    CUDA_HOME: /usr/local/cuda
    NVCC: Cuda compilation tools, release 12.1, V12.1.105
    GCC: gcc (Ubuntu 11.4.0-1ubuntu1~22.04) 11.4.0
    PyTorch: 2.3.1+cu121
    PyTorch compiling details: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v3.3.6 (Git Hash 86e6af5974177e513fd3fee58425e1063e7f1361)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: AVX512
  - CUDA Runtime 12.1
  - NVCC architecture flags: -gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_90,code=sm_90
  - CuDNN 8.9.2
  - Magma 2.6.1
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=12.1, CUDNN_VERSION=8.9.2, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wsuggest-override -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_VERSION=2.3.1, USE_CUDA=ON, USE_CUDNN=ON, USE_CUSPARSELT=1, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_GLOO=ON, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=1, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, USE_ROCM_KERNEL_ASSERT=OFF, 

    TorchVision: 0.18.1+cu121
    OpenCV: 4.10.0
    MMEngine: 0.10.4

Runtime environment:
    launcher: pytorch
    randomness: {'seed': None, 'deterministic': False}
    cudnn_benchmark: False
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: None
    deterministic: False
    Distributed launcher: pytorch
    Distributed training: True
    GPU number: 8
------------------------------------------------------------

06/30 13:55:30 - mmengine - INFO - Config:
SYSTEM = ''
accumulative_counts = 2
batch_size = 8
betas = (
    0.9,
    0.999,
)
custom_hooks = [
    dict(
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.engine.hooks.DatasetInfoHook'),
    dict(
        evaluation_images='view.jpg',
        evaluation_inputs=[
            'è¯·æè¿°ä¸€ä¸‹è¿™å¼ ç…§ç‰‡',
            'Please describe this picture',
        ],
        every_n_iters=1000,
        image_processor=dict(
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
            trust_remote_code=True,
            type='transformers.CLIPImageProcessor.from_pretrained'),
        prompt_template='xtuner.utils.PROMPT_TEMPLATE.phi3_chat',
        system='',
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.engine.hooks.EvaluateChatHook'),
]
data_path = './data/llava_data/LLaVA-Instruct-150K/llava_v1_5_mix665k.json'
data_root = './data/llava_data/'
dataloader_num_workers = 4
default_hooks = dict(
    checkpoint=dict(
        by_epoch=False,
        interval=1000,
        max_keep_ckpts=2,
        type='mmengine.hooks.CheckpointHook'),
    logger=dict(
        interval=10,
        log_metric_by_epoch=False,
        type='mmengine.hooks.LoggerHook'),
    param_scheduler=dict(type='mmengine.hooks.ParamSchedulerHook'),
    sampler_seed=dict(type='mmengine.hooks.DistSamplerSeedHook'),
    timer=dict(type='mmengine.hooks.IterTimerHook'))
env_cfg = dict(
    cudnn_benchmark=False,
    dist_cfg=dict(backend='nccl'),
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0))
evaluation_freq = 1000
evaluation_images = 'view.jpg'
evaluation_inputs = [
    'è¯·æè¿°ä¸€ä¸‹è¿™å¼ ç…§ç‰‡',
    'Please describe this picture',
]
image_folder = './data/llava_data/llava_images'
image_processor = dict(
    pretrained_model_name_or_path=
    '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
    trust_remote_code=True,
    type='transformers.CLIPImageProcessor.from_pretrained')
launcher = 'pytorch'
llava_dataset = dict(
    data_path='./data/llava_data/LLaVA-Instruct-150K/llava_v1_5_mix665k.json',
    dataset_map_fn='xtuner.dataset.map_fns.llava_map_fn',
    image_folder='./data/llava_data/llava_images',
    image_processor=dict(
        pretrained_model_name_or_path=
        '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
        trust_remote_code=True,
        type='transformers.CLIPImageProcessor.from_pretrained'),
    max_length=1472,
    pad_image_to_square=True,
    template_map_fn=dict(
        template='xtuner.utils.PROMPT_TEMPLATE.phi3_chat',
        type='xtuner.dataset.map_fns.template_map_fn_factory'),
    tokenizer=dict(
        padding_side='right',
        pretrained_model_name_or_path=
        '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
        trust_remote_code=True,
        type='transformers.AutoTokenizer.from_pretrained'),
    type='xtuner.dataset.LLaVADataset')
llm_name_or_path = '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct'
load_from = None
log_level = 'INFO'
log_processor = dict(by_epoch=False)
lr = 2e-05
max_epochs = 1
max_length = 1472
max_norm = 1
model = dict(
    freeze_llm=False,
    freeze_visual_encoder=True,
    llm=dict(
        pretrained_model_name_or_path=
        '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
        trust_remote_code=True,
        type='transformers.AutoModelForCausalLM.from_pretrained'),
    pretrained_pth=
    './work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth',
    type='xtuner.model.LLaVAModel',
    visual_encoder=dict(
        pretrained_model_name_or_path=
        '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
        type='transformers.CLIPVisionModel.from_pretrained'))
optim_type = 'torch.optim.AdamW'
optim_wrapper = dict(
    optimizer=dict(
        betas=(
            0.9,
            0.999,
        ),
        lr=2e-05,
        type='torch.optim.AdamW',
        weight_decay=0),
    type='DeepSpeedOptimWrapper')
param_scheduler = [
    dict(
        begin=0,
        by_epoch=True,
        convert_to_iter_based=True,
        end=0.03,
        start_factor=1e-05,
        type='mmengine.optim.LinearLR'),
    dict(
        begin=0.03,
        by_epoch=True,
        convert_to_iter_based=True,
        end=1,
        eta_min=0.0,
        type='mmengine.optim.CosineAnnealingLR'),
]
pretrained_pth = './work_dirs/llava_phi3_mini_4k_instruct_clip_vit_large_p14_336_e1_gpu8_pretrain_copy/iter_2181.pth'
prompt_template = 'xtuner.utils.PROMPT_TEMPLATE.phi3_chat'
randomness = dict(deterministic=False, seed=None)
resume = False
runner_type = 'FlexibleRunner'
save_steps = 1000
save_total_limit = 2
strategy = dict(
    config=dict(
        bf16=dict(enabled=True),
        fp16=dict(enabled=False, initial_scale_power=16),
        gradient_accumulation_steps='auto',
        gradient_clipping='auto',
        train_micro_batch_size_per_gpu='auto',
        zero_allow_untested_optimizer=True,
        zero_force_ds_cpu_optimizer=False,
        zero_optimization=dict(overlap_comm=True, stage=2)),
    exclude_frozen_parameters=True,
    gradient_accumulation_steps=2,
    gradient_clipping=1,
    sequence_parallel_size=1,
    train_micro_batch_size_per_gpu=8,
    type='xtuner.engine.DeepSpeedStrategy')
tokenizer = dict(
    padding_side='right',
    pretrained_model_name_or_path=
    '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
    trust_remote_code=True,
    type='transformers.AutoTokenizer.from_pretrained')
train_cfg = dict(max_epochs=1, type='xtuner.engine.runner.TrainLoop')
train_dataloader = dict(
    batch_size=8,
    collate_fn=dict(type='xtuner.dataset.collate_fns.default_collate_fn'),
    dataset=dict(
        data_path=
        './data/llava_data/LLaVA-Instruct-150K/llava_v1_5_mix665k.json',
        dataset_map_fn='xtuner.dataset.map_fns.llava_map_fn',
        image_folder='./data/llava_data/llava_images',
        image_processor=dict(
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336',
            trust_remote_code=True,
            type='transformers.CLIPImageProcessor.from_pretrained'),
        max_length=1472,
        pad_image_to_square=True,
        template_map_fn=dict(
            template='xtuner.utils.PROMPT_TEMPLATE.phi3_chat',
            type='xtuner.dataset.map_fns.template_map_fn_factory'),
        tokenizer=dict(
            padding_side='right',
            pretrained_model_name_or_path=
            '/root/autodl-tmp/tzn/Projects/pretrained/LLM-Research/Phi-3-mini-4k-instruct',
            trust_remote_code=True,
            type='transformers.AutoTokenizer.from_pretrained'),
        type='xtuner.dataset.LLaVADataset'),
    num_workers=4,
    pin_memory=True,
    sampler=dict(
        length_property='modality_length',
        per_device_batch_size=16,
        type='xtuner.dataset.samplers.LengthGroupedSampler'))
use_wandb = True
visual_encoder_name_or_path = '/root/autodl-tmp/tzn/Projects/pretrained/clip/clip-vit-large-patch14-336'
visualizer = dict(
    type='mmengine.visualization.Visualizer',
    vis_backends=[
        dict(
            init_kwargs=dict(project='lava-clipL32-Phi-3-mini'),
            type='mmengine.visualization.WandbVisBackend'),
    ])
wandb_name = 'lava-clipL32-Phi-3-mini'
warmup_ratio = 0.03
weight_decay = 0
work_dir = './work_dirs/llava_phi3_mini_4k_instruct_full_clip_vit_large_p14_336_e1_gpu8_finetune_copy'

Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
wandb: Currently logged in as: jzyztzn. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.17.3
wandb: Run data is saved locally in /root/autodl-tmp/tzn/Projects/xtuner/work_dirs/llava_phi3_mini_4k_instruct_full_clip_vit_large_p14_336_e1_gpu8_finetune_copy/20240630_135524/vis_data/wandb/run-20240630_135531-zcidfim1
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run cosmic-darkness-9
wandb: â­ï¸ View project at https://wandb.ai/jzyztzn/lava-clipL32-Phi-3-mini
wandb: ğŸš€ View run at https://wandb.ai/jzyztzn/lava-clipL32-Phi-3-mini/runs/zcidfim1
06/30 13:55:37 - mmengine - WARNING - Failed to search registry with scope "mmengine" in the "builder" registry tree. As a workaround, the current "builder" registry in "xtuner" is used to build instance. This may cause unexpected failure when running the built modules. Please check whether "mmengine" is a correct scope, or whether the registry is initialized.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
06/30 13:55:37 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DatasetInfoHook                    
(LOW         ) EvaluateChatHook                   
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(LOW         ) EvaluateChatHook                   
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) DatasetInfoHook                    
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_val:
(VERY_HIGH   ) RuntimeInfoHook                    
(LOW         ) EvaluateChatHook                   
 -------------------- 
after_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(LOW         ) EvaluateChatHook                   
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) DatasetInfoHook                    
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test:
(VERY_HIGH   ) RuntimeInfoHook                    
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
06/30 13:55:58 - mmengine - INFO - xtuner_dataset_timeout = 1:00:00
Map (num_proc=32):   0%|          | 0/665298 [00:00<?, ? examples/s]Map (num_proc=32):   0%|          | 456/665298 [00:05<2:22:10, 77.94 examples/s]Map (num_proc=32):   0%|          | 1658/665298 [00:06<30:52, 358.33 examples/s]Map (num_proc=32):   0%|          | 2509/665298 [00:06<17:45, 622.13 examples/s]Map (num_proc=32):   1%|          | 3401/665298 [00:06<11:15, 979.35 examples/s]Map (num_proc=32):   1%|          | 4186/665298 [00:06<07:57, 1384.42 examples/s]Map (num_proc=32):   1%|          | 5000/665298 [00:06<05:49, 1891.94 examples/s]Map (num_proc=32):   1%|          | 6000/665298 [00:06<04:09, 2642.37 examples/s]Map (num_proc=32):   1%|          | 7000/665298 [00:06<03:25, 3195.65 examples/s]Map (num_proc=32):   1%|          | 7991/665298 [00:06<02:43, 4029.17 examples/s]Map (num_proc=32):   1%|â–         | 9022/665298 [00:06<02:11, 4980.84 examples/s]Map (num_proc=32):   2%|â–         | 10000/665298 [00:07<04:40, 2339.12 examples/s]Map (num_proc=32):   2%|â–         | 11661/665298 [00:08<03:01, 3603.30 examples/s]Map (num_proc=32):   2%|â–         | 12500/665298 [00:08<05:10, 2100.91 examples/s]Map (num_proc=32):   3%|â–         | 21242/665298 [00:09<01:49, 5906.69 examples/s]Map (num_proc=32):   3%|â–         | 22391/665298 [00:09<01:45, 6089.95 examples/s]Map (num_proc=32):   4%|â–         | 23294/665298 [00:09<01:42, 6255.18 examples/s]Map (num_proc=32):   4%|â–         | 24290/665298 [00:10<01:45, 6087.54 examples/s]Map (num_proc=32):   4%|â–         | 25791/665298 [00:10<01:38, 6463.83 examples/s]Map (num_proc=32):   4%|â–         | 26785/665298 [00:10<02:30, 4256.24 examples/s]Map (num_proc=32):   4%|â–         | 27448/665298 [00:11<02:49, 3756.17 examples/s]Map (num_proc=32):   4%|â–         | 29791/665298 [00:11<01:48, 5848.14 examples/s]Map (num_proc=32):   5%|â–         | 30768/665298 [00:12<03:31, 3001.97 examples/s]Map (num_proc=32):   6%|â–‹         | 41582/665298 [00:12<00:54, 11451.73 examples/s]Map (num_proc=32):   7%|â–‹         | 44582/665298 [00:13<01:34, 6586.42 examples/s] Map (num_proc=32):   7%|â–‹         | 47513/665298 [00:13<01:16, 8082.03 examples/s]Map (num_proc=32):   8%|â–Š         | 50039/665298 [00:14<01:51, 5499.94 examples/s]Map (num_proc=32):   8%|â–Š         | 52554/665298 [00:14<01:30, 6773.77 examples/s]Map (num_proc=32):   8%|â–Š         | 54550/665298 [00:15<02:19, 4367.01 examples/s]Map (num_proc=32):  10%|â–‰         | 63212/665298 [00:16<01:21, 7349.20 examples/s]Map (num_proc=32):  10%|â–ˆ         | 68304/665298 [00:16<00:58, 10129.09 examples/s]Map (num_proc=32):  11%|â–ˆ         | 71288/665298 [00:16<00:54, 10825.19 examples/s]Map (num_proc=32):  11%|â–ˆ         | 73375/665298 [00:16<00:54, 10766.68 examples/s]Map (num_proc=32):  11%|â–ˆâ–        | 75373/665298 [00:16<00:50, 11584.19 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 77664/665298 [00:17<01:17, 7596.22 examples/s] Map (num_proc=32):  12%|â–ˆâ–        | 79287/665298 [00:17<01:22, 7142.44 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 81373/665298 [00:17<01:07, 8643.06 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 84002/665298 [00:19<02:16, 4272.93 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 86164/665298 [00:19<01:50, 5256.94 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 87770/665298 [00:19<01:33, 6198.81 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 89164/665298 [00:19<01:21, 7052.85 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 90631/665298 [00:19<01:11, 8092.56 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 92164/665298 [00:19<01:02, 9159.73 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 93620/665298 [00:19<00:56, 10169.26 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 95164/665298 [00:19<00:51, 11082.55 examples/s]Map (num_proc=32):  15%|â–ˆâ–        | 96649/665298 [00:20<00:51, 11129.13 examples/s]Map (num_proc=32):  15%|â–ˆâ–        | 98164/665298 [00:20<00:48, 11755.81 examples/s]Map (num_proc=32):  15%|â–ˆâ–        | 99764/665298 [00:20<02:02, 4627.63 examples/s] Map (num_proc=32):  15%|â–ˆâ–Œ        | 102041/665298 [00:21<01:27, 6412.14 examples/s]Map (num_proc=32):  16%|â–ˆâ–Œ        | 104792/665298 [00:22<02:41, 3480.34 examples/s]Map (num_proc=32):  16%|â–ˆâ–Œ        | 106955/665298 [00:22<02:03, 4522.79 examples/s]Map (num_proc=32):  16%|â–ˆâ–‹        | 108564/665298 [00:22<01:41, 5500.55 examples/s]Map (num_proc=32):  17%|â–ˆâ–‹        | 109957/665298 [00:22<01:26, 6408.92 examples/s]Map (num_proc=32):  17%|â–ˆâ–‹        | 111433/665298 [00:22<01:13, 7522.45 examples/s]Map (num_proc=32):  17%|â–ˆâ–‹        | 112955/665298 [00:23<01:03, 8658.83 examples/s]Map (num_proc=32):  17%|â–ˆâ–‹        | 114415/665298 [00:23<00:56, 9753.42 examples/s]Map (num_proc=32):  17%|â–ˆâ–‹        | 115955/665298 [00:23<00:55, 9930.39 examples/s]Map (num_proc=32):  18%|â–ˆâ–Š        | 118243/665298 [00:23<00:47, 11406.51 examples/s]Map (num_proc=32):  18%|â–ˆâ–Š        | 119867/665298 [00:23<01:04, 8434.19 examples/s] Map (num_proc=32):  18%|â–ˆâ–Š        | 121237/665298 [00:24<01:55, 4724.96 examples/s]Map (num_proc=32):  18%|â–ˆâ–Š        | 122872/665298 [00:25<02:38, 3417.23 examples/s]Map (num_proc=32):  19%|â–ˆâ–Š        | 123955/665298 [00:25<02:15, 4007.38 examples/s]Map (num_proc=32):  19%|â–ˆâ–‰        | 125440/665298 [00:25<02:21, 3802.68 examples/s]Map (num_proc=32):  19%|â–ˆâ–‰        | 126746/665298 [00:25<01:54, 4715.97 examples/s]Map (num_proc=32):  19%|â–ˆâ–‰        | 128317/665298 [00:25<01:28, 6083.46 examples/s]Map (num_proc=32):  20%|â–ˆâ–‰        | 129746/665298 [00:26<01:13, 7298.82 examples/s]Map (num_proc=32):  20%|â–ˆâ–‰        | 131246/665298 [00:26<01:01, 8659.07 examples/s]Map (num_proc=32):  20%|â–ˆâ–‰        | 132746/665298 [00:26<00:54, 9815.00 examples/s]Map (num_proc=32):  20%|â–ˆâ–ˆ        | 134326/665298 [00:26<00:47, 11072.69 examples/s]Map (num_proc=32):  20%|â–ˆâ–ˆ        | 135746/665298 [00:26<00:50, 10556.88 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆ        | 137983/665298 [00:26<00:42, 12341.72 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆ        | 139657/665298 [00:27<01:40, 5232.46 examples/s] Map (num_proc=32):  21%|â–ˆâ–ˆ        | 140992/665298 [00:27<01:26, 6082.29 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆâ–       | 142658/665298 [00:28<01:54, 4572.83 examples/s]Map (num_proc=32):  22%|â–ˆâ–ˆâ–       | 143746/665298 [00:28<02:06, 4137.70 examples/s]Map (num_proc=32):  22%|â–ˆâ–ˆâ–       | 146368/665298 [00:29<02:18, 3749.73 examples/s]Map (num_proc=32):  22%|â–ˆâ–ˆâ–       | 148457/665298 [00:29<01:44, 4954.11 examples/s]Map (num_proc=32):  23%|â–ˆâ–ˆâ–       | 149756/665298 [00:29<01:29, 5764.39 examples/s]Map (num_proc=32):  23%|â–ˆâ–ˆâ–       | 151448/665298 [00:29<01:15, 6828.00 examples/s]Map (num_proc=32):  23%|â–ˆâ–ˆâ–       | 153537/665298 [00:29<01:00, 8398.19 examples/s]Map (num_proc=32):  23%|â–ˆâ–ˆâ–       | 155080/665298 [00:30<01:31, 5574.50 examples/s]Map (num_proc=32):  24%|â–ˆâ–ˆâ–       | 156521/665298 [00:30<01:35, 5308.95 examples/s]Map (num_proc=32):  24%|â–ˆâ–ˆâ–       | 157796/665298 [00:31<02:52, 2935.52 examples/s]Map (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 166749/665298 [00:32<01:11, 7003.45 examples/s]Map (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 167837/665298 [00:32<01:10, 7040.03 examples/s]Map (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 168946/665298 [00:32<01:10, 7085.95 examples/s]Map (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 169757/665298 [00:32<01:10, 7056.18 examples/s]Map (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 170902/665298 [00:32<01:12, 6856.81 examples/s]Map (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 171765/665298 [00:32<01:10, 6964.40 examples/s]Map (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 172879/665298 [00:33<02:30, 3270.31 examples/s]Map (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 173776/665298 [00:33<02:09, 3806.15 examples/s]Map (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 174482/665298 [00:34<03:40, 2227.25 examples/s]Map (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 186328/665298 [00:34<00:42, 11354.43 examples/s]Map (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 189057/665298 [00:36<01:16, 6191.24 examples/s] Map (num_proc=32):  29%|â–ˆâ–ˆâ–Š       | 191065/665298 [00:36<01:42, 4626.11 examples/s]Map (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 192571/665298 [00:37<01:32, 5129.52 examples/s]Map (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 194119/665298 [00:38<02:08, 3654.12 examples/s]Map (num_proc=32):  31%|â–ˆâ–ˆâ–ˆ       | 204197/665298 [00:38<00:47, 9627.77 examples/s]Map (num_proc=32):  31%|â–ˆâ–ˆâ–ˆâ–      | 208006/665298 [00:38<00:48, 9493.64 examples/s]Map (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 210910/665298 [00:38<00:46, 9753.35 examples/s]Map (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 213347/665298 [00:40<01:36, 4664.65 examples/s]Map (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 215361/665298 [00:41<01:56, 3862.68 examples/s]Map (num_proc=32):  34%|â–ˆâ–ˆâ–ˆâ–      | 229130/665298 [00:41<00:45, 9522.37 examples/s]Map (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–      | 231591/665298 [00:41<00:47, 9064.08 examples/s]Map (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 233611/665298 [00:42<00:48, 8822.40 examples/s]Map (num_proc=32):  35%|â–ˆâ–ˆâ–ˆâ–Œ      | 235169/665298 [00:42<00:52, 8164.48 examples/s]Map (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 236701/665298 [00:42<00:52, 8122.61 examples/s]Map (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 237751/665298 [00:43<01:30, 4732.48 examples/s]Map (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 238948/665298 [00:43<01:22, 5179.58 examples/s]Map (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–Œ      | 240144/665298 [00:44<01:47, 3941.35 examples/s]Map (num_proc=32):  36%|â–ˆâ–ˆâ–ˆâ–‹      | 241431/665298 [00:44<01:50, 3852.63 examples/s]Map (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 250336/665298 [00:46<01:22, 5009.68 examples/s]Map (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 250967/665298 [00:46<01:44, 3953.20 examples/s]Map (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 254119/665298 [00:46<01:14, 5492.27 examples/s]Map (num_proc=32):  38%|â–ˆâ–ˆâ–ˆâ–Š      | 255538/665298 [00:47<01:28, 4608.95 examples/s]Map (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–Š      | 257058/665298 [00:47<01:29, 4552.41 examples/s]Map (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 271128/665298 [00:48<00:31, 12413.88 examples/s]Map (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 273283/665298 [00:48<00:31, 12601.19 examples/s]Map (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 274911/665298 [00:48<00:30, 12980.31 examples/s]Map (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 277137/665298 [00:48<00:29, 13285.58 examples/s]Map (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 279283/665298 [00:48<00:28, 13456.75 examples/s]Map (num_proc=32):  42%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 280924/665298 [00:48<00:27, 13935.28 examples/s]Map (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 283147/665298 [00:49<00:28, 13503.76 examples/s]Map (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 285283/665298 [00:49<00:26, 14195.16 examples/s]Map (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 286902/665298 [00:49<01:01, 6160.12 examples/s] Map (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 289204/665298 [00:50<00:47, 7964.09 examples/s]Map (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 291074/665298 [00:50<01:23, 4496.90 examples/s]Map (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 292569/665298 [00:51<01:29, 4154.54 examples/s]Map (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 294074/665298 [00:51<01:13, 5064.42 examples/s]Map (num_proc=32):  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 295714/665298 [00:51<00:58, 6289.13 examples/s]Map (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 297148/665298 [00:51<00:49, 7365.05 examples/s]Map (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 298606/665298 [00:51<00:43, 8521.16 examples/s]Map (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 300074/665298 [00:51<00:37, 9623.04 examples/s]Map (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 301721/665298 [00:51<00:32, 11054.05 examples/s]Map (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 303167/665298 [00:52<00:31, 11496.08 examples/s]Map (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 304983/665298 [00:52<00:29, 12407.06 examples/s]Map (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 307074/665298 [00:52<01:06, 5418.06 examples/s] Map (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 310074/665298 [00:53<00:44, 7965.15 examples/s]Map (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 311665/665298 [00:53<01:18, 4529.68 examples/s]Map (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 313343/665298 [00:54<01:27, 4025.65 examples/s]Map (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 314865/665298 [00:54<01:10, 4943.26 examples/s]Map (num_proc=32):  47%|â–ˆâ–ˆâ–ˆâ–ˆâ–‹     | 315975/665298 [00:54<01:02, 5598.40 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 317277/665298 [00:54<00:59, 5810.33 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 318267/665298 [00:55<00:58, 5883.75 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 319256/665298 [00:55<00:58, 5918.67 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 320256/665298 [00:55<01:40, 3448.42 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 320865/665298 [00:56<01:52, 3051.78 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 321655/665298 [00:56<02:33, 2237.64 examples/s]Map (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 322244/665298 [00:57<02:42, 2112.32 examples/s]Map (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 331865/665298 [00:57<00:29, 11257.82 examples/s]Map (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 334971/665298 [00:58<00:45, 7330.18 examples/s] Map (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 337477/665298 [00:58<00:42, 7687.33 examples/s]Map (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 339468/665298 [00:59<01:06, 4921.09 examples/s]Map (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 341466/665298 [00:59<00:54, 5897.88 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 342989/665298 [01:00<01:18, 4094.94 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 344471/665298 [01:00<01:05, 4875.93 examples/s]Map (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 351096/665298 [01:00<00:29, 10476.76 examples/s]Map (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 353844/665298 [01:00<00:32, 9528.53 examples/s] Map (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 355924/665298 [01:01<00:35, 8682.68 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 357945/665298 [01:02<01:21, 3760.82 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 359205/665298 [01:02<01:12, 4222.72 examples/s]Map (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 360476/665298 [01:03<01:37, 3138.81 examples/s]Map (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 372376/665298 [01:03<00:27, 10702.22 examples/s]Map (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 376796/665298 [01:04<00:30, 9384.50 examples/s] Map (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 380135/665298 [01:06<01:12, 3933.54 examples/s]Map (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 388840/665298 [01:06<00:39, 6959.15 examples/s]Map (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 392416/665298 [01:07<00:38, 7141.08 examples/s]Map (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 395420/665298 [01:07<00:35, 7551.01 examples/s]Map (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 397853/665298 [01:07<00:34, 7840.64 examples/s]Map (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 399839/665298 [01:08<00:33, 7868.77 examples/s]Map (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 401633/665298 [01:09<00:52, 5069.30 examples/s]Map (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 402941/665298 [01:10<01:27, 2998.51 examples/s]Map (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 411028/665298 [01:10<00:36, 6941.51 examples/s]Map (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 413940/665298 [01:11<00:39, 6442.16 examples/s]Map (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 416207/665298 [01:11<00:40, 6161.28 examples/s]Map (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 417818/665298 [01:12<00:46, 5342.72 examples/s]Map (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 419126/665298 [01:13<01:34, 2616.82 examples/s]Map (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 427743/665298 [01:13<00:36, 6424.96 examples/s]Map (num_proc=32):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 431026/665298 [01:14<00:33, 6902.16 examples/s]Map (num_proc=32):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 433700/665298 [01:14<00:30, 7499.75 examples/s]Map (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 435778/665298 [01:14<00:29, 7711.07 examples/s]Map (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 437753/665298 [01:14<00:28, 8084.13 examples/s]Map (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 439392/665298 [01:15<00:46, 4882.31 examples/s]Map (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 440751/665298 [01:16<00:43, 5209.11 examples/s]Map (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 441765/665298 [01:16<00:47, 4717.20 examples/s]Map (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 442716/665298 [01:17<01:15, 2956.94 examples/s]Map (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 447608/665298 [01:17<00:43, 5059.06 examples/s]Map (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 453930/665298 [01:17<00:22, 9449.75 examples/s]Map (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 456116/665298 [01:18<00:24, 8377.23 examples/s]Map (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 457774/665298 [01:18<00:28, 7349.94 examples/s]Map (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 459171/665298 [01:19<00:51, 4007.90 examples/s]Map (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 460115/665298 [01:19<00:54, 3758.31 examples/s]Map (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 460934/665298 [01:20<01:20, 2541.53 examples/s]Map (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 464398/665298 [01:21<00:54, 3696.92 examples/s]Map (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 468574/665298 [01:21<00:31, 6199.72 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 470161/665298 [01:21<00:33, 5862.43 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 471321/665298 [01:21<00:34, 5669.30 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 472462/665298 [01:22<00:40, 4727.68 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 473385/665298 [01:22<00:53, 3589.05 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 474030/665298 [01:23<00:49, 3834.54 examples/s]Map (num_proc=32):  71%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 475233/665298 [01:23<01:02, 3062.46 examples/s]Map (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 475902/665298 [01:23<01:08, 2756.69 examples/s]Map (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 486020/665298 [01:24<00:14, 12382.85 examples/s]Map (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 488938/665298 [01:24<00:16, 11014.35 examples/s]Map (num_proc=32):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 491260/665298 [01:24<00:16, 10515.64 examples/s]Map (num_proc=32):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 493317/665298 [01:27<01:03, 2701.79 examples/s] Map (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 513732/665298 [01:27<00:15, 9486.04 examples/s]Map (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 516978/665298 [01:27<00:14, 10566.23 examples/s]Map (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 519748/665298 [01:28<00:14, 10338.32 examples/s]Map (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 522119/665298 [01:28<00:13, 10300.85 examples/s]Map (num_proc=32):  79%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 524154/665298 [01:30<00:36, 3848.03 examples/s] Map (num_proc=32):  81%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 540494/665298 [01:30<00:11, 10475.20 examples/s]Map (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 546195/665298 [01:32<00:19, 6131.19 examples/s] Map (num_proc=32):  83%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 550259/665298 [01:33<00:20, 5711.34 examples/s]Map (num_proc=32):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 557660/665298 [01:34<00:13, 8224.02 examples/s]Map (num_proc=32):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 561793/665298 [01:37<00:25, 3982.70 examples/s]Map (num_proc=32):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 579318/665298 [01:37<00:09, 8870.86 examples/s]Map (num_proc=32):  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 586163/665298 [01:39<00:13, 5883.85 examples/s]Map (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰ | 591104/665298 [01:40<00:11, 6241.96 examples/s]Map (num_proc=32):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 599082/665298 [01:40<00:07, 8660.28 examples/s]Map (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 603772/665298 [01:42<00:12, 4979.50 examples/s]Map (num_proc=32):  91%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 606852/665298 [01:43<00:11, 5013.21 examples/s]Map (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 624562/665298 [01:44<00:05, 8139.90 examples/s]Map (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 626718/665298 [01:44<00:04, 8260.89 examples/s]Map (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 628456/665298 [01:44<00:04, 8491.08 examples/s]Map (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 630666/665298 [01:45<00:03, 8787.63 examples/s]Map (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 632657/665298 [01:45<00:03, 8953.47 examples/s]Map (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 634110/665298 [01:45<00:03, 9049.70 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 635430/665298 [01:45<00:03, 9374.13 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 636950/665298 [01:45<00:02, 9517.85 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 638718/665298 [01:45<00:02, 9780.72 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 639847/665298 [01:45<00:02, 10031.59 examples/s]Map (num_proc=32):  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 641534/665298 [01:46<00:02, 10349.08 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 643086/665298 [01:46<00:02, 10347.63 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 644428/665298 [01:46<00:01, 10658.44 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 646166/665298 [01:47<00:06, 2952.56 examples/s] Map (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 653908/665298 [01:48<00:01, 8045.97 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 656364/665298 [01:48<00:01, 8664.69 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 658508/665298 [01:48<00:00, 8932.06 examples/s]Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 660348/665298 [01:48<00:00, 9489.65 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 662482/665298 [01:48<00:00, 9772.26 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 664222/665298 [01:48<00:00, 9985.40 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 665298/665298 [01:50<00:00, 6009.02 examples/s]
Map (num_proc=32):   0%|          | 0/665298 [00:00<?, ? examples/s]Map (num_proc=32):   0%|          | 500/665298 [00:00<16:48, 659.15 examples/s]Map (num_proc=32):   0%|          | 1729/665298 [00:00<04:40, 2367.27 examples/s]Map (num_proc=32):   0%|          | 3000/665298 [00:01<02:56, 3752.38 examples/s]Map (num_proc=32):   1%|          | 4000/665298 [00:01<02:18, 4771.27 examples/s]Map (num_proc=32):   1%|          | 5040/665298 [00:01<01:53, 5811.27 examples/s]Map (num_proc=32):   1%|          | 6932/665298 [00:01<01:17, 8501.92 examples/s]Map (num_proc=32):   1%|â–         | 8751/665298 [00:01<01:02, 10557.11 examples/s]Map (num_proc=32):   2%|â–         | 11261/665298 [00:01<00:50, 12944.75 examples/s]Map (num_proc=32):   2%|â–         | 14929/665298 [00:01<00:36, 17980.12 examples/s]Map (num_proc=32):   3%|â–         | 19041/665298 [00:01<00:27, 23367.99 examples/s]Map (num_proc=32):   3%|â–         | 22008/665298 [00:01<00:26, 24660.89 examples/s]Map (num_proc=32):   4%|â–         | 29032/665298 [00:02<00:17, 35413.54 examples/s]Map (num_proc=32):   5%|â–         | 33220/665298 [00:02<00:17, 35660.62 examples/s]Map (num_proc=32):   6%|â–Œ         | 40493/665298 [00:02<00:13, 45197.19 examples/s]Map (num_proc=32):   7%|â–‹         | 46972/665298 [00:02<00:12, 49725.36 examples/s]Map (num_proc=32):   8%|â–Š         | 52668/665298 [00:02<00:11, 51635.19 examples/s]Map (num_proc=32):   9%|â–‰         | 62740/665298 [00:02<00:09, 64941.38 examples/s]Map (num_proc=32):  10%|â–ˆ         | 69573/665298 [00:02<00:09, 61221.75 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 82945/665298 [00:02<00:07, 80547.57 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 91387/665298 [00:02<00:07, 74271.07 examples/s]Map (num_proc=32):  16%|â–ˆâ–Œ        | 106647/665298 [00:03<00:06, 93068.06 examples/s]Map (num_proc=32):  18%|â–ˆâ–Š        | 116446/665298 [00:03<00:06, 85207.01 examples/s]Map (num_proc=32):  19%|â–ˆâ–‰        | 127882/665298 [00:03<00:05, 90812.61 examples/s]Map (num_proc=32):  21%|â–ˆâ–ˆâ–       | 141826/665298 [00:03<00:05, 100860.82 examples/s]Map (num_proc=32):  23%|â–ˆâ–ˆâ–       | 152171/665298 [00:03<00:05, 95586.16 examples/s] Map (num_proc=32):  24%|â–ˆâ–ˆâ–       | 162054/665298 [00:03<00:07, 68836.71 examples/s]Map (num_proc=32):  26%|â–ˆâ–ˆâ–Œ       | 171852/665298 [00:04<00:19, 25187.24 examples/s]Map (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 178169/665298 [00:05<00:19, 25421.86 examples/s]Map (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 183389/665298 [00:05<00:18, 26297.22 examples/s]Map (num_proc=32):  30%|â–ˆâ–ˆâ–ˆ       | 202431/665298 [00:06<00:19, 23928.74 examples/s]Map (num_proc=32):  31%|â–ˆâ–ˆâ–ˆ       | 207868/665298 [00:07<00:32, 14089.54 examples/s]Map (num_proc=32):  32%|â–ˆâ–ˆâ–ˆâ–      | 210651/665298 [00:07<00:40, 11113.66 examples/s]Map (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–      | 218425/665298 [00:08<00:38, 11720.02 examples/s]Map (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–      | 220895/665298 [00:09<00:54, 8113.85 examples/s] Map (num_proc=32):  34%|â–ˆâ–ˆâ–ˆâ–      | 223070/665298 [00:10<01:01, 7154.68 examples/s]Map (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 339578/665298 [00:10<00:05, 63742.01 examples/s]Map (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 349185/665298 [00:10<00:05, 61123.20 examples/s]Map (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 363892/665298 [00:10<00:04, 60729.90 examples/s]Map (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 371370/665298 [00:11<00:07, 39530.40 examples/s]Map (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 376792/665298 [00:11<00:07, 38992.88 examples/s]Map (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 383852/665298 [00:12<00:08, 32601.72 examples/s]Map (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 389920/665298 [00:12<00:10, 27360.95 examples/s]Map (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 394786/665298 [00:12<00:11, 23110.18 examples/s]Map (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 397565/665298 [00:13<00:18, 14650.07 examples/s]Map (num_proc=32):  73%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 482411/665298 [00:14<00:03, 52195.56 examples/s]Map (num_proc=32):  75%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 497339/665298 [00:14<00:03, 46715.87 examples/s]Map (num_proc=32):  77%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹  | 510661/665298 [00:15<00:03, 43664.77 examples/s]Map (num_proc=32):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 616869/665298 [00:15<00:00, 110900.91 examples/s]Map (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 633488/665298 [00:15<00:00, 100477.13 examples/s]Map (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 646571/665298 [00:18<00:00, 31831.91 examples/s] Map (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 655973/665298 [00:18<00:00, 28203.19 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 662944/665298 [00:20<00:00, 18816.34 examples/s]Map (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 665298/665298 [00:22<00:00, 29861.24 examples/s]
Filter (num_proc=32):   0%|          | 0/665298 [00:00<?, ? examples/s]Filter (num_proc=32):   0%|          | 1000/665298 [00:00<10:34, 1047.12 examples/s]Filter (num_proc=32):   0%|          | 3000/665298 [00:01<03:28, 3173.74 examples/s]Filter (num_proc=32):   1%|          | 5000/665298 [00:01<02:12, 4994.52 examples/s]Filter (num_proc=32):   1%|          | 8000/665298 [00:01<01:22, 7961.26 examples/s]Filter (num_proc=32):   2%|â–         | 11000/665298 [00:01<00:56, 11507.59 examples/s]Filter (num_proc=32):   2%|â–         | 14000/665298 [00:01<00:45, 14461.37 examples/s]Filter (num_proc=32):   3%|â–         | 19000/665298 [00:01<00:30, 21041.21 examples/s]Filter (num_proc=32):   4%|â–         | 27000/665298 [00:01<00:18, 33947.09 examples/s]Filter (num_proc=32):   5%|â–         | 32000/665298 [00:02<00:17, 36602.97 examples/s]Filter (num_proc=32):   7%|â–‹         | 44000/665298 [00:02<00:10, 56733.38 examples/s]Filter (num_proc=32):   8%|â–Š         | 53000/665298 [00:02<00:09, 64305.75 examples/s]Filter (num_proc=32):   9%|â–‰         | 62000/665298 [00:02<00:13, 44627.97 examples/s]Filter (num_proc=32):  15%|â–ˆâ–        | 98582/665298 [00:02<00:05, 101189.81 examples/s]Filter (num_proc=32):  17%|â–ˆâ–‹        | 111373/665298 [00:02<00:06, 90540.75 examples/s]Filter (num_proc=32):  19%|â–ˆâ–Š        | 124164/665298 [00:03<00:05, 90461.60 examples/s]Filter (num_proc=32):  20%|â–ˆâ–ˆ        | 136164/665298 [00:03<00:05, 92125.07 examples/s]Filter (num_proc=32):  22%|â–ˆâ–ˆâ–       | 146746/665298 [00:03<00:05, 91897.86 examples/s]Filter (num_proc=32):  24%|â–ˆâ–ˆâ–       | 159537/665298 [00:03<00:05, 84296.05 examples/s]Filter (num_proc=32):  25%|â–ˆâ–ˆâ–Œ       | 169537/665298 [00:03<00:08, 59531.15 examples/s]Filter (num_proc=32):  27%|â–ˆâ–ˆâ–‹       | 177537/665298 [00:04<00:09, 51943.64 examples/s]Filter (num_proc=32):  28%|â–ˆâ–ˆâ–Š       | 184537/665298 [00:04<00:08, 55014.51 examples/s]Filter (num_proc=32):  29%|â–ˆâ–ˆâ–‰       | 192537/665298 [00:04<00:07, 59689.42 examples/s]Filter (num_proc=32):  31%|â–ˆâ–ˆâ–ˆ       | 206328/665298 [00:04<00:06, 72624.42 examples/s]Filter (num_proc=32):  33%|â–ˆâ–ˆâ–ˆâ–      | 219328/665298 [00:04<00:05, 78801.07 examples/s]Filter (num_proc=32):  37%|â–ˆâ–ˆâ–ˆâ–‹      | 246119/665298 [00:04<00:03, 119837.63 examples/s]Filter (num_proc=32):  39%|â–ˆâ–ˆâ–ˆâ–‰      | 260119/665298 [00:04<00:03, 115354.36 examples/s]Filter (num_proc=32):  41%|â–ˆâ–ˆâ–ˆâ–ˆ      | 272910/665298 [00:04<00:03, 114028.01 examples/s]Filter (num_proc=32):  43%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 285701/665298 [00:05<00:04, 94132.89 examples/s] Filter (num_proc=32):  45%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 296701/665298 [00:05<00:03, 92942.54 examples/s]Filter (num_proc=32):  46%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ     | 306701/665298 [00:05<00:04, 80340.89 examples/s]Filter (num_proc=32):  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 316492/665298 [00:05<00:04, 80189.08 examples/s]Filter (num_proc=32):  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–‰     | 325074/665298 [00:05<00:05, 60411.38 examples/s]Filter (num_proc=32):  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 333074/665298 [00:05<00:05, 61568.98 examples/s]Filter (num_proc=32):  51%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 340865/665298 [00:06<00:06, 51613.15 examples/s]Filter (num_proc=32):  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 346865/665298 [00:06<00:06, 46775.42 examples/s]Filter (num_proc=32):  53%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 352865/665298 [00:06<00:07, 41625.55 examples/s]Filter (num_proc=32):  54%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 357865/665298 [00:06<00:07, 42694.51 examples/s]Filter (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 362865/665298 [00:06<00:07, 41557.43 examples/s]Filter (num_proc=32):  55%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 367865/665298 [00:06<00:07, 40862.60 examples/s]Filter (num_proc=32):  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 372865/665298 [00:06<00:07, 39198.60 examples/s]Filter (num_proc=32):  57%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹    | 377865/665298 [00:07<00:07, 40908.57 examples/s]Filter (num_proc=32):  58%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 382656/665298 [00:07<00:08, 31789.25 examples/s]Filter (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š    | 390656/665298 [00:07<00:06, 41681.76 examples/s]Filter (num_proc=32):  59%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰    | 395656/665298 [00:07<00:06, 41471.75 examples/s]Filter (num_proc=32):  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 401447/665298 [00:07<00:06, 39367.45 examples/s]Filter (num_proc=32):  61%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 406447/665298 [00:07<00:06, 38979.63 examples/s]Filter (num_proc=32):  62%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 411447/665298 [00:07<00:06, 40687.21 examples/s]Filter (num_proc=32):  63%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 416447/665298 [00:08<00:06, 40750.66 examples/s]Filter (num_proc=32):  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 423238/665298 [00:08<00:05, 45212.72 examples/s]Filter (num_proc=32):  65%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 430238/665298 [00:08<00:04, 47892.76 examples/s]Filter (num_proc=32):  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 436238/665298 [00:08<00:04, 48702.00 examples/s]Filter (num_proc=32):  67%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹   | 443238/665298 [00:08<00:04, 52162.36 examples/s]Filter (num_proc=32):  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 451238/665298 [00:08<00:03, 58622.51 examples/s]Filter (num_proc=32):  69%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰   | 458238/665298 [00:08<00:03, 61388.67 examples/s]Filter (num_proc=32):  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 468238/665298 [00:08<00:02, 71682.09 examples/s]Filter (num_proc=32):  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 477238/665298 [00:08<00:02, 75196.14 examples/s]Filter (num_proc=32):  74%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 495238/665298 [00:09<00:01, 103139.12 examples/s]Filter (num_proc=32):  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 506238/665298 [00:09<00:01, 100703.99 examples/s]Filter (num_proc=32):  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 518238/665298 [00:09<00:01, 98881.39 examples/s] Filter (num_proc=32):  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰  | 529028/665298 [00:09<00:01, 81208.18 examples/s]Filter (num_proc=32):  82%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 546028/665298 [00:09<00:01, 101420.11 examples/s]Filter (num_proc=32):  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 557818/665298 [00:09<00:01, 99302.41 examples/s] Filter (num_proc=32):  85%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 568608/665298 [00:09<00:00, 97450.78 examples/s]Filter (num_proc=32):  87%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹ | 579398/665298 [00:09<00:00, 92352.78 examples/s]Filter (num_proc=32):  89%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 589398/665298 [00:10<00:00, 93236.00 examples/s]Filter (num_proc=32):  90%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ | 599188/665298 [00:10<00:00, 90807.50 examples/s]Filter (num_proc=32):  92%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 608978/665298 [00:10<00:00, 85911.54 examples/s]Filter (num_proc=32):  93%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 617768/665298 [00:10<00:00, 84286.11 examples/s]Filter (num_proc=32):  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 626558/665298 [00:10<00:00, 81471.04 examples/s]Filter (num_proc=32):  95%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 635138/665298 [00:10<00:00, 69845.60 examples/s]Filter (num_proc=32):  97%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‹| 643138/665298 [00:10<00:00, 50857.58 examples/s]Filter (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 649928/665298 [00:11<00:00, 26613.00 examples/s]Filter (num_proc=32):  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 654718/665298 [00:11<00:00, 22637.29 examples/s]Filter (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 658718/665298 [00:12<00:00, 16789.97 examples/s]Filter (num_proc=32):  99%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 661508/665298 [00:12<00:00, 13528.42 examples/s]Filter (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–‰| 664508/665298 [00:13<00:00, 8363.52 examples/s] Filter (num_proc=32): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 665298/665298 [00:14<00:00, 47007.86 examples/s]
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Map (num_proc=32):   0%|          | 0/665298 [00:00<?, ? examples/s]Map (num_proc=32):   0%|          | 42/665298 [00:13<60:43:16,  3.04 examples/s]Map (num_proc=32):   0%|          | 131/665298 [00:13<15:17:42, 12.08 examples/s]Map (num_proc=32):   0%|          | 200/665298 [00:14<8:28:37, 21.79 examples/s] Map (num_proc=32):   0%|          | 299/665298 [00:14<4:31:12, 40.87 examples/s]Map (num_proc=32):   0%|          | 365/665298 [00:14<3:11:28, 57.88 examples/s]Map (num_proc=32):   0%|          | 459/665298 [00:14<2:03:27, 89.76 examples/s]Map (num_proc=32):   0%|          | 527/665298 [00:14<1:32:55, 119.24 examples/s]Map (num_proc=32):   0%|          | 628/665298 [00:14<1:04:03, 172.94 examples/s]Map (num_proc=32):   0%|          | 721/665298 [00:14<49:01, 225.90 examples/s]  Map (num_proc=32):   0%|          | 818/665298 [00:15<38:24, 288.37 examples/s]Map (num_proc=32):   0%|          | 885/665298 [00:15<33:03, 334.99 examples/s]Map (num_proc=32):   0%|          | 984/665298 [00:15<27:34, 401.56 examples/s]Map (num_proc=32):   0%|          | 1064/665298 [00:15<28:44, 385.07 examples/s]Map (num_proc=32):   0%|          | 1129/665298 [00:15<25:55, 427.06 examples/s]Map (num_proc=32):   0%|          | 1228/665298 [00:15<22:48, 485.09 examples/s]Map (num_proc=32):   0%|          | 1297/665298 [00:15<21:17, 519.90 examples/s]Map (num_proc=32):   0%|          | 1392/665298 [00:16<19:48, 558.82 examples/s]Map (num_proc=32):   0%|          | 1460/665298 [00:16<19:09, 577.27 examples/s]Map (num_proc=32):   0%|          | 1529/665298 [00:16<18:26, 599.63 examples/s]Map (num_proc=32):   0%|          | 1624/665298 [00:16<18:12, 607.72 examples/s]Map (num_proc=32):   0%|          | 1692/665298 [00:16<17:45, 623.09 examples/s]Map (num_proc=32):   0%|          | 1760/665298 [00:16<28:46, 384.43 examples/s]Map (num_proc=32):   0%|          | 2000/665298 [00:17<17:24, 635.29 examples/s]Map (num_proc=32):   0%|          | 2093/665298 [00:17<17:48, 620.81 examples/s]Map (num_proc=32):   0%|          | 2186/665298 [00:17<18:02, 612.31 examples/s]Map (num_proc=32):   0%|          | 2285/665298 [00:17<17:46, 621.96 examples/s]Map (num_proc=32):   0%|          | 2356/665298 [00:17<17:21, 636.62 examples/s]Map (num_proc=32):   0%|          | 2458/665298 [00:17<18:20, 602.07 examples/s]Map (num_proc=32):   0%|          | 2562/665298 [00:17<16:05, 686.36 examples/s]Map (num_proc=32):   0%|          | 2649/665298 [00:18<16:59, 649.88 examples/s]Map (num_proc=32):   0%|          | 2746/665298 [00:18<17:04, 646.80 examples/s]Map (num_proc=32):   0%|          | 2815/665298 [00:18<16:54, 653.26 examples/s]Map (num_proc=32):   0%|          | 2912/665298 [00:18<17:04, 646.61 examples/s]Map (num_proc=32):   0%|          | 3000/665298 [00:18<20:42, 533.16 examples/s]Map (num_proc=32):   0%|          | 3065/665298 [00:18<19:51, 555.93 examples/s]Map (num_proc=32):   0%|          | 3125/665298 [00:18<19:33, 564.03 examples/s]Map (num_proc=32):   0%|          | 3216/665298 [00:19<19:13, 574.01 examples/s]Map (num_proc=32):   0%|          | 3280/665298 [00:19<18:46, 587.64 examples/s]Map (num_proc=32):   1%|          | 3345/665298 [00:19<18:19, 602.21 examples/s]Map (num_proc=32):   1%|          | 3409/665298 [00:19<18:03, 610.63 examples/s]Map (num_proc=32):   1%|          | 3475/665298 [00:19<17:45, 621.04 examples/s]Map (num_proc=32):   1%|          | 3542/665298 [00:19<17:29, 630.81 examples/s]Map (num_proc=32):   1%|          | 3609/665298 [00:19<17:13, 640.16 examples/s]Map (num_proc=32):   1%|          | 3675/665298 [00:19<17:12, 640.82 examples/s]Map (num_proc=32):   1%|          | 3742/665298 [00:19<17:07, 643.93 examples/s]Map (num_proc=32):   1%|          | 3811/665298 [00:19<16:49, 655.28 examples/s]Map (num_proc=32):   1%|          | 3880/665298 [00:20<16:37, 663.24 examples/s]Map (num_proc=32):   1%|          | 3969/665298 [00:20<17:25, 632.58 examples/s]Map (num_proc=32):   1%|          | 4062/665298 [00:20<21:49, 505.13 examples/s]Map (num_proc=32):   1%|          | 4128/665298 [00:20<20:30, 537.20 examples/s]Map (num_proc=32):   1%|          | 4195/665298 [00:20<19:29, 565.10 examples/s]Map (num_proc=32):   1%|          | 4261/665298 [00:20<27:22, 402.44 examples/s]Map (num_proc=32):   1%|          | 4448/665298 [00:21<17:02, 646.28 examples/s]Map (num_proc=32):   1%|          | 4548/665298 [00:21<16:51, 653.27 examples/s]Map (num_proc=32):   1%|          | 4645/665298 [00:21<17:10, 641.02 examples/s]Map (num_proc=32):   1%|          | 4736/665298 [00:21<17:32, 627.43 examples/s]Map (num_proc=32):   1%|          | 4842/665298 [00:21<16:58, 648.49 examples/s]Map (num_proc=32):   1%|          | 4943/665298 [00:21<16:40, 660.25 examples/s]Map (num_proc=32):   1%|          | 5022/665298 [00:22<20:10, 545.61 examples/s]Map (num_proc=32):   1%|          | 5089/665298 [00:22<19:42, 558.12 examples/s]Map (num_proc=32):   1%|          | 5186/665298 [00:22<18:55, 581.32 examples/s]Map (num_proc=32):   1%|          | 5283/665298 [00:22<18:11, 604.42 examples/s]Map (num_proc=32):   1%|          | 5349/665298 [00:22<18:06, 607.64 examples/s]Map (num_proc=32):   1%|          | 5415/665298 [00:22<17:48, 617.76 examples/s]Map (num_proc=32):   1%|          | 5516/665298 [00:22<17:08, 641.26 examples/s]Map (num_proc=32):   1%|          | 5585/665298 [00:22<17:07, 641.76 examples/s]Map (num_proc=32):   1%|          | 5680/665298 [00:23<17:18, 635.47 examples/s]Map (num_proc=32):   1%|          | 5746/665298 [00:23<17:14, 637.35 examples/s]Map (num_proc=32):   1%|          | 5812/665298 [00:23<17:08, 641.03 examples/s]Map (num_proc=32):   1%|          | 5877/665298 [00:23<17:13, 638.13 examples/s]Map (num_proc=32):   1%|          | 5945/665298 [00:23<16:58, 647.53 examples/s]Map (num_proc=32):   1%|          | 6030/665298 [00:23<21:33, 509.49 examples/s]Map (num_proc=32):   1%|          | 6096/665298 [00:23<20:19, 540.64 examples/s]Map (num_proc=32):   1%|          | 6160/665298 [00:23<19:30, 563.14 examples/s]Map (num_proc=32):   1%|          | 6287/665298 [00:24<14:47, 742.71 examples/s]Map (num_proc=32):   1%|          | 6421/665298 [00:24<12:14, 896.84 examples/s]Map (num_proc=32):   1%|          | 6519/665298 [00:24<12:00, 914.58 examples/s]Map (num_proc=32):   1%|          | 6673/665298 [00:24<10:07, 1084.99 examples/s]Map (num_proc=32):   1%|          | 6802/665298 [00:24<17:44, 618.74 examples/s] Map (num_proc=32):   1%|          | 7123/665298 [00:24<10:15, 1069.71 examples/s]Map (num_proc=32):   1%|          | 7412/665298 [00:24<07:40, 1429.83 examples/s]Map (num_proc=32):   1%|          | 7603/665298 [00:25<07:52, 1391.73 examples/s]Map (num_proc=32):   1%|          | 7793/665298 [00:25<08:22, 1308.12 examples/s]Map (num_proc=32):   1%|          | 7971/665298 [00:25<08:53, 1232.57 examples/s]Map (num_proc=32):   1%|          | 8129/665298 [00:25<09:19, 1174.60 examples/s]Map (num_proc=32):   1%|          | 8281/665298 [00:25<09:45, 1121.22 examples/s]Map (num_proc=32):   1%|â–         | 8418/665298 [00:25<09:21, 1169.36 examples/s]Map (num_proc=32):   1%|â–         | 8575/665298 [00:25<09:07, 1200.35 examples/s]Map (num_proc=32):   1%|â–         | 8703/665298 [00:26<09:09, 1194.28 examples/s]Map (num_proc=32):   1%|â–         | 8838/665298 [00:26<08:53, 1229.84 examples/s]Map (num_proc=32):   1%|â–         | 8974/665298 [00:26<08:44, 1252.35 examples/s]Map (num_proc=32):   1%|â–         | 9138/665298 [00:26<08:26, 1296.40 examples/s]Map (num_proc=32):   1%|â–         | 9293/665298 [00:26<08:47, 1244.07 examples/s]Map (num_proc=32):   1%|â–         | 9427/665298 [00:26<08:41, 1257.52 examples/s]Map (num_proc=32):   1%|â–         | 9593/665298 [00:26<08:21, 1308.03 examples/s]Map (num_proc=32):   1%|â–         | 9726/665298 [00:26<08:28, 1289.99 examples/s]Map (num_proc=32):   1%|â–         | 9885/665298 [00:27<09:48, 1114.21 examples/s]Map (num_proc=32):   2%|â–         | 10051/665298 [00:27<09:12, 1185.21 examples/s]Map (num_proc=32):   2%|â–         | 10186/665298 [00:27<08:58, 1216.01 examples/s]Map (num_proc=32):   2%|â–         | 10322/665298 [00:27<09:57, 1095.74 examples/s]Map (num_proc=32):   2%|â–         | 10450/665298 [00:27<09:50, 1109.36 examples/s]Map (num_proc=32):   2%|â–         | 10609/665298 [00:27<09:45, 1119.07 examples/s]Map (num_proc=32):   2%|â–         | 10745/665298 [00:27<09:23, 1161.79 examples/s]Map (num_proc=32):   2%|â–         | 10877/665298 [00:27<09:06, 1196.77 examples/s]Map (num_proc=32):   2%|â–         | 11081/665298 [00:28<08:47, 1240.67 examples/s]Map (num_proc=32):   2%|â–         | 11217/665298 [00:28<08:38, 1261.76 examples/s]Map (num_proc=32):   2%|â–         | 11352/665298 [00:28<08:59, 1212.03 examples/s]Map (num_proc=32):   2%|â–         | 11482/665298 [00:28<09:05, 1197.56 examples/s]Map (num_proc=32):   2%|â–         | 11689/665298 [00:28<07:51, 1387.32 examples/s]Map (num_proc=32):   2%|â–         | 11856/665298 [00:28<08:34, 1270.30 examples/s]Map (num_proc=32):   2%|â–         | 12004/665298 [00:28<08:54, 1222.87 examples/s]Map (num_proc=32):   2%|â–         | 12133/665298 [00:28<08:50, 1232.24 examples/s]Map (num_proc=32):   2%|â–         | 12282/665298 [00:29<09:31, 1142.84 examples/s]Map (num_proc=32):   2%|â–         | 12412/665298 [00:29<09:15, 1175.43 examples/s]Map (num_proc=32):   2%|â–         | 12544/665298 [00:29<08:59, 1210.18 examples/s]Map (num_proc=32):   2%|â–         | 12671/665298 [00:29<08:55, 1219.81 examples/s]Map (num_proc=32):   2%|â–         | 12806/665298 [00:29<08:40, 1254.34 examples/s]Map (num_proc=32):   2%|â–         | 12946/665298 [00:29<08:26, 1288.03 examples/s]Map (num_proc=32):   2%|â–         | 13079/665298 [00:29<08:24, 1292.83 examples/s]Map (num_proc=32):   2%|â–         | 13214/665298 [00:29<08:21, 1301.55 examples/s]Map (num_proc=32):   2%|â–         | 13346/665298 [00:29<08:20, 1302.16 examples/s]Map (num_proc=32):   2%|â–         | 13512/665298 [00:29<08:36, 1262.37 examples/s]Map (num_proc=32):   2%|â–         | 13644/665298 [00:30<08:30, 1276.28 examples/s]Map (num_proc=32):   2%|â–         | 13792/665298 [00:30<09:20, 1162.68 examples/s]Map (num_proc=32):   2%|â–         | 13941/665298 [00:30<09:02, 1199.94 examples/s]Map (num_proc=32):   2%|â–         | 14075/665298 [00:30<08:48, 1231.83 examples/s]Map (num_proc=32):   2%|â–         | 14208/665298 [00:30<08:38, 1256.49 examples/s]Map (num_proc=32):   2%|â–         | 14351/665298 [00:30<09:59, 1085.38 examples/s]Map (num_proc=32):   2%|â–         | 14485/665298 [00:30<09:29, 1143.78 examples/s]Map (num_proc=32):   2%|â–         | 14620/665298 [00:30<09:05, 1192.43 examples/s]Map (num_proc=32):   2%|â–         | 14774/665298 [00:31<09:10, 1181.39 examples/s]Map (num_proc=32):   2%|â–         | 14908/665298 [00:31<08:54, 1215.89 examples/s]Map (num_proc=32):   2%|â–         | 15035/665298 [00:31<08:53, 1219.80 examples/s]Map (num_proc=32):   2%|â–         | 15169/665298 [00:31<08:42, 1243.45 examples/s]Map (num_proc=32):   2%|â–         | 15300/665298 [00:31<08:37, 1256.31 examples/s]Map (num_proc=32):   2%|â–         | 15435/665298 [00:31<08:28, 1278.79 examples/s]Map (num_proc=32):   2%|â–         | 15565/665298 [00:31<08:28, 1277.18 examples/s]Map (num_proc=32):   2%|â–         | 15706/665298 [00:31<08:38, 1253.69 examples/s]Map (num_proc=32):   2%|â–         | 15853/665298 [00:31<10:44, 1007.79 examples/s]Map (num_proc=32):   2%|â–         | 15980/665298 [00:32<13:41, 790.28 examples/s] Map (num_proc=32):   2%|â–         | 16185/665298 [00:32<10:39, 1014.96 examples/s]Map (num_proc=32):   2%|â–         | 16345/665298 [00:32<09:30, 1136.68 examples/s]Map (num_proc=32):   2%|â–         | 16509/665298 [00:32<09:05, 1188.76 examples/s]Map (num_proc=32):   3%|â–         | 16642/665298 [00:32<08:53, 1215.26 examples/s]Map (num_proc=32):   3%|â–         | 16806/665298 [00:32<08:42, 1240.22 examples/s]Map (num_proc=32):   3%|â–         | 16944/665298 [00:32<08:32, 1264.62 examples/s]Map (num_proc=32):   3%|â–         | 17111/665298 [00:33<08:28, 1275.91 examples/s]Map (num_proc=32):   3%|â–         | 17271/665298 [00:33<08:26, 1279.71 examples/s]Map (num_proc=32):   3%|â–         | 17404/665298 [00:33<08:24, 1283.25 examples/s]Map (num_proc=32):   3%|â–         | 17568/665298 [00:33<08:15, 1306.26 examples/s]Map (num_proc=32):   3%|â–         | 17702/665298 [00:33<08:19, 1296.26 examples/s]Map (num_proc=32):   3%|â–         | 17851/665298 [00:33<14:51, 726.39 examples/s] Map (num_proc=32):   3%|â–         | 18146/665298 [00:34<09:42, 1111.92 examples/s]Map (num_proc=32):   3%|â–         | 18391/665298 [00:34<08:06, 1329.65 examples/s]Map (num_proc=32):   3%|â–         | 18588/665298 [00:34<08:10, 1319.59 examples/s]Map (num_proc=32):   3%|â–         | 18750/665298 [00:34<08:08, 1324.12 examples/s]Map (num_proc=32):   3%|â–         | 18919/665298 [00:34<08:13, 1308.54 examples/s]Map (num_proc=32):   3%|â–         | 19091/665298 [00:34<07:59, 1348.43 examples/s]Map (num_proc=32):   3%|â–         | 19250/665298 [00:34<08:18, 1295.22 examples/s]Map (num_proc=32):   3%|â–         | 19415/665298 [00:34<08:14, 1305.31 examples/s]Map (num_proc=32):   3%|â–         | 19577/665298 [00:35<08:22, 1284.17 examples/s]Map (num_proc=32):   3%|â–         | 19743/665298 [00:35<08:12, 1311.64 examples/s]Map (num_proc=32):   3%|â–         | 19889/665298 [00:35<09:08, 1177.02 examples/s]Map (num_proc=32):   3%|â–         | 20021/665298 [00:35<11:13, 957.81 examples/s] Map (num_proc=32):   3%|â–         | 20341/665298 [00:35<07:34, 1420.25 examples/s]Map (num_proc=32):   3%|â–         | 20639/665298 [00:35<06:23, 1681.15 examples/s]Map (num_proc=32):   3%|â–         | 20872/665298 [00:35<05:57, 1802.32 examples/s]Map (num_proc=32):   3%|â–         | 21068/665298 [00:35<05:55, 1813.90 examples/s]Map (num_proc=32):   3%|â–         | 21268/665298 [00:36<05:47, 1852.60 examples/s]Map (num_proc=32):   3%|â–         | 21494/665298 [00:36<05:32, 1934.02 examples/s]Map (num_proc=32):   3%|â–         | 21718/665298 [00:36<05:50, 1835.08 examples/s]Map (num_proc=32):   3%|â–         | 21912/665298 [00:36<05:47, 1850.99 examples/s]Map (num_proc=32):   3%|â–         | 22116/665298 [00:36<05:40, 1891.67 examples/s]Map (num_proc=32):   3%|â–         | 22345/665298 [00:36<05:27, 1963.90 examples/s]Map (num_proc=32):   3%|â–         | 22548/665298 [00:36<05:24, 1981.41 examples/s]Map (num_proc=32):   3%|â–         | 22776/665298 [00:36<05:48, 1842.81 examples/s]Map (num_proc=32):   3%|â–         | 22983/665298 [00:37<06:35, 1624.39 examples/s]Map (num_proc=32):   3%|â–         | 23232/665298 [00:37<06:03, 1764.82 examples/s]Map (num_proc=32):   4%|â–         | 23438/665298 [00:37<06:26, 1662.09 examples/s]Map (num_proc=32):   4%|â–         | 23668/665298 [00:37<06:03, 1763.30 examples/s]Map (num_proc=32):   4%|â–         | 23863/665298 [00:37<05:55, 1805.27 examples/s]Map (num_proc=32):   4%|â–         | 24053/665298 [00:37<11:46, 907.00 examples/s] Map (num_proc=32):   4%|â–         | 24314/665298 [00:38<09:07, 1170.29 examples/s]Map (num_proc=32):   4%|â–         | 24645/665298 [00:38<06:53, 1551.02 examples/s]Map (num_proc=32):   4%|â–         | 24984/665298 [00:38<05:35, 1910.89 examples/s]Map (num_proc=32):   4%|â–         | 25288/665298 [00:38<04:55, 2164.35 examples/s]Map (num_proc=32):   4%|â–         | 25626/665298 [00:38<04:25, 2409.51 examples/s]Map (num_proc=32):   4%|â–         | 25922/665298 [00:38<04:25, 2410.45 examples/s]Map (num_proc=32):   4%|â–         | 26207/665298 [00:38<04:43, 2258.27 examples/s]Map (num_proc=32):   4%|â–         | 26476/665298 [00:38<05:12, 2042.20 examples/s]Map (num_proc=32):   4%|â–         | 26733/665298 [00:39<05:28, 1945.60 examples/s]Map (num_proc=32):   4%|â–         | 26969/665298 [00:39<05:18, 2001.48 examples/s]Map (num_proc=32):   4%|â–         | 27200/665298 [00:39<05:22, 1977.35 examples/s]Map (num_proc=32):   4%|â–         | 27435/665298 [00:39<05:22, 1978.78 examples/s]Map (num_proc=32):   4%|â–         | 27672/665298 [00:39<05:23, 1968.35 examples/s]Map (num_proc=32):   4%|â–         | 27889/665298 [00:39<05:27, 1945.32 examples/s]Map (num_proc=32):   4%|â–         | 28122/665298 [00:39<05:32, 1917.33 examples/s]Map (num_proc=32):   4%|â–         | 28358/665298 [00:39<05:27, 1946.82 examples/s]Map (num_proc=32):   4%|â–         | 28584/665298 [00:40<05:23, 1970.16 examples/s]Map (num_proc=32):   4%|â–         | 28785/665298 [00:40<05:42, 1856.73 examples/s]Map (num_proc=32):   4%|â–         | 28987/665298 [00:40<06:44, 1573.82 examples/s]Map (num_proc=32):   4%|â–         | 29180/665298 [00:40<07:14, 1463.59 examples/s]Map (num_proc=32):   4%|â–         | 29470/665298 [00:40<05:57, 1779.93 examples/s]Map (num_proc=32):   4%|â–         | 29666/665298 [00:40<05:49, 1819.45 examples/s]Map (num_proc=32):   4%|â–         | 29866/665298 [00:40<05:42, 1856.48 examples/s]Map (num_proc=32):   5%|â–         | 30062/665298 [00:40<05:38, 1877.71 examples/s]Map (num_proc=32):   5%|â–         | 30256/665298 [00:41<05:42, 1856.12 examples/s]Map (num_proc=32):   5%|â–         | 30449/665298 [00:41<05:40, 1866.22 examples/s]Map (num_proc=32):   5%|â–         | 30650/665298 [00:41<05:34, 1896.89 examples/s]Map (num_proc=32):   5%|â–         | 30844/665298 [00:41<05:33, 1900.94 examples/s]Map (num_proc=32):   5%|â–         | 31044/665298 [00:41<05:29, 1924.41 examples/s]Map (num_proc=32):   5%|â–         | 31247/665298 [00:41<05:25, 1947.39 examples/s]Map (num_proc=32):   5%|â–         | 31449/665298 [00:41<05:23, 1958.21 examples/s]Map (num_proc=32):   5%|â–         | 31650/665298 [00:41<05:22, 1964.68 examples/s]Map (num_proc=32):   5%|â–         | 31855/665298 [00:41<05:51, 1801.44 examples/s]Map (num_proc=32):   5%|â–         | 32056/665298 [00:42<06:45, 1561.42 examples/s]Map (num_proc=32):   5%|â–         | 32226/665298 [00:42<06:47, 1555.21 examples/s]Map (num_proc=32):   5%|â–         | 32420/665298 [00:42<07:14, 1455.43 examples/s]Map (num_proc=32):   5%|â–         | 32613/665298 [00:42<06:44, 1565.44 examples/s]Map (num_proc=32):   5%|â–         | 32804/665298 [00:42<06:23, 1650.70 examples/s]Map (num_proc=32):   5%|â–         | 33005/665298 [00:42<06:04, 1733.42 examples/s]Map (num_proc=32):   5%|â–         | 33211/665298 [00:42<05:49, 1809.97 examples/s]Map (num_proc=32):   5%|â–Œ         | 33413/665298 [00:42<05:38, 1867.34 examples/s]Map (num_proc=32):   5%|â–Œ         | 33609/665298 [00:42<05:36, 1879.83 examples/s]Map (num_proc=32):   5%|â–Œ         | 33814/665298 [00:43<05:29, 1915.65 examples/s]Map (num_proc=32):   5%|â–Œ         | 34008/665298 [00:43<05:31, 1903.27 examples/s]Map (num_proc=32):   5%|â–Œ         | 34200/665298 [00:43<05:31, 1903.77 examples/s]Map (num_proc=32):   5%|â–Œ         | 34399/665298 [00:43<05:28, 1919.93 examples/s]Map (num_proc=32):   5%|â–Œ         | 34597/665298 [00:43<05:42, 1841.80 examples/s]Map (num_proc=32):   5%|â–Œ         | 34791/665298 [00:43<08:10, 1285.03 examples/s]Map (num_proc=32):   5%|â–Œ         | 35085/665298 [00:43<06:28, 1620.56 examples/s]Map (num_proc=32):   5%|â–Œ         | 35319/665298 [00:43<05:55, 1772.09 examples/s]Map (num_proc=32):   5%|â–Œ         | 35552/665298 [00:44<05:35, 1877.44 examples/s]Map (num_proc=32):   5%|â–Œ         | 35782/665298 [00:44<05:27, 1921.21 examples/s]Map (num_proc=32):   5%|â–Œ         | 35987/665298 [00:44<05:24, 1938.49 examples/s]Map (num_proc=32):   5%|â–Œ         | 36216/665298 [00:44<05:28, 1915.86 examples/s]Map (num_proc=32):   5%|â–Œ         | 36447/665298 [00:44<05:29, 1909.93 examples/s]Map (num_proc=32):   6%|â–Œ         | 36672/665298 [00:44<05:25, 1930.52 examples/s]Map (num_proc=32):   6%|â–Œ         | 36898/665298 [00:44<05:32, 1888.86 examples/s]Map (num_proc=32):   6%|â–Œ         | 37094/665298 [00:44<05:29, 1905.82 examples/s]Map (num_proc=32):   6%|â–Œ         | 37330/665298 [00:44<05:28, 1912.73 examples/s]Map (num_proc=32):   6%|â–Œ         | 37570/665298 [00:45<05:18, 1970.38 examples/s]Map (num_proc=32):   6%|â–Œ         | 37772/665298 [00:45<05:50, 1788.48 examples/s]Map (num_proc=32):   6%|â–Œ         | 37984/665298 [00:45<06:01, 1737.34 examples/s]Map (num_proc=32):   6%|â–Œ         | 38165/665298 [00:45<05:58, 1747.22 examples/s]Map (num_proc=32):   6%|â–Œ         | 38354/665298 [00:45<06:14, 1675.29 examples/s]Map (num_proc=32):   6%|â–Œ         | 38529/665298 [00:45<06:12, 1682.96 examples/s]Map (num_proc=32):   6%|â–Œ         | 38722/665298 [00:45<06:00, 1740.15 examples/s]Map (num_proc=32):   6%|â–Œ         | 38921/665298 [00:45<05:48, 1796.18 examples/s]Map (num_proc=32):   6%|â–Œ         | 39110/665298 [00:45<05:45, 1813.62 examples/s]Map (num_proc=32):   6%|â–Œ         | 39307/665298 [00:46<05:51, 1782.58 examples/s]Map (num_proc=32):   6%|â–Œ         | 39566/665298 [00:46<05:21, 1947.65 examples/s]Map (num_proc=32):   6%|â–Œ         | 39768/665298 [00:46<05:26, 1915.25 examples/s]Map (num_proc=32):   6%|â–Œ         | 40004/665298 [00:46<05:24, 1924.91 examples/s]Map (num_proc=32):   6%|â–Œ         | 40199/665298 [00:46<08:12, 1269.68 examples/s]Map (num_proc=32):   6%|â–Œ         | 40467/665298 [00:46<06:47, 1534.87 examples/s]Map (num_proc=32):   6%|â–Œ         | 40791/665298 [00:46<05:26, 1910.27 examples/s]Map (num_proc=32):   6%|â–Œ         | 41151/665298 [00:47<04:45, 2184.17 examples/s]Map (num_proc=32):   6%|â–Œ         | 41502/665298 [00:47<04:10, 2493.28 examples/s]Map (num_proc=32):   6%|â–‹         | 41822/665298 [00:47<03:56, 2641.84 examples/s]Map (num_proc=32):   6%|â–‹         | 42114/665298 [00:47<03:51, 2688.52 examples/s]Map (num_proc=32):   6%|â–‹         | 42465/665298 [00:47<03:41, 2810.34 examples/s]Map (num_proc=32):   6%|â–‹         | 42792/665298 [00:47<03:33, 2910.19 examples/s]Map (num_proc=32):   6%|â–‹         | 43177/665298 [00:47<03:24, 3044.39 examples/s]Map (num_proc=32):   7%|â–‹         | 43645/665298 [00:47<03:02, 3410.87 examples/s]Map (num_proc=32):   7%|â–‹         | 44072/665298 [00:47<02:52, 3601.43 examples/s]Map (num_proc=32):   7%|â–‹         | 44459/665298 [00:48<02:55, 3537.18 examples/s]Map (num_proc=32):   7%|â–‹         | 44849/665298 [00:48<03:03, 3384.70 examples/s]Map (num_proc=32):   7%|â–‹         | 45223/665298 [00:48<03:05, 3335.41 examples/s]Map (num_proc=32):   7%|â–‹         | 45561/665298 [00:48<03:05, 3338.31 examples/s]Map (num_proc=32):   7%|â–‹         | 46072/665298 [00:48<02:46, 3725.56 examples/s]Map (num_proc=32):   7%|â–‹         | 46515/665298 [00:48<02:46, 3707.82 examples/s]Map (num_proc=32):   7%|â–‹         | 46920/665298 [00:49<05:59, 1720.30 examples/s]Map (num_proc=32):   7%|â–‹         | 47206/665298 [00:49<05:53, 1750.27 examples/s]Map (num_proc=32):   7%|â–‹         | 47478/665298 [00:50<19:17, 533.89 examples/s] Map (num_proc=32):   7%|â–‹         | 47666/665298 [00:51<16:55, 608.41 examples/s]Map (num_proc=32):   7%|â–‹         | 47912/665298 [00:51<17:12, 598.10 examples/s]Map (num_proc=32):   7%|â–‹         | 48201/665298 [00:51<13:05, 785.71 examples/s]Map (num_proc=32):   7%|â–‹         | 48589/665298 [00:51<09:14, 1112.99 examples/s]Map (num_proc=32):   7%|â–‹         | 48921/665298 [00:51<07:22, 1394.42 examples/s]Map (num_proc=32):   7%|â–‹         | 49420/665298 [00:51<05:18, 1932.91 examples/s]Map (num_proc=32):   7%|â–‹         | 49871/665298 [00:52<04:18, 2384.50 examples/s]Map (num_proc=32):   8%|â–Š         | 50310/665298 [00:52<03:43, 2752.87 examples/s]Map (num_proc=32):   8%|â–Š         | 50699/665298 [00:52<03:43, 2747.21 examples/s]Map (num_proc=32):   8%|â–Š         | 54840/665298 [00:52<00:56, 10864.47 examples/s]Map (num_proc=32):   8%|â–Š         | 56083/665298 [00:52<01:46, 5718.95 examples/s] Map (num_proc=32):   9%|â–Š         | 57050/665298 [00:53<02:10, 4671.23 examples/s]Map (num_proc=32):   9%|â–Š         | 57821/665298 [00:53<02:32, 3977.88 examples/s]Map (num_proc=32):   9%|â–‰         | 58467/665298 [00:53<02:43, 3707.91 examples/s]Map (num_proc=32):   9%|â–‰         | 59045/665298 [00:53<02:52, 3504.54 examples/s]Map (num_proc=32):   9%|â–‰         | 59510/665298 [00:54<02:58, 3390.26 examples/s]Map (num_proc=32):   9%|â–‰         | 59928/665298 [00:54<03:19, 3041.41 examples/s]Map (num_proc=32):   9%|â–‰         | 60329/665298 [00:54<03:40, 2739.01 examples/s]Map (num_proc=32):   9%|â–‰         | 60696/665298 [00:54<03:46, 2666.94 examples/s]Map (num_proc=32):   9%|â–‰         | 61005/665298 [00:54<04:17, 2343.09 examples/s]Map (num_proc=32):   9%|â–‰         | 61295/665298 [00:54<04:07, 2436.21 examples/s]Map (num_proc=32):   9%|â–‰         | 61604/665298 [00:55<04:19, 2326.07 examples/s]Map (num_proc=32):   9%|â–‰         | 61900/665298 [00:55<04:05, 2454.76 examples/s]Map (num_proc=32):   9%|â–‰         | 62190/665298 [00:55<05:02, 1990.99 examples/s]Map (num_proc=32):   9%|â–‰         | 62475/665298 [00:55<04:51, 2068.39 examples/s]Map (num_proc=32):   9%|â–‰         | 62837/665298 [00:55<04:12, 2385.40 examples/s]Map (num_proc=32):  10%|â–‰         | 63304/665298 [00:55<03:32, 2832.69 examples/s]Map (num_proc=32):  10%|â–‰         | 63673/665298 [00:55<03:21, 2982.67 examples/s]Map (num_proc=32):  10%|â–‰         | 64035/665298 [00:56<03:17, 3040.46 examples/s]Map (num_proc=32):  10%|â–‰         | 64364/665298 [00:56<03:38, 2755.17 examples/s]Map (num_proc=32):  10%|â–‰         | 64686/665298 [00:56<03:31, 2837.64 examples/s]Map (num_proc=32):  10%|â–‰         | 65008/665298 [00:56<03:45, 2660.56 examples/s]Map (num_proc=32):  10%|â–‰         | 65362/665298 [00:56<03:52, 2575.43 examples/s]Map (num_proc=32):  10%|â–‰         | 65648/665298 [00:56<03:46, 2641.89 examples/s]Map (num_proc=32):  10%|â–‰         | 65966/665298 [00:56<03:38, 2737.29 examples/s]Map (num_proc=32):  10%|â–‰         | 66259/665298 [00:56<03:38, 2747.47 examples/s]Map (num_proc=32):  10%|â–ˆ         | 66553/665298 [00:56<03:36, 2759.77 examples/s]Map (num_proc=32):  10%|â–ˆ         | 66880/665298 [00:57<03:28, 2875.43 examples/s]Map (num_proc=32):  10%|â–ˆ         | 67201/665298 [00:57<03:52, 2570.37 examples/s]Map (num_proc=32):  10%|â–ˆ         | 67570/665298 [00:57<03:38, 2737.95 examples/s]Map (num_proc=32):  10%|â–ˆ         | 67880/665298 [00:57<03:43, 2677.13 examples/s]Map (num_proc=32):  10%|â–ˆ         | 68164/665298 [00:57<03:46, 2638.73 examples/s]Map (num_proc=32):  10%|â–ˆ         | 68498/665298 [00:57<03:37, 2738.97 examples/s]Map (num_proc=32):  10%|â–ˆ         | 68801/665298 [00:57<03:45, 2648.72 examples/s]Map (num_proc=32):  10%|â–ˆ         | 69087/665298 [00:57<03:41, 2686.67 examples/s]Map (num_proc=32):  10%|â–ˆ         | 69433/665298 [00:58<03:54, 2539.70 examples/s]Map (num_proc=32):  10%|â–ˆ         | 69716/665298 [00:58<03:51, 2574.58 examples/s]Map (num_proc=32):  11%|â–ˆ         | 70005/665298 [00:58<03:44, 2653.83 examples/s]Map (num_proc=32):  11%|â–ˆ         | 70438/665298 [00:58<03:12, 3092.84 examples/s]Map (num_proc=32):  11%|â–ˆ         | 70821/665298 [00:58<03:04, 3222.23 examples/s]Map (num_proc=32):  11%|â–ˆ         | 71217/665298 [00:58<02:56, 3356.73 examples/s]Map (num_proc=32):  11%|â–ˆ         | 71713/665298 [00:58<02:37, 3758.79 examples/s]Map (num_proc=32):  11%|â–ˆ         | 72139/665298 [00:58<02:32, 3891.29 examples/s]Map (num_proc=32):  11%|â–ˆ         | 72656/665298 [00:58<02:23, 4143.95 examples/s]Map (num_proc=32):  11%|â–ˆ         | 73093/665298 [00:59<02:41, 3672.33 examples/s]Map (num_proc=32):  11%|â–ˆ         | 73471/665298 [00:59<04:25, 2227.63 examples/s]Map (num_proc=32):  11%|â–ˆ         | 73939/665298 [00:59<03:41, 2665.88 examples/s]Map (num_proc=32):  11%|â–ˆ         | 74285/665298 [00:59<05:46, 1704.70 examples/s]Map (num_proc=32):  11%|â–ˆ         | 74652/665298 [01:00<04:56, 1992.68 examples/s]Map (num_proc=32):  11%|â–ˆâ–        | 75009/665298 [01:00<04:30, 2183.16 examples/s]Map (num_proc=32):  11%|â–ˆâ–        | 75352/665298 [01:00<04:07, 2383.38 examples/s]Map (num_proc=32):  11%|â–ˆâ–        | 75667/665298 [01:01<14:24, 681.88 examples/s] Map (num_proc=32):  11%|â–ˆâ–        | 76196/665298 [01:01<09:30, 1031.86 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 76680/665298 [01:01<07:00, 1399.47 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 77120/665298 [01:01<05:36, 1748.03 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 77540/665298 [01:02<04:57, 1976.32 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 77951/665298 [01:02<04:13, 2315.65 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 78404/665298 [01:02<03:35, 2727.62 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 78846/665298 [01:02<03:14, 3018.30 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 79410/665298 [01:02<02:45, 3535.73 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 79927/665298 [01:02<02:30, 3894.34 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 80468/665298 [01:02<02:18, 4213.28 examples/s]Map (num_proc=32):  12%|â–ˆâ–        | 80942/665298 [01:02<03:02, 3208.20 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 83359/665298 [01:03<02:37, 3685.02 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 84430/665298 [01:03<02:12, 4395.36 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 84948/665298 [01:03<02:37, 3680.17 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 85437/665298 [01:04<02:52, 3366.11 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 85874/665298 [01:04<02:57, 3262.04 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 86262/665298 [01:04<03:06, 3096.56 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 86643/665298 [01:04<03:22, 2853.33 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 86937/665298 [01:04<03:21, 2866.42 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 87234/665298 [01:04<03:20, 2882.83 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 87596/665298 [01:04<03:10, 3029.31 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 87959/665298 [01:05<03:23, 2841.67 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 88252/665298 [01:05<03:21, 2860.38 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 88589/665298 [01:05<03:38, 2640.09 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 88868/665298 [01:05<03:35, 2670.68 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 89152/665298 [01:05<03:32, 2709.28 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 89450/665298 [01:05<03:27, 2775.76 examples/s]Map (num_proc=32):  13%|â–ˆâ–        | 89747/665298 [01:05<03:23, 2825.03 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 90046/665298 [01:05<03:20, 2867.99 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 90347/665298 [01:05<03:51, 2483.15 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 90657/665298 [01:06<05:02, 1897.77 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 91090/665298 [01:06<04:02, 2370.29 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 91600/665298 [01:06<03:13, 2964.13 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 91957/665298 [01:06<03:10, 3014.34 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 92318/665298 [01:06<03:18, 2882.83 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 92655/665298 [01:06<03:38, 2623.24 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 92994/665298 [01:07<03:37, 2636.31 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 93354/665298 [01:07<03:31, 2709.79 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 93719/665298 [01:07<03:25, 2787.44 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 94065/665298 [01:07<06:29, 1467.90 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 94357/665298 [01:07<05:52, 1619.45 examples/s]Map (num_proc=32):  14%|â–ˆâ–        | 94604/665298 [01:08<08:30, 1118.06 examples/s]